[
{
	"uri": "http://localhost:1313/internship_report/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1: Getting acquainted with the team and getting familiar with the basic of AWS\nWeek 2: Not yet\nWeek 3: Not yet\nWeek 4: Not yet\n"
},
{
	"uri": "http://localhost:1313/internship_report/4-eventparticipated/4.1-kick-off/",
	"title": "Kick-off AWS FCJ Workforce - FPTU OJT FALL 2025",
	"tags": [],
	"description": "",
	"content": "Post-experience Report Event Objectives Build a high-quality generation of AWS Builders for Vietnam. Equip them with practical skills in Cloud, DevOps, AI/ML, Security, and Data \u0026amp; Analytics. Connect students with the 47,000+ member AWS Study Group community and AWS partner businesses. Speakers Nguyen Gia Hung – Head of Solutions Architect, AWS Vietnam Do Huy Thang – DevOps Lead, VNG Doanh Doan Hieu Nghi – GenAI Engineer, Renova Bui Ho Linh Nhi – AI Engineer, SoftwareOne Pham Nguyen Hai Anh – Cloud Engineer, G-Asia Pacific Nguyen Dong Thanh Hiep – Principal Cloud Engineer, G-Asia Pacific Key Highlights Identifying the common pitfalls lead to failure Spending on what make you fun for the mean time while ignoring what make you better. Learning a course for it job title instead of viewing it as a competitive edges -\u0026gt; No one would see you as a valuable workforce. Learning is a life-long journey and no one can take a shortcut in the ladder of knowledge. The journey for a working opportunities No one have it easy when it come to finding a job. Its a long and challenging way involving hard working and the ability to seize opportunity. What awaits me at AWS First Cloud Journey A way to connect to others around me and find companions to accompany me for the AWS First Cloud Journey and even for life. Lots of challenges that I need to cross to be a better version of myself. Opportunities for hand-on experience to further improve my abilities. Key Takeaways Prioritize Long-Term Growth Over Short-Term Fun Success requires investing in skills that build your future value, not just spending on temporary enjoyment. Approach learning as a way to gain a genuine competitive edge, rather than just collecting a job title, to become a truly valuable professional.\nEmbrace the Challenge as a Lifelong Journey Understand that securing career opportunities is a difficult process requiring hard work and persistence. There are no shortcuts on the ladder of knowledge; view every challenge as a necessary step to becoming a better version of yourself.\nEvent Experience Attending the “Kick-off AWS First Cloud Journey” workshop was extremely valuable, giving me a solid foundation of essential concepts, the practical skills to start building, and the inspiration to continue my lifelong learning journey in the cloud. Key experiences included:\nLearning from highly skilled speakers The event provided a multi-faceted learning experience, blending high-level industry vision with practical, on-the-ground career advice. We received a strategic overview of the cloud\u0026rsquo;s future from Nguyễn Gia Hưng, Head of Solutions Architect at AWS Vietnam, and gained deep insights into the crucial role of DevOps from Đỗ Huy Thắng, DevOps Lead at VNG. This was perfectly complemented by the relatable and inspiring stories from program alumni, who shared their personal journeys from being students to becoming specialized professionals like a GenAI Engineer and a Cloud Engineer. Hearing directly about \u0026ldquo;a day in the life\u0026rdquo; and the transition from the program into a full-time tech role provided a clear and tangible picture of the path ahead.\nNetworking and discussions From the moment of check-in to the dedicated tea break, the atmosphere was buzzing with energy. There were invaluable opportunities to connect with fellow students who will be our peers and collaborators throughout this On-the-Job Training program. Beyond peer networking, the final Q\u0026amp;A session was a highlight, allowing us to directly engage with the speakers and mentors. This open forum provided a chance to ask specific questions about career paths, technical challenges, and personal development, turning the one-way flow of information into a dynamic and collaborative discussion.\nLessons learned Three core lessons stood out from the event. First, cloud computing is the foundational launchpad for modern careers, not just a single destination; it\u0026rsquo;s the gateway to specializations in AI, DevOps, Security, and Data. Second, the journey is a marathon, not a sprint. The diverse stories from the alumni emphasized that this program is a critical first step, but continuous learning and resilience are what shape a successful career. Finally, community is a powerful accelerator. The event solidified that we are now part of a larger ecosystem-the AWS Builders community-where collaboration and shared knowledge are essential for growth.\nSome event photos Overall, the event not only provided technical knowledge but also helped me reshape my thinking about the way of learning and encourage me to keep pushing harder.\n"
},
{
	"uri": "http://localhost:1313/internship_report/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "How We Built a Flywheel to Steadily Improve Security for Amazon RDS by Joshua Brindle\nThis blog details the process an AWS security team undertook to secure a new feature, PL/Rust, on Amazon Relational Database Service (Amazon RDS). The author, a principal security engineer, explains how the team moved beyond a simple implementation to build a comprehensive, self-improving security system—a \u0026ldquo;flywheel\u0026rdquo;—that combines technology, process, and testing to protect customers.\nThe Pieces of the System The project\u0026rsquo;s central component was PL/Rust, an extension that allows users to write custom PostgreSQL functions in Rust that are then compiled into highly performant native machine code. The core of this extension is a library called postgrestd, which was designed to prevent database escapes. However, at the time, the library was new and had not yet been hardened for the realities of a large-scale production environment. The primary security challenge arose from the fact that PL/Rust compiles these extensions directly on the database instance itself. This design requires a full development toolchain to be locally available, significantly increasing the potential risk. A poorly constructed extension could destabilize the database or its host instance, and attackers could use various techniques to try and bypass security controls like the write xor execute (W^X) model. This context made it clear that a series of robust mitigations were necessary to provide this functionality to customers safely.\nChallenging the Approach The AWS culture of obsessing over operational excellence—with a focus on automation, resilience, and simplicity—heavily influenced the search for a solution. The team considered SELinux (Security-Enhanced Linux), which was described as a long-debated option. SELinux is a set of kernel features that enforces mandatory access control (MAC), adding a powerful layer of protection on top of the standard authorization system. Using SELinux policies, an administrator can be extremely specific about what is allowed on a system, for instance, by preventing a process from writing to a file even if its ownership permissions would normally permit it. This level of deterministic control can greatly increase an operating system\u0026rsquo;s security. The trade-off, however, is reduced flexibility and the significant effort required to configure the access controls to meet specific security requirements. After a thorough internal debate, which involved senior leaders challenging the idea to anticipate future issues, the team agreed that for the PL/Rust use case, the benefits of SELinux outweighed the downsides. The decision was made to proceed with this approach.\nBuilding the Security Flywheel Simply implementing a tool wasn\u0026rsquo;t enough; the team built a complete, constantly improving process around it.\nEnforce and Monitor: They built the SELinux environment and created policies to lock down the system. Crucially, they configured these policies to send any denial messages to their internal telemetry systems for analysis. Respond: Working with the internal blue team, they developed specific incident response playbooks for the Amazon RDS team to investigate these denial messages. Test and Refine: The team began running quarterly \u0026ldquo;game days\u0026rdquo;. During these exercises, the red team would stage exploits against the system, and the service team would respond using their playbooks. Afterwards, all teams would analyze the response to find bottlenecks and areas for improvement. This cycle of enforcement, monitoring, response, and testing created a strong, well-oiled security machine.\nThe Flywheel in Action: A Real-World Example The effectiveness of this system was validated in a production environment. An SELinux denial message automatically generated a high-severity ticket for the service team. The system had worked as expected—it successfully blocked an unauthorized activity, acting as a proactive intrusion detection system. Even though the immediate risk was neutralized, the team\u0026rsquo;s process required them to investigate the root cause to see if the system could be improved further. The investigation eventually revealed that the activity was initiated by the research team at Varonis Threat Labs. AWS security then reached out to them to collaborate, demonstrating how a security event can lead to positive engagement with the research community. This incident provided a concrete and rewarding example of how the team\u0026rsquo;s work directly benefited customers.\nFor the security engineers involved, this was deeply validating, as it provided a concrete example of how their proactive work directly benefited customers by preventing a potential issue.\n"
},
{
	"uri": "http://localhost:1313/internship_report/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Create an SSL connection to Amazon RDS for Db2 in Java without KeyStore or Keytool by Vikram Khatri, Amine Yahsine, Ashish Saraswat, and Sumit Kumar\nThis article outlines a simplified method for establishing a secure SSL database connection in Java, specifically for Amazon Relational Database Service (Amazon RDS) for Db2. The approach allows developers to bypass the traditional complexities associated with the keytool utility and the management of Java KeyStores. The primary benefits of this technique include its simplicity, its suitability for automated environments like CI/CD pipelines, and its ability to maintain strong security through proper TLS 1.2 negotiation and server certificate validation.\nSolution Overview Instead of relying on a traditional Java TrustStore, this solution leverages a specific configuration property supported by the IBM JDBC driver. The driver can be instructed to use a PEM-formatted server certificate directly, which eliminates the need to convert the certificate or import it into a .jks file.\nThis is accomplished by setting the sslCertLocation property:\nproperties.put(\u0026#34;sslCertLocation\u0026#34;, \u0026#34;/path/to/certchain.pem\u0026#34;); To ensure the connection is encrypted and uses a secure protocol, the following JDBC driver connection properties must also be set:\nsslConnection=true sslVersion=TLSv1.2 This method is particularly well-suited for cloud environments like AWS, where Amazon RDS provides a PEM-formatted certificate bundle. The solution was tested with an IBM Db2 JDBC Driver (db2jcc4.jar v4.33.31), Java 11+, and a PEM certificate from Amazon RDS.\nPrerequisites Before implementing this solution, the following resources are assumed to be in place:\nAn Amazon RDS for Db2 server instance with SSL already enabled. A certificate chain PEM file, such as the region-specific us-east-1-bundle.pem available for download from AWS. A recent version of the IBM Data Server Driver (db2jcc4.jar version 4.33 or later). Java 8 or higher, with support for TLS 1.2. The Java Program The full source code for a Java program (Db2SSLTest.java) that connects to Amazon RDS for Db2 using this SSL method is provided below:\nimport java.sql.*; import java.util.Properties; public class Db2SSLTest { public static void main(String[] args) { if (args.length != 6) { System.out.println(\u0026#34;Usage: java Db2SSLTest \u0026#34; + \u0026#34; \u0026lt;certchain.pem\u0026gt; \u0026#34; + \u0026#34; \u0026lt;hostname\u0026gt; \u0026lt;port\u0026gt; \u0026lt;database\u0026gt; \u0026lt;userid\u0026gt; \u0026lt;password\u0026gt;\u0026#34;); System.exit(1); } Properties properties = new Properties(); String certPath = args[0]; String hostname = args[1]; String port = args[2]; String database = args[3]; String userid = args[4]; String password = args[5]; properties.put(\u0026#34;sslConnection\u0026#34;, \u0026#34;true\u0026#34;); properties.put(\u0026#34;sslVersion\u0026#34;, \u0026#34;TLSv1.2\u0026#34;); properties.put(\u0026#34;sslCertLocation\u0026#34;, certPath); properties.put(\u0026#34;user\u0026#34;, userid); properties.put(\u0026#34;password\u0026#34;, password); String url = \u0026#34;jdbc:db2://\u0026#34; + hostname + \u0026#34;:\u0026#34; + port + \u0026#34;/\u0026#34; + database; try { Class.forName(\u0026#34;com.ibm.db2.jcc.DB2Driver\u0026#34;); Connection conn = DriverManager.getConnection(url, properties); Statement stmt = conn.createStatement(); ResultSet rs = stmt.executeQuery(\u0026#34;SELECT CURRENT \u0026#34; + \u0026#34; TIMESTAMP \u0026#34; + \u0026#34; FROM SYSIBM.SYSDUMMY1\u0026#34;); if (rs.next()) { System.out.println(\u0026#34;SSL Connection successful!\u0026#34;); System.out.println(\u0026#34;Current timestamp: \u0026#34; + rs.getString(1)); } rs.close(); stmt.close(); conn.close(); } catch (Exception e) { System.err.println(\u0026#34;Error: \u0026#34; + e.getMessage()); e.printStackTrace(); } } } Compile and Run Assuming the IBM JDBC driver is located at ~/sqllib/java/db2jcc4.jar, the following shell script illustrates how to compile and run the Java program. The script also includes a function to retrieve the database password from AWS Secrets Manager or prompt the user for it manually.\n#!/usr/bin/env bash # Retrieves the master user password for a specified DB instance. # This function attempts to obtain the master user password for the provided # DB instance ID. It first checks if the password can be retrieved from the # AWS Secrets Manager. If a valid secret is not found, it prompts the user # to manually enter the password. # # Args: # DB_INSTANCE_ID (str): The database instance identifier. # # Environment Variables: # REGION: The AWS region where the DB instance is located. # # Exports: # MASTER_USER_PASSWORD: The retrieved or entered master user password. # # Returns: # int: Returns 1 if the password retrieval fails, otherwise 0. get_master_password() { DB_INSTANCE_ID=$1 SECRET_ARN=$(aws rds describe-db-instances \\ --db-instance-identifier \u0026#34;$DB_INSTANCE_ID\u0026#34; \\ --region $REGION \\ --query \u0026#34;DBInstances[0].MasterUserSecret.SecretArn\u0026#34; \\ --output text) if [[ -z \u0026#34;$SECRET_ARN\u0026#34; || \u0026#34;$SECRET_ARN\u0026#34; == \u0026#34;None\u0026#34; ]]; then read -rsp \u0026#34;Enter Master User password: \u0026#34; MASTER_USER_PASSWORD echo else SECRET_JSON=$(aws secretsmanager get-secret-value \\ --secret-id \u0026#34;$SECRET_ARN\u0026#34; \\ --query \u0026#34;SecretString\u0026#34; \\ --region $REGION \\ --output text) MASTER_USER_PASSWORD=$(jq -r \u0026#39;.password\u0026#39; \u0026lt;\u0026lt;\u0026lt; \u0026#34;$SECRET_JSON\u0026#34;) if [[ -z \u0026#34;$MASTER_USER_PASSWORD\u0026#34; ]]; then echo \u0026#34;Failed to get password from secret manager \u0026#39;$SECRET_ARN\u0026#39;. Exiting...\u0026#34; return 1 fi export MASTER_USER_PASSWORD=$MASTER_USER_PASSWORD fi } # Retrieves the master user name for a specified DB instance. # # This function queries AWS RDS to obtain the master user name for the provided # DB instance identifier. If the master user name is not found, it returns an # error message. # # Environment Variables: # DB_INSTANCE_IDENTIFIER: The database instance identifier. # REGION: The AWS region where the DB instance is located. # # Exports: # MASTER_USER_NAME: The retrieved master user name. # # Returns: # int: Returns 1 if the master user name is not found, otherwise 0. get_master_user_name() { local master_user_name=($(aws rds describe-db-instances \\ --db-instance-identifier \u0026#34;$DB_INSTANCE_IDENTIFIER\u0026#34; \\ --region $REGION \\ --query \u0026#34;DBInstances[0].MasterUsername\u0026#34; \\ --output text)) if [ \u0026#34;$master_user_name\u0026#34; = \u0026#34;None\u0026#34; ]; then echo \u0026#34;Not found\u0026#34; return 1 else export MASTER_USER_NAME=$master_user_name fi } # Retrieves the database address for a specified DB instance. # # This function queries AWS RDS to obtain the database endpoint address for the # provided DB instance identifier. If the address is not found, it returns an # error message. # # Environment Variables: # DB_INSTANCE_IDENTIFIER: The database instance identifier. # REGION: The AWS region where the DB instance is located. # # Exports: # DB_ADDRESS: The retrieved database endpoint address. # # Returns: # int: Returns 1 if the database address is not found, otherwise 0. get_db_address() { local db_address=($(aws rds describe-db-instances \\ --db-instance-identifier \u0026#34;$DB_INSTANCE_IDENTIFIER\u0026#34; \\ --region $REGION \\ --query \u0026#34;DBInstances[0].Endpoint.Address\u0026#34; \\ --output text)) if [ -z \u0026#34;$db_address\u0026#34; ]; then echo \u0026#34;Not found\u0026#34; return 1 else export DB_ADDRESS=$db_address fi } # Retrieves the SSL port number for a specified DB instance. # # This function queries AWS RDS to obtain the parameter group name associated # with the provided DB instance identifier, and then queries the parameter # group to obtain the SSL port number. If the SSL port is not found, it returns # an error message. # # Environment Variables: # DB_INSTANCE_IDENTIFIER: The database instance identifier. # REGION: The AWS region where the DB instance is located. # # Exports: # SSL_PORT: The retrieved SSL port number. # # Returns: # int: Returns 1 if the SSL port is not found, otherwise 0. get_ssl_port() { SSL_PORT=\u0026#34;\u0026#34; DB_PARAM_GROUP_NAME=$(aws rds describe-db-instances \\ --db-instance-identifier \u0026#34;$DB_INSTANCE_IDENTIFIER\u0026#34; \\ --region $REGION \\ --query \u0026#34;DBInstances[0].DBParameterGroups[0].DBParameterGroupName\u0026#34; \\ --output text) if [ \u0026#34;$DB_PARAM_GROUP_NAME\u0026#34; != \u0026#34;\u0026#34; ]; then SSL_PORT=$(aws rds describe-db-parameters \\ --db-parameter-group-name \u0026#34;$DB_PARAM_GROUP_NAME\u0026#34; \\ --region $REGION \\ --query \u0026#34;Parameters[?ParameterName==\u0026#39;ssl_svcename\u0026#39;].ParameterValue\u0026#34; \\ --output text) if [ \u0026#34;$SSL_PORT\u0026#34; = \u0026#34;None\u0026#34; ]; then SSL_PORT=\u0026#34;\u0026#34; return 1 fi fi export SSL_PORT=$SSL_PORT return 0 } # Main entry point for the script. # # This function compiles a Java program, downloads the SSL certificate, retrieves # the master user name, master password, database address, and SSL port from AWS # RDS, and then runs the Java program with the retrieved parameters. # # Exports: # None # # Returns: # int: Returns 0 if the program runs successfully, otherwise 1. main () { DB_INSTANCE_IDENTIFIER=\u0026#34;viz-demo\u0026#34; CL_PATH=.:$HOME/sqllib/java/db2jcc4.jar REGION=\u0026#34;us-east-1\u0026#34; PROG_NAME=Db2SSLTest JAVA_FILE=${PROG_NAME}.java DBNAME=\u0026#34;TEST\u0026#34; if ! command -v javac \u0026amp;\u0026gt;/dev/null; then echo \u0026#34;javac is not installed. Please install Java Development Kit (JDK) to compile Java programs.\u0026#34; exit 1 fi echo \u0026#34;Compile Java program $JAVA_FILE\u0026#34; javac -cp $CL_PATH $JAVA_FILE echo \u0026#34;Downloading SSL certificate...\u0026#34; CERTCHAIN=\u0026#34;/home/db2inst1/us-east-1-bundle.pem\u0026#34; if [ -f \u0026#34;$CERTCHAIN\u0026#34; ]; then echo \u0026#34;Certificate already exists. Skipping download.\u0026#34; else echo \u0026#34;Certificate does not exist. Downloading...\u0026#34; if ! curl -sL \u0026#34;https://truststore.pki.rds.amazonaws.com/us-east-1/$REGION-bundle.pem\u0026#34; -o $REGION-bundle.pem; then echo \u0026#34;Failed to download SSL certificate. Please check your network connection or the URL.\u0026#34; exit 1 fi fi if get_master_user_name \u0026#34;$DB_INSTANCE_IDENTIFIER\u0026#34;; then echo \u0026#34;Master user name: $MASTER_USER_NAME\u0026#34; USER=\u0026#34;$MASTER_USER_NAME\u0026#34; else echo \u0026#34;Failed to retrieve master user name. Exiting...\u0026#34; exit 1 fi if get_master_password \u0026#34;$DB_INSTANCE_IDENTIFIER\u0026#34;; then PASSWORD=$MASTER_USER_PASSWORD else echo \u0026#34;Failed to retrieve master password. Exiting...\u0026#34; exit 1 fi if get_db_address \u0026#34;$DB_INSTANCE_IDENTIFIER\u0026#34;; then echo \u0026#34;DB Address: $DB_ADDRESS\u0026#34; HOST=\u0026#34;$DB_ADDRESS\u0026#34; else echo \u0026#34;Failed to retrieve DB address. Exiting...\u0026#34; exit 1 fi if get_ssl_port \u0026#34;$DB_INSTANCE_IDENTIFIER\u0026#34;; then echo \u0026#34;SSL Port: $SSL_PORT\u0026#34; PORT=\u0026#34;$SSL_PORT\u0026#34; else echo \u0026#34;Failed to retrieve SSL port. Exiting...\u0026#34; exit 1 fi # Use -Djavax.net.debug=ssl:handshake:verbose to debug SSL issues echo \u0026#34;Running Java program...\u0026#34; java \\ -cp \u0026#34;$CL_PATH\u0026#34; $PROG_NAME $CERTCHAIN $HOST $PORT $DBNAME $USER $PASSWORD } main \u0026#34;$@\u0026#34; Considerations There is a limitation in the JDBC driver (at the time of writing) that prevents the use of a global certificate bundle (like global-bundle.pem) with the sslCertLocation property. If an application connects to a single AWS Region, it is recommended to use a region-specific certificate file (e.g., us-east-1-bundle.pem). If there is an absolute requirement to use the global bundle, developers must revert to the traditional method of using keytool to store the certificates in a keystore.\nTroubleshooting The following are solutions to common issues:\nFailing SSL connection: If the SSL connection fails after being enabled in the RDS for Db2 instance, the instance must be restarted. The SSL enablement only takes effect after a restart. Unable to locate the db2jcc4.jar file: This file is included with various IBM DB2 client packages, such as the data server client or runtime client. Unable to connect to the RDS database: If a connection fails after cataloging a database with SSL using db2cli commands, there might be an existing database connection that is not aware of the newly cataloged database. The db2 terminate command should be used to close existing connections before testing again. Conclusion This post demonstrates that it is not always necessary to use the Java KeyStore or keytool utility to enable SSL connections. With a PEM certificate and a modern JDBC driver, a secure connection can be established with minimal setup. This approach is especially valuable for developers who need to perform rapid SSL testing, for automated environments such as CI/CD and containers, and for anyone looking to simplify secure Java-to-Db2 connectivity.\n"
},
{
	"uri": "http://localhost:1313/internship_report/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Enhance the local testing experience for serverless applications with LocalStack by Patrick Galvin and Debasis Rath\nThis article announces and explains new capabilities designed to simplify the local testing experience for serverless applications. Through an integration with AWS Partner, LocalStack, the AWS Toolkit for Visual Studio Code now provides a more streamlined way for developers to build, test, and debug their serverless applications without leaving their development environment.\nChallenges with Local Serverless Development While serverless architectures are generally simple to operate and scale, the development and testing process can introduce friction that slows down the code-test-debug cycle. Developers often encounter several common roadblocks:\nSlow Iteration from Cloud-Based Validation: Previously, developers had to deploy AWS Serverless Application Model (AWS SAM) templates to the cloud just to test changes, which created significant delays in the feedback loop. Friction from Tool Context Switching: The need to constantly move between integrated development environments (IDEs), command-line interfaces (CLIs), and resource emulators like LocalStack leads to fragmented and inefficient workflows. Complex Manual Setup: Manually configuring port mapping and making code edits for local integration tests can introduce inconsistencies between the local and cloud environments. Limited Service Integration Debugging: Troubleshooting Lambda functions that interact with other AWS services, such as DynamoDB or Amazon SQS, has traditionally required complex manual configuration, extending the time needed to resolve issues. Automated Setup Process The LocalStack VSCode Extension can be installed directly from the AWS Toolkit, which provides an intelligent wizard for a streamlined setup. This wizard automatically detects if LocalStack is configured and guides the user through the process. It also handles authentication through a browser-based flow and securely stores the token. Furthermore, the wizard checks for and creates the necessary AWS CLI profiles for LocalStack, allowing developers to easily switch between their local and cloud environments.\nTesting a Serverless Application The article demonstrates these capabilities with a practical example: an event-driven order processing system that uses API Gateway, Amazon SQS, Lambda, and Amazon Simple Notification Service (Amazon SNS).\nThe Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Create a gateway endpoint",
	"tags": [],
	"description": "",
	"content": " Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "
},
{
	"uri": "http://localhost:1313/internship_report/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Learn to write worklog and working handle workshop. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 09/08/2025 09/08/2025 Policies: https://policies.fcjuni.com/ 3 - Learn about AWS and its types of services (for further project) + Compute + Storage + Networking + Database + Etc. - Learn how to write workshop via video and instructions\n09/09/2025 09/09/2025 About AWS: https://cloudjourney.awsstudygroup.com/ About workshop: https://van-hoang-kha.github.io/vi/ 4 - Apply previous day knowledge about workshop to write worklog - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Try out: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 09/10/2025 09/10/2025 My workshop git: https://github.com/isntbim/internship_report AWS Console: https://aws.amazon.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + Storage + Test launch an EC2 instance - SSH connection methods to EC2 - Learn about Elastic IP 09/11/2025 09/11/2025 EC2 console: https://ap-southeast-1.console.aws.amazon.com/ec2/ Amazon EC2 Basics: https://www.coursera.org/learn/aws-amazon-ec2-basics/ 6 - Further practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 09/12/2025 09/12/2025 EC2 console: https://ap-southeast-1.console.aws.amazon.com/ec2/ Week 1 Achievements (not finished): Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database Etc. Successfully created and configured an AWS Free Tier account.\nGetting used to workshop and writing worklogs.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region Etc. Learned basic EC2 concepts:\nInstance types: Balancing between cost and performance. AMI: Pre-configured templates for launching instances. EBS: Persistent block storage for instances. Storage: Different types of storage options for various use cases. Elastic IP: Static IP addresses for dynamic cloud computing. "
},
{
	"uri": "http://localhost:1313/internship_report/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Define and scope the first typing game project (core features, microservice boundaries, future matchmaking). Establish team foundation: shared repo, initial backlog, ER diagrams, tech stack, ownership. Standardize WPM and accuracy formulas. Prototype FastAPI service: text generation, sentence assembly, chat; validate Bedrock integration path. Set up AWS Budgets with alerting. Gain working proficiency: Lambda (function URL), VPC (subnets, gateways, peering vs transit), Flow Logs, load balancing concepts. Provision Amazon RDS; design schema and seed dataset for text retrieval. Refactor text service to DB-backed retrieval; benchmark vs prior API method. Introduce early operational practices: role assignment, benchmarking, monitoring for scalability. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Hold a team brainstorming session to define and prioritize concepts for the first project + Transfer our brainstorming notes from the project canvas into an initial task backlog on the project management board + Research and document the specific algorithms for calculating WPM and accuracy to ensure consistency +Create the shared code repository and establish the basic project structure for the chosen languages and microservices + Refine the sketched ER diagrams and begin drafting the initial database schemas + Formally assign lead responsibility for each microservice to individual team members to streamline development 09/15/2025 09/15/2025 3 - Set up AWS Budgets: + Review budget types (Cost, Usage, RI, etc.) + Define monthly cost threshold + Configure budget in the AWS console + Set up email/SNS alerts - Create a web app using AWS Lambda: + Learn Lambda and function URL fundamentals + Code a simple \u0026ldquo;Hello World\u0026rdquo; function + Configure the function and its IAM role + Enable and test the function URL endpoint - Learn and test FastAPI for microservices: + Go through the official FastAPI tutorial + Set up a local development environment + Create a proof-of-concept API + Implement and test basic endpoints using Swagger UI 09/16/2025 09/16/2025 AWS Lambda: https://ap-southeast-1.console.aws.amazon.com/lambda AWS Budgets: https://us-east-1.console.aws.amazon.com/costmanagement/ FastAPI: https://www.coursera.org/learn/packt-mastering-rest-apis-with-fastapi-1xeea/ 4 - Exploring the Fundamentals of AWS Networking and Security + Review the core concept of an Amazon VPC (Virtual Private Cloud) as an isolated section of the AWS cloud + Understand the purpose of subnets, and the distinction between public and private subnets for structuring a network + Learn the roles of an Internet Gateway for providing internet access to public subnets and a NAT Gateway for allowing private subnets to access the internet securely + Explore VPC Flow Logs as a tool for monitoring and troubleshooting network traffic within your VPC + Compare methods for connecting an on-premise data center to AWS: Site-to-Site VPN for encrypted connections over the internet and Direct Connect for a dedicated, private connection + Understand the use cases for VPC Peering (connecting two VPCs directly) versus a Transit Gateway + Grasp the overall purpose of Elastic Load Balancing (ELB) to distribute application traffic across multiple servers for high availability 09/17/2025 09/17/2025 Module 02-(01 to 03): https://www.youtube.com/watch?v=O9Ac_vGHquM https://www.youtube.com/watch?v=BPuD1l2hEQ4 https://www.youtube.com/watch?v=CXU8D3kyxIc 5 - Explore the Amazon Bedrock playground: + Review the available foundation models (e.g., Claude, Titan) + Select a model for text generation + Experiment with different prompts and parameters + Generate and analyze a sample response - Prototype the designated microservice: + Structured the service\u0026rsquo;s logic and integrated multiple APIs for generating text + Implemented the core functions for creating random sentences and managing a chat feature + Reviewed the initial build to identify limitations and outline next steps for improvement + Validate our technical approach 09/18/2025 09/18/2025 Amazon Bedrock:https://ap-southeast-1.console.aws.amazon.com/bedrock 6 - Launch and Configure an Amazon RDS Database: + Review and select a suitable database engine for the project\u0026rsquo;s needs + Configure the core instance settings, including credentials, VPC, and security group access rules + Launch the database, monitor its creation, and securely record the connection endpoint - Prototype a Database-Driven TextService: + Design a simple database schema with tables for words and sentences to store text content + Create a one-time script to populate the new RDS database with an initial dataset + Refactor the TextService to fetch data from the database instead of external APIs and benchmark the performance improvement + Run a simple benchmark to compare the response times between the old API-based method and the new database query method 09/19/2025 09/19/2025 Aurora and RDS: https://ap-southeast-1.console.aws.amazon.com/rds Week 2 Achievements: Defined first typing game scope:\nCore features Microservice boundaries Backlog seeded Ownership assigned. Standard WPM and accuracy formulas researched and documented.\nShared repository initialized with baseline multi-language structure.\nER diagrams refined and initial relational schema drafted.\nFastAPI prototype delivered (text generation, sentence assembly, chat) and verified via Swagger UI.\nAWS Budgets configured with monthly threshold and alerting.\nCore AWS foundations learned:\nLambda (function URL) VPC (subnets, IGW, NAT, Flow Logs) Peering vs transit gateway Load balancing concepts Amazon Bedrock models explored; candidate model and prompt approach validated.\nRDS instance launched, schema created, seed dataset loaded.\nTextService refactored to DB-backed retrieval with initial performance improvement benchmarked.\nEarly operational practices introduced: role ownership, benchmarking focus, scalability considerations.\n"
},
{
	"uri": "http://localhost:1313/internship_report/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Complete foundational AWS hands-on labs (Site-to-Site VPN, EC2 fundamentals). Progress through and finish all four modules of the AWS Cloud Technical Essentials course. Strengthen practical usage of AWS Console and CLI (configuration, key management, region/service discovery). Collaborate with product/design to review and document TypeRush Figma UI/UX for future implementation. Evaluate storage options and decide on a scalable NoSQL approach for TextService data. Prototype and integrate MongoDB (environment setup, seeding, service refactor, validation). Build effective communication cadence with First Cloud Journey team members. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Lab 03: AWS Site-to-Site VPN: + Create a complete Site-to-Site VPN environment, including a new VPC, an EC2 instance to act as a customer gateway, a Virtual Private Gateway, and the VPN connection itself. + Configure and test the VPN tunnel connectivity. - Lab 04: Amazon EC2 Fundamentals: + Launch and connect to both Microsoft Windows Server and Amazon Linux EC2 instances. + Deploy a sample \u0026ldquo;AWS User Management\u0026rdquo; application on both Windows and Linux environments to practice basic CRUD operations. + Explore core EC2 features like modifying instance types, managing EBS snapshots, and creating custom AMIs. 09/22/2025 09/22/2025 VPN Lab (Lab 03): https://000003.awsstudygroup.com/ EC2 Lab (Lab 04): https://000004.awsstudygroup.com/ 3 - Getting start with AWS Cloud Technical Essentials course, covering 2 modules: + Module 1: Cloud Foundations \u0026amp; IAM - Define cloud computing and its value proposition. - Differentiate between on-premises and cloud workloads. - Create an AWS account and review different methods for interacting with AWS services. - Describe the AWS Global Infrastructure, including Regions and Availability Zones. - Learn and apply best practices for AWS Identity and Access Management (IAM). + Module 2: Compute \u0026amp; Networking - Review the basic components of Amazon EC2 architecture. - Differentiate between containers and virtual machines. - Discover the features and advantages of serverless technologies. - Learn basic networking concepts and the features of Amazon Virtual Private Cloud (VPC). - Create a VPC. 09/23/2025 09/23/2025 AWS Cloud Technical Essentials: https://www.coursera.org/learn/aws-cloud-technical-essentials 4 - Collaborate and Document Figma Design for TypeRush UI/UX: + Participate in a dedicated cross-functional design review meeting to comprehensively examine the latest Figma mockups provided by the UI/UX team. + Systematically analyze each key screen (e.g., login/registration, main game interface, post-game score summary, settings menu) to grasp the visual hierarchy, component states, and intended user interactions. + Compile a list of initial technical feasibility questions and potential UI implementation considerations to facilitate further discussion and alignment between design and engineering. + Begin translating key design elements into preliminary front-end component requirements or user stories, setting the groundwork for future development sprints. - Discuss Text Service Storage with Team Lead \u0026amp; Confirm NoSQL Choice: + Engage in a focused discussion with the team leader regarding the optimal database solution for the TextService (e.g., for storing words and sentences). + Present the pros and cons of relational (SQL) vs. non-relational (NoSQL) options in the context of our data structure and anticipated access patterns. + Confirm the decision to proceed with a NoSQL database as the chosen storage solution due to its flexibility and scalability for text content. 09/24/2025 09/24/2025 5 - Integrate and Test MongoDB with TextService Prototype: + Set up MongoDB Environment: Set up a local instance using Docker. + Refactor Data Seeding Script: Modify the population script to insert the words and sentences into MongoDB collections as documents. + Rewrite Service Logic: Update the TextService data retrieval methods to query the MongoDB collections instead of the previous data source. + Verify Integration: Thoroughly test the refactored service to confirm that it can successfully connect, write to, and read from the MongoDB database. 09/25/2025 09/25/2025 6 - Completing AWS Cloud Technical Essentials course, with 2 final modules: + Module 3: Storage \u0026amp; Databases - Differentiate between file, block, and object storage models. - Explain core Amazon S3 concepts like buckets and objects, then create an S3 bucket. - Describe the function and use cases of Amazon EBS with EC2. - Explore the various database services available on AWS. - Understand the function of Amazon DynamoDB and create a DynamoDB table. + Module 4: Monitoring \u0026amp; High Availability - Define the benefits of monitoring and the role of Amazon CloudWatch. - Understand how to optimize solutions for performance and cost on AWS. - Describe the function of Elastic Load Balancing (ELB) to route and distribute traffic. - Differentiate between vertical scaling (scaling up) and horizontal scaling (scaling out). - Configure a solution for high availability. 09/26/2025 09/26/2025 AWS Cloud Technical Essentials: https://www.coursera.org/learn/aws-cloud-technical-essentials Week 3 Achievements: AWS Infrastructure Labs:\nBuilt full Site-to-Site VPN (VPC, customer gateway EC2, virtual private gateway, tunnel validation) Executed EC2 fundamentals (Windows \u0026amp; Linux instances, CRUD app deployment, snapshots, custom AMIs) Finished all four modules of AWS Cloud Technical Essentials (Foundations/IAM, Compute \u0026amp; Networking, Storage \u0026amp; Databases, Monitoring \u0026amp; High Availability).\nAWS Console \u0026amp; CLI Proficiency:\nAccount setup, credential \u0026amp; config management Region \u0026amp; service exploration Key pair handling and resource inspection workflows TypeRush UI/UX Documentation:\nAnalyzed Figma screens (login, game, score summary, settings) and navigation flows Captured component states \u0026amp; feasibility questions Drafted initial user stories / component requirements TextService Storage Architecture:\nEvaluated SQL vs NoSQL trade*offs for word/sentence storage Selected NoSQL approach for flexibility \u0026amp; scalability MongoDB Integration Prototype:\nDockerized local MongoDB environment Converted seeding script to insert documents Refactored service data layer to query MongoDB Validated read/write operations end-to-end "
},
{
	"uri": "http://localhost:1313/internship_report/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 4 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Create an S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Test the Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": "Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.3-s3-vpc/",
	"title": "Access S3 from VPC",
	"tags": [],
	"description": "",
	"content": "Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Test the Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "
},
{
	"uri": "http://localhost:1313/internship_report/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "This section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"
},
{
	"uri": "http://localhost:1313/internship_report/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "During my internship, I participated in [x] events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nKick-off Event Name: Kick-off AWS FCJ Workforce - FPTU OJT FALL 2025\nDate \u0026amp; Time: 08:30, September 06, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.4-s3-onprem/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "On-premises DNS Simulation",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "
},
{
	"uri": "http://localhost:1313/internship_report/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Le Tri Dung\nPhone Number: 0941423585\nEmail: tridungit2005@gmail.com\nUniversity: FPT Ho Chi Minh University\nMajor: Data Science\nStudent ID: SE196261\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 09/2025 to 02/2026\nReport Content Worklog Proposal "
},
{
	"uri": "http://localhost:1313/internship_report/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/internship_report/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]