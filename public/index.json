[
{
	"uri": "http://localhost:1313/internship_report/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1: Getting acquainted with the team and mastering AWS basics\nWeek 2: Typing game project scoping, AWS microservices, and operational foundations\nWeek 3: AWS hands-on labs, UI/UX documentation, and NoSQL integration for project 1\nWeek 4: RDS, Auto Scaling, CloudWatch, Route 53, CLI, CI/CD, Docker, Serverless, Security Hub\nWeek 5: From Networking to Deployment: VPC Peering, Transit Gateway, WordPress, Lambda Cost Optimization, CI/CD, Storage Gateway, FSx \u0026amp; WAF\nWeek 6: IAM access control, monitoring stack (Grafana \u0026amp; CloudWatch), Systems Manager, EC2 right-sizing, S3 encryption, Cost Explorer, data lake, and CloudFormation automation\nWeek 7: Deep dive into DynamoDB, IAM federation \u0026amp; cost optimization, Lightsail \u0026amp; containers, Step Functions, Cloud9, Elastic Beanstalk, CI/CD pipelines, and AWS security foundations\nWeek 8: Infrastructure as Code with CloudFormation, resiliency \u0026amp; auto scaling, refactoring to microservices, serverless SPA with Cognito \u0026amp; X-Ray, AI services (Polly, Rekognition, Lex), S3 \u0026amp; CloudFront, and CloudWatch dashboards\nWeek 9: Lex chatbot and SNS pub/sub, DynamoDB and ElastiCache labs, EKS CI/CD and Blueprints, serverless \u0026amp; Fargate microservices, storage performance evaluation, S3 security best practices, and data lake with Glue, Athena \u0026amp; QuickSight\nWeek 10: ROSA deployments, analytics platform with Kinesis, Glue, EMR, Athena, QuickSight \u0026amp; Redshift, business dashboards, VPC Flow Logs, billing delegation, CDK, event-driven SNS/SQS, and full serverless stack with Cognito, CloudFront, SQS/SNS, and CI/CD\nWeek 11: AWS community event, serverless text service with DynamoDB, caching \u0026amp; validation, Amazon Bedrock Agent integration, monitoring with CloudWatch \u0026amp; X-Ray, and GraphQL APIs with AppSync \u0026amp; DynamoDB\n"
},
{
	"uri": "http://localhost:1313/internship_report/4-eventparticipated/4.1-kick-off/",
	"title": "Kick-off AWS FCJ Workforce - FPTU OJT FALL 2025",
	"tags": [],
	"description": "",
	"content": "Post-experience Report Event Objectives Build a high-quality generation of AWS Builders for Vietnam. Equip them with practical skills in Cloud, DevOps, AI/ML, Security, and Data \u0026amp; Analytics. Connect students with the 47,000+ member AWS Study Group community and AWS partner businesses. Speakers Nguyen Gia Hung – Head of Solutions Architect, AWS Vietnam Do Huy Thang – DevOps Lead, VNG Doanh Doan Hieu Nghi – GenAI Engineer, Renova Bui Ho Linh Nhi – AI Engineer, SoftwareOne Pham Nguyen Hai Anh – Cloud Engineer, G-Asia Pacific Nguyen Dong Thanh Hiep – Principal Cloud Engineer, G-Asia Pacific Key Highlights Identifying the common pitfalls lead to failure Spending on what make you fun for the mean time while ignoring what make you better. Learning a course for it job title instead of viewing it as a competitive edges -\u0026gt; No one would see you as a valuable workforce. Learning is a life-long journey and no one can take a shortcut in the ladder of knowledge. The journey for a working opportunities No one have it easy when it come to finding a job. Its a long and challenging way involving hard working and the ability to seize opportunity. What awaits me at AWS First Cloud Journey A way to connect to others around me and find companions to accompany me for the AWS First Cloud Journey and even for life. Lots of challenges that I need to cross to be a better version of myself. Opportunities for hand-on experience to further improve my abilities. Key Takeaways Prioritize Long-Term Growth Over Short-Term Fun Success requires investing in skills that build your future value, not just spending on temporary enjoyment. Approach learning as a way to gain a genuine competitive edge, rather than just collecting a job title, to become a truly valuable professional.\nEmbrace the Challenge as a Lifelong Journey Understand that securing career opportunities is a difficult process requiring hard work and persistence. There are no shortcuts on the ladder of knowledge; view every challenge as a necessary step to becoming a better version of yourself.\nEvent Experience Attending the “Kick-off AWS First Cloud Journey” workshop was extremely valuable, giving me a solid foundation of essential concepts, the practical skills to start building, and the inspiration to continue my lifelong learning journey in the cloud. Key experiences included:\nLearning from highly skilled speakers The event provided a multi-faceted learning experience, blending high-level industry vision with practical, on-the-ground career advice. We received a strategic overview of the cloud\u0026rsquo;s future from Nguyễn Gia Hưng, Head of Solutions Architect at AWS Vietnam, and gained deep insights into the crucial role of DevOps from Đỗ Huy Thắng, DevOps Lead at VNG. This was perfectly complemented by the relatable and inspiring stories from program alumni, who shared their personal journeys from being students to becoming specialized professionals like a GenAI Engineer and a Cloud Engineer. Hearing directly about \u0026ldquo;a day in the life\u0026rdquo; and the transition from the program into a full-time tech role provided a clear and tangible picture of the path ahead.\nNetworking and discussions From the moment of check-in to the dedicated tea break, the atmosphere was buzzing with energy. There were invaluable opportunities to connect with fellow students who will be our peers and collaborators throughout this On-the-Job Training program. Beyond peer networking, the final Q\u0026amp;A session was a highlight, allowing us to directly engage with the speakers and mentors. This open forum provided a chance to ask specific questions about career paths, technical challenges, and personal development, turning the one-way flow of information into a dynamic and collaborative discussion.\nLessons learned Three core lessons stood out from the event. First, cloud computing is the foundational launchpad for modern careers, not just a single destination; it\u0026rsquo;s the gateway to specializations in AI, DevOps, Security, and Data. Second, the journey is a marathon, not a sprint. The diverse stories from the alumni emphasized that this program is a critical first step, but continuous learning and resilience are what shape a successful career. Finally, community is a powerful accelerator. The event solidified that we are now part of a larger ecosystem-the AWS Builders community-where collaboration and shared knowledge are essential for growth.\nSome event photos Overall, the event not only provided technical knowledge but also helped me reshape my thinking about the way of learning and encourage me to keep pushing harder.\n"
},
{
	"uri": "http://localhost:1313/internship_report/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "How We Built a Flywheel to Steadily Improve Security for Amazon RDS by Joshua Brindle\nThis blog details the process an AWS security team undertook to secure a new feature, PL/Rust, on Amazon Relational Database Service (Amazon RDS). The author, a principal security engineer, explains how the team moved beyond a simple implementation to build a comprehensive, self-improving security system—a \u0026ldquo;flywheel\u0026rdquo;—that combines technology, process, and testing to protect customers.\nThe Pieces of the System The project\u0026rsquo;s central component was PL/Rust, an extension that allows users to write custom PostgreSQL functions in Rust that are then compiled into highly performant native machine code. The core of this extension is a library called postgrestd, which was designed to prevent database escapes. However, at the time, the library was new and had not yet been hardened for the realities of a large-scale production environment. The primary security challenge arose from the fact that PL/Rust compiles these extensions directly on the database instance itself. This design requires a full development toolchain to be locally available, significantly increasing the potential risk. A poorly constructed extension could destabilize the database or its host instance, and attackers could use various techniques to try and bypass security controls like the write xor execute (W^X) model. This context made it clear that a series of robust mitigations were necessary to provide this functionality to customers safely.\nChallenging the Approach The AWS culture of obsessing over operational excellence—with a focus on automation, resilience, and simplicity—heavily influenced the search for a solution. The team considered SELinux (Security-Enhanced Linux), which was described as a long-debated option. SELinux is a set of kernel features that enforces mandatory access control (MAC), adding a powerful layer of protection on top of the standard authorization system. Using SELinux policies, an administrator can be extremely specific about what is allowed on a system, for instance, by preventing a process from writing to a file even if its ownership permissions would normally permit it. This level of deterministic control can greatly increase an operating system\u0026rsquo;s security. The trade-off, however, is reduced flexibility and the significant effort required to configure the access controls to meet specific security requirements. After a thorough internal debate, which involved senior leaders challenging the idea to anticipate future issues, the team agreed that for the PL/Rust use case, the benefits of SELinux outweighed the downsides. The decision was made to proceed with this approach.\nBuilding the Security Flywheel Simply implementing a tool wasn\u0026rsquo;t enough; the team built a complete, constantly improving process around it.\nEnforce and Monitor: They built the SELinux environment and created policies to lock down the system. Crucially, they configured these policies to send any denial messages to their internal telemetry systems for analysis. Respond: Working with the internal blue team, they developed specific incident response playbooks for the Amazon RDS team to investigate these denial messages. Test and Refine: The team began running quarterly \u0026ldquo;game days\u0026rdquo;. During these exercises, the red team would stage exploits against the system, and the service team would respond using their playbooks. Afterwards, all teams would analyze the response to find bottlenecks and areas for improvement. This cycle of enforcement, monitoring, response, and testing created a strong, well-oiled security machine.\nThe Flywheel in Action: A Real-World Example The effectiveness of this system was validated in a production environment. An SELinux denial message automatically generated a high-severity ticket for the service team. The system had worked as expected—it successfully blocked an unauthorized activity, acting as a proactive intrusion detection system. Even though the immediate risk was neutralized, the team\u0026rsquo;s process required them to investigate the root cause to see if the system could be improved further. The investigation eventually revealed that the activity was initiated by the research team at Varonis Threat Labs. AWS security then reached out to them to collaborate, demonstrating how a security event can lead to positive engagement with the research community. This incident provided a concrete and rewarding example of how the team\u0026rsquo;s work directly benefited customers.\nFor the security engineers involved, this was deeply validating, as it provided a concrete example of how their proactive work directly benefited customers by preventing a potential issue.\n"
},
{
	"uri": "http://localhost:1313/internship_report/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Create an SSL connection to Amazon RDS for Db2 in Java without KeyStore or Keytool by Vikram Khatri, Amine Yahsine, Ashish Saraswat, and Sumit Kumar\nThis article outlines a simplified method for establishing a secure SSL database connection in Java, specifically for Amazon Relational Database Service (Amazon RDS) for Db2. The approach allows developers to bypass the traditional complexities associated with the keytool utility and the management of Java KeyStores. The primary benefits of this technique include its simplicity, its suitability for automated environments like CI/CD pipelines, and its ability to maintain strong security through proper TLS 1.2 negotiation and server certificate validation.\nSolution Overview Instead of relying on a traditional Java TrustStore, this solution leverages a specific configuration property supported by the IBM JDBC driver. The driver can be instructed to use a PEM-formatted server certificate directly, which eliminates the need to convert the certificate or import it into a .jks file.\nThis is accomplished by setting the sslCertLocation property:\nproperties.put(\u0026#34;sslCertLocation\u0026#34;, \u0026#34;/path/to/certchain.pem\u0026#34;); To ensure the connection is encrypted and uses a secure protocol, the following JDBC driver connection properties must also be set:\nsslConnection=true sslVersion=TLSv1.2 This method is particularly well-suited for cloud environments like AWS, where Amazon RDS provides a PEM-formatted certificate bundle. The solution was tested with an IBM Db2 JDBC Driver (db2jcc4.jar v4.33.31), Java 11+, and a PEM certificate from Amazon RDS.\nPrerequisites Before implementing this solution, the following resources are assumed to be in place:\nAn Amazon RDS for Db2 server instance with SSL already enabled. A certificate chain PEM file, such as the region-specific us-east-1-bundle.pem available for download from AWS. A recent version of the IBM Data Server Driver (db2jcc4.jar version 4.33 or later). Java 8 or higher, with support for TLS 1.2. The Java Program The full source code for a Java program (Db2SSLTest.java) that connects to Amazon RDS for Db2 using this SSL method is provided below:\nimport java.sql.*; import java.util.Properties; public class Db2SSLTest { public static void main(String[] args) { if (args.length != 6) { System.out.println(\u0026#34;Usage: java Db2SSLTest \u0026#34; + \u0026#34; \u0026lt;certchain.pem\u0026gt; \u0026#34; + \u0026#34; \u0026lt;hostname\u0026gt; \u0026lt;port\u0026gt; \u0026lt;database\u0026gt; \u0026lt;userid\u0026gt; \u0026lt;password\u0026gt;\u0026#34;); System.exit(1); } Properties properties = new Properties(); String certPath = args[0]; String hostname = args[1]; String port = args[2]; String database = args[3]; String userid = args[4]; String password = args[5]; properties.put(\u0026#34;sslConnection\u0026#34;, \u0026#34;true\u0026#34;); properties.put(\u0026#34;sslVersion\u0026#34;, \u0026#34;TLSv1.2\u0026#34;); properties.put(\u0026#34;sslCertLocation\u0026#34;, certPath); properties.put(\u0026#34;user\u0026#34;, userid); properties.put(\u0026#34;password\u0026#34;, password); String url = \u0026#34;jdbc:db2://\u0026#34; + hostname + \u0026#34;:\u0026#34; + port + \u0026#34;/\u0026#34; + database; try { Class.forName(\u0026#34;com.ibm.db2.jcc.DB2Driver\u0026#34;); Connection conn = DriverManager.getConnection(url, properties); Statement stmt = conn.createStatement(); ResultSet rs = stmt.executeQuery(\u0026#34;SELECT CURRENT \u0026#34; + \u0026#34; TIMESTAMP \u0026#34; + \u0026#34; FROM SYSIBM.SYSDUMMY1\u0026#34;); if (rs.next()) { System.out.println(\u0026#34;SSL Connection successful!\u0026#34;); System.out.println(\u0026#34;Current timestamp: \u0026#34; + rs.getString(1)); } rs.close(); stmt.close(); conn.close(); } catch (Exception e) { System.err.println(\u0026#34;Error: \u0026#34; + e.getMessage()); e.printStackTrace(); } } } Compile and Run Assuming the IBM JDBC driver is located at ~/sqllib/java/db2jcc4.jar, the following shell script illustrates how to compile and run the Java program. The script also includes a function to retrieve the database password from AWS Secrets Manager or prompt the user for it manually.\n#!/usr/bin/env bash # Retrieves the master user password for a specified DB instance. # This function attempts to obtain the master user password for the provided # DB instance ID. It first checks if the password can be retrieved from the # AWS Secrets Manager. If a valid secret is not found, it prompts the user # to manually enter the password. # # Args: # DB_INSTANCE_ID (str): The database instance identifier. # # Environment Variables: # REGION: The AWS region where the DB instance is located. # # Exports: # MASTER_USER_PASSWORD: The retrieved or entered master user password. # # Returns: # int: Returns 1 if the password retrieval fails, otherwise 0. get_master_password() { DB_INSTANCE_ID=$1 SECRET_ARN=$(aws rds describe-db-instances \\ --db-instance-identifier \u0026#34;$DB_INSTANCE_ID\u0026#34; \\ --region $REGION \\ --query \u0026#34;DBInstances[0].MasterUserSecret.SecretArn\u0026#34; \\ --output text) if [[ -z \u0026#34;$SECRET_ARN\u0026#34; || \u0026#34;$SECRET_ARN\u0026#34; == \u0026#34;None\u0026#34; ]]; then read -rsp \u0026#34;Enter Master User password: \u0026#34; MASTER_USER_PASSWORD echo else SECRET_JSON=$(aws secretsmanager get-secret-value \\ --secret-id \u0026#34;$SECRET_ARN\u0026#34; \\ --query \u0026#34;SecretString\u0026#34; \\ --region $REGION \\ --output text) MASTER_USER_PASSWORD=$(jq -r \u0026#39;.password\u0026#39; \u0026lt;\u0026lt;\u0026lt; \u0026#34;$SECRET_JSON\u0026#34;) if [[ -z \u0026#34;$MASTER_USER_PASSWORD\u0026#34; ]]; then echo \u0026#34;Failed to get password from secret manager \u0026#39;$SECRET_ARN\u0026#39;. Exiting...\u0026#34; return 1 fi export MASTER_USER_PASSWORD=$MASTER_USER_PASSWORD fi } # Retrieves the master user name for a specified DB instance. # # This function queries AWS RDS to obtain the master user name for the provided # DB instance identifier. If the master user name is not found, it returns an # error message. # # Environment Variables: # DB_INSTANCE_IDENTIFIER: The database instance identifier. # REGION: The AWS region where the DB instance is located. # # Exports: # MASTER_USER_NAME: The retrieved master user name. # # Returns: # int: Returns 1 if the master user name is not found, otherwise 0. get_master_user_name() { local master_user_name=($(aws rds describe-db-instances \\ --db-instance-identifier \u0026#34;$DB_INSTANCE_IDENTIFIER\u0026#34; \\ --region $REGION \\ --query \u0026#34;DBInstances[0].MasterUsername\u0026#34; \\ --output text)) if [ \u0026#34;$master_user_name\u0026#34; = \u0026#34;None\u0026#34; ]; then echo \u0026#34;Not found\u0026#34; return 1 else export MASTER_USER_NAME=$master_user_name fi } # Retrieves the database address for a specified DB instance. # # This function queries AWS RDS to obtain the database endpoint address for the # provided DB instance identifier. If the address is not found, it returns an # error message. # # Environment Variables: # DB_INSTANCE_IDENTIFIER: The database instance identifier. # REGION: The AWS region where the DB instance is located. # # Exports: # DB_ADDRESS: The retrieved database endpoint address. # # Returns: # int: Returns 1 if the database address is not found, otherwise 0. get_db_address() { local db_address=($(aws rds describe-db-instances \\ --db-instance-identifier \u0026#34;$DB_INSTANCE_IDENTIFIER\u0026#34; \\ --region $REGION \\ --query \u0026#34;DBInstances[0].Endpoint.Address\u0026#34; \\ --output text)) if [ -z \u0026#34;$db_address\u0026#34; ]; then echo \u0026#34;Not found\u0026#34; return 1 else export DB_ADDRESS=$db_address fi } # Retrieves the SSL port number for a specified DB instance. # # This function queries AWS RDS to obtain the parameter group name associated # with the provided DB instance identifier, and then queries the parameter # group to obtain the SSL port number. If the SSL port is not found, it returns # an error message. # # Environment Variables: # DB_INSTANCE_IDENTIFIER: The database instance identifier. # REGION: The AWS region where the DB instance is located. # # Exports: # SSL_PORT: The retrieved SSL port number. # # Returns: # int: Returns 1 if the SSL port is not found, otherwise 0. get_ssl_port() { SSL_PORT=\u0026#34;\u0026#34; DB_PARAM_GROUP_NAME=$(aws rds describe-db-instances \\ --db-instance-identifier \u0026#34;$DB_INSTANCE_IDENTIFIER\u0026#34; \\ --region $REGION \\ --query \u0026#34;DBInstances[0].DBParameterGroups[0].DBParameterGroupName\u0026#34; \\ --output text) if [ \u0026#34;$DB_PARAM_GROUP_NAME\u0026#34; != \u0026#34;\u0026#34; ]; then SSL_PORT=$(aws rds describe-db-parameters \\ --db-parameter-group-name \u0026#34;$DB_PARAM_GROUP_NAME\u0026#34; \\ --region $REGION \\ --query \u0026#34;Parameters[?ParameterName==\u0026#39;ssl_svcename\u0026#39;].ParameterValue\u0026#34; \\ --output text) if [ \u0026#34;$SSL_PORT\u0026#34; = \u0026#34;None\u0026#34; ]; then SSL_PORT=\u0026#34;\u0026#34; return 1 fi fi export SSL_PORT=$SSL_PORT return 0 } # Main entry point for the script. # # This function compiles a Java program, downloads the SSL certificate, retrieves # the master user name, master password, database address, and SSL port from AWS # RDS, and then runs the Java program with the retrieved parameters. # # Exports: # None # # Returns: # int: Returns 0 if the program runs successfully, otherwise 1. main () { DB_INSTANCE_IDENTIFIER=\u0026#34;viz-demo\u0026#34; CL_PATH=.:$HOME/sqllib/java/db2jcc4.jar REGION=\u0026#34;us-east-1\u0026#34; PROG_NAME=Db2SSLTest JAVA_FILE=${PROG_NAME}.java DBNAME=\u0026#34;TEST\u0026#34; if ! command -v javac \u0026amp;\u0026gt;/dev/null; then echo \u0026#34;javac is not installed. Please install Java Development Kit (JDK) to compile Java programs.\u0026#34; exit 1 fi echo \u0026#34;Compile Java program $JAVA_FILE\u0026#34; javac -cp $CL_PATH $JAVA_FILE echo \u0026#34;Downloading SSL certificate...\u0026#34; CERTCHAIN=\u0026#34;/home/db2inst1/us-east-1-bundle.pem\u0026#34; if [ -f \u0026#34;$CERTCHAIN\u0026#34; ]; then echo \u0026#34;Certificate already exists. Skipping download.\u0026#34; else echo \u0026#34;Certificate does not exist. Downloading...\u0026#34; if ! curl -sL \u0026#34;https://truststore.pki.rds.amazonaws.com/us-east-1/$REGION-bundle.pem\u0026#34; -o $REGION-bundle.pem; then echo \u0026#34;Failed to download SSL certificate. Please check your network connection or the URL.\u0026#34; exit 1 fi fi if get_master_user_name \u0026#34;$DB_INSTANCE_IDENTIFIER\u0026#34;; then echo \u0026#34;Master user name: $MASTER_USER_NAME\u0026#34; USER=\u0026#34;$MASTER_USER_NAME\u0026#34; else echo \u0026#34;Failed to retrieve master user name. Exiting...\u0026#34; exit 1 fi if get_master_password \u0026#34;$DB_INSTANCE_IDENTIFIER\u0026#34;; then PASSWORD=$MASTER_USER_PASSWORD else echo \u0026#34;Failed to retrieve master password. Exiting...\u0026#34; exit 1 fi if get_db_address \u0026#34;$DB_INSTANCE_IDENTIFIER\u0026#34;; then echo \u0026#34;DB Address: $DB_ADDRESS\u0026#34; HOST=\u0026#34;$DB_ADDRESS\u0026#34; else echo \u0026#34;Failed to retrieve DB address. Exiting...\u0026#34; exit 1 fi if get_ssl_port \u0026#34;$DB_INSTANCE_IDENTIFIER\u0026#34;; then echo \u0026#34;SSL Port: $SSL_PORT\u0026#34; PORT=\u0026#34;$SSL_PORT\u0026#34; else echo \u0026#34;Failed to retrieve SSL port. Exiting...\u0026#34; exit 1 fi # Use -Djavax.net.debug=ssl:handshake:verbose to debug SSL issues echo \u0026#34;Running Java program...\u0026#34; java \\ -cp \u0026#34;$CL_PATH\u0026#34; $PROG_NAME $CERTCHAIN $HOST $PORT $DBNAME $USER $PASSWORD } main \u0026#34;$@\u0026#34; Considerations There is a limitation in the JDBC driver (at the time of writing) that prevents the use of a global certificate bundle (like global-bundle.pem) with the sslCertLocation property. If an application connects to a single AWS Region, it is recommended to use a region-specific certificate file (e.g., us-east-1-bundle.pem). If there is an absolute requirement to use the global bundle, developers must revert to the traditional method of using keytool to store the certificates in a keystore.\nTroubleshooting The following are solutions to common issues:\nFailing SSL connection: If the SSL connection fails after being enabled in the RDS for Db2 instance, the instance must be restarted. The SSL enablement only takes effect after a restart. Unable to locate the db2jcc4.jar file: This file is included with various IBM DB2 client packages, such as the data server client or runtime client. Unable to connect to the RDS database: If a connection fails after cataloging a database with SSL using db2cli commands, there might be an existing database connection that is not aware of the newly cataloged database. The db2 terminate command should be used to close existing connections before testing again. Conclusion This post demonstrates that it is not always necessary to use the Java KeyStore or keytool utility to enable SSL connections. With a PEM certificate and a modern JDBC driver, a secure connection can be established with minimal setup. This approach is especially valuable for developers who need to perform rapid SSL testing, for automated environments such as CI/CD and containers, and for anyone looking to simplify secure Java-to-Db2 connectivity.\n"
},
{
	"uri": "http://localhost:1313/internship_report/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Enhance the local testing experience for serverless applications with LocalStack by Patrick Galvin and Debasis Rath\nThis article announces and explains new capabilities designed to simplify the local testing experience for serverless applications. Through an integration with AWS Partner, LocalStack, the AWS Toolkit for Visual Studio Code now provides a more streamlined way for developers to build, test, and debug their serverless applications without leaving their development environment.\nChallenges with Local Serverless Development While serverless architectures are generally simple to operate and scale, the development and testing process can introduce friction that slows down the code-test-debug cycle. Developers often encounter several common roadblocks:\nSlow Iteration from Cloud-Based Validation: Previously, developers had to deploy AWS Serverless Application Model (AWS SAM) templates to the cloud just to test changes, which created significant delays in the feedback loop. Friction from Tool Context Switching: The need to constantly move between integrated development environments (IDEs), command-line interfaces (CLIs), and resource emulators like LocalStack leads to fragmented and inefficient workflows. Complex Manual Setup: Manually configuring port mapping and making code edits for local integration tests can introduce inconsistencies between the local and cloud environments. Limited Service Integration Debugging: Troubleshooting Lambda functions that interact with other AWS services, such as DynamoDB or Amazon SQS, has traditionally required complex manual configuration, extending the time needed to resolve issues. Automated Setup Process The LocalStack VSCode Extension can be installed directly from the AWS Toolkit, which provides an intelligent wizard for a streamlined setup. This wizard automatically detects if LocalStack is configured and guides the user through the process. It also handles authentication through a browser-based flow and securely stores the token. Furthermore, the wizard checks for and creates the necessary AWS CLI profiles for LocalStack, allowing developers to easily switch between their local and cloud environments.\nTesting a Serverless Application The article demonstrates these capabilities with a practical example: an event-driven order processing system that uses API Gateway, Amazon SQS, Lambda, and Amazon Simple Notification Service (Amazon SNS).\nWith the new integration, the entire workflow can be tested locally:\nDeploy Locally: The AWS SAM application is deployed to the local LocalStack environment using the LocalStack AWS profile. Debug Locally: Developers can set breakpoints in their Lambda function code directly in VS Code and use the integrated debugger to step through the execution as it interacts with other locally emulated services. Validate End-to-End: The complete workflow, from message ingestion at the API Gateway to the final notification from Amazon SNS, can be tested to confirm all service integrations work correctly before deploying to the cloud. For an in-depth technical demonstration of this LocalStack integration, refer to this youtube video.\nBest Practices for Local Testing To make the most of this new workflow, the article recommends a layered and strategic approach to testing. This involves starting with fast, isolated unit tests to validate core logic, and then progressively moving to broader integration and system-level validation.\nThe recommended testing strategy follows four main steps:\nBegin with local unit tests to focus on isolated function logic. Proceed to local integration testing using LocalStack to confirm interactions between AWS services. After local validation, test the application in the actual AWS environment to surface issues that cannot be emulated, such as IAM permission mismatches or VPC networking challenges. Finally, conduct performance and load testing in AWS to assess how the application handles real-world traffic. When to Use Local Versus Cloud Testing While local testing provides significant speed and cost advantages, it\u0026rsquo;s important to understand its limitations and when to test in the cloud. The following table lists potential use cases for each strategy.\nTesting Scenario Local Testing Cloud Testing Reason Function logic validation ✓ Fast feedback for core business logic Service integration testing ✓ Quick validation of AWS service interactions Rapid iteration during development ✓ Immediate feedback without deployment overhead Cost-sensitive development environments ✓ Minimizes cloud resource costs during development Offline development scenarios ✓ No internet connectivity required Performance and scalability testing ✓ Requires actual AWS infrastructure for accurate results IAM permission validation ✓ LocalStack doesn\u0026rsquo;t fully replicate IAM behavior VPC networking scenarios ✓ Network configurations can\u0026rsquo;t be accurately emulated Production-like load testing ✓ Real performance metrics only available in AWS Final validation before deployment ✓ Supports compatibility with actual AWS environment Conclusion The integration of LocalStack into the AWS Toolkit for VS Code significantly enhances the local development experience for serverless applications. By allowing developers to run and debug complex, multi-service applications directly in their IDE, this new capability helps reduce context switching, catch issues earlier, and lower development costs. This leads to faster test cycles and higher-quality deployments, all while keeping the developer in full control of their local environment.\n"
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Create a gateway endpoint",
	"tags": [],
	"description": "",
	"content": " Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "
},
{
	"uri": "http://localhost:1313/internship_report/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Learn to write worklog and working handle workshop. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 09/08/2025 09/08/2025 Policies: https://policies.fcjuni.com/ 3 - Learn about AWS and its types of services (for further project) + Compute + Storage + Networking + Database + Etc. - Learn how to write workshop via video and instructions\n09/09/2025 09/09/2025 About AWS: https://cloudjourney.awsstudygroup.com/ About workshop: https://van-hoang-kha.github.io/vi/ 4 - Apply previous day knowledge about workshop to write worklog - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Try out: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 09/10/2025 09/10/2025 My workshop git: https://github.com/isntbim/internship_report AWS Console: https://aws.amazon.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + Storage + Test launch an EC2 instance - SSH connection methods to EC2 - Learn about Elastic IP 09/11/2025 09/11/2025 EC2 console: https://ap-southeast-1.console.aws.amazon.com/ec2/ Amazon EC2 Basics: https://www.coursera.org/learn/aws-amazon-ec2-basics/ 6 - Further practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 09/12/2025 09/12/2025 EC2 console: https://ap-southeast-1.console.aws.amazon.com/ec2/ Week 1 Achievements (not finished): Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database Etc. Successfully created and configured an AWS Free Tier account.\nGetting used to workshop and writing worklogs.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region Etc. Learned basic EC2 concepts:\nInstance types: Balancing between cost and performance. AMI: Pre-configured templates for launching instances. EBS: Persistent block storage for instances. Storage: Different types of storage options for various use cases. Elastic IP: Static IP addresses for dynamic cloud computing. "
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Create an S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"
},
{
	"uri": "http://localhost:1313/internship_report/4-eventparticipated/4.2-event2/",
	"title": "Data Science On AWS",
	"tags": [],
	"description": "",
	"content": "Summary Report Event Objectives Provide a comprehensive overview of building a modern Data Science system on AWS. Introduce the end-to-end Data Science pipeline, from data processing to model deployment. Offer hands-on experience with key AWS services like AWS Glue and Amazon SageMaker. Discuss practical considerations such as cost, performance, and the benefits of cloud vs. on-premise solutions. Speakers Văn Hoàng Kha – Cloud Solutions Architect, AWS Community Builder Bạch Doãn Vương – Cloud Develops Engineer, AWS Community Builder Key Highlights End-to-End Data Science Pipeline on AWS The workshop outlined the complete data science journey on the cloud, using core services:\nAmazon S3: For scalable data storage. AWS Glue: For serverless data integration, ETL (Extract, Transform, Load), and data cleaning. Amazon SageMaker: For building, training, and deploying machine learning models at scale. Practical Demonstrations Demo 1: Data Processing with AWS Glue: Showcased how to process and clean a real-world IMDb dataset, emphasizing the importance of data quality for model accuracy. Demo 2: Model Training with SageMaker: Demonstrated the process of training and deploying a Sentiment Analysis model, making the abstract concepts of ML deployment concrete. Integrating Custom Models: Showcased how to leverage frameworks like TensorFlow and PyTorch within SageMaker, using a sample project from a provided GitHub repository. Broadening AI/ML Capabilities with Managed Services An overview of AWS\u0026rsquo;s pre-built AI services that accelerate development:\nAmazon Transcribe: Speech-to-text conversion. Amazon Comprehend: Natural language processing for sentiment analysis and topic extraction. Amazon Rekognition: Image and video analysis. Amazon Personalize: Building personalized recommendation systems. Key Takeaways Data-First Mindset Business-first approach: Always start with the business context of the data, as emphasized by the need for feature engineering. Data quality is paramount: The workshop stressed that the accuracy of any ML model is directly dependent on the quality of the input data. Data as an asset: Data collection, governance, and security are the foundational pillars of a data-driven organization. Technical Architecture Modular Pipeline: The standard architecture involves a pipeline from S3 (storage) to AWS Glue (ETL) to Amazon SageMaker (ML), allowing for a clean separation of concerns. Flexibility: AWS supports both low-code solutions like SageMaker Canvas and code-intensive custom model training using frameworks like TensorFlow/PyTorch. Serverless benefits: Using services like AWS Glue removes the need for managing infrastructure, allowing teams to focus on data and models. Strategy Phased approach: Start with data collection and cleaning before moving to complex model training. Cloud vs. On-premise: The discussion highlighted that the cloud offers significant advantages in scalability, pay-for-what-you-use cost models, and access to powerful computing resources without upfront investment. ROI Measurement: Leverage cloud benefits to reduce development time and infrastructure overhead, leading to faster time-to-market for AI-powered features. Applying to Work Automate ETL: Use AWS Glue to create automated data cleaning and preparation jobs for analytics and ML. Adopt SageMaker: Pilot Amazon SageMaker for training and deploying ML models to streamline the MLOps lifecycle. Implement Sentiment Analysis: Apply the concepts from the demo to analyze customer feedback from reviews or support tickets. Explore Pre-built AI: Integrate services like Amazon Rekognition for content moderation or Amazon Transcribe for call center analytics. Consolidate Knowledge: Build a small project based on the workshop\u0026rsquo;s guidance to reinforce the concepts learned. Event Experience Attending the \u0026ldquo;Data Science on AWS\u0026rdquo; workshop provided a valuable, hands-on journey into cloud-based machine learning. Key experiences included:\nLearning from highly skilled speakers The speakers, both AWS Community Builders, shared practical insights and best practices from their real-world experience. Hands-on technical exposure The live demos of processing data with AWS Glue and training a model with SageMaker were extremely effective at translating theory into practice. Leveraging modern tools Explored the comprehensive AWS ecosystem for data science, understanding how different services fit together to form a cohesive pipeline. Learned about both fully managed AI services and the powerful customization options available within SageMaker. Lessons learned A solid data preparation strategy is non-negotiable for success in machine learning. AWS significantly lowers the barrier to entry for building and deploying sophisticated data science systems. Modern cloud platforms provide the flexibility to choose between low-code tools for speed and custom code for specific, complex requirements. Some event photos Overall, the workshop provided not only technical knowledge but also practical experience with building end-to-end data science pipelines on AWS, emphasizing the importance of data quality and the power of cloud-native ML services.\n"
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "
},
{
	"uri": "http://localhost:1313/internship_report/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "Real-Time Typing Challenge Platform TypeRush An AWS-Powered Architecture for a Multiplayer Typing App 1. Executive Summary TypeRush is an interactive web application that replicates the core experience of Monkeytype while introducing multiplayer functionality for live competitive typing sessions. Designed as a showcase of real-time web technologies and scalable cloud architecture, the platform combines a modern frontend experience with a serverless backend powered by AWS.\nUsers can create or join typing rooms, compete in real time, and view performance metrics such as Words Per Minute (WPM), accuracy, and rankings. The application leverages React for the frontend, AWS Lambda and API Gateway (WebSocket) for real-time synchronization, and Amazon RDS for player data and Amazon ElastiCache for session data. It also leverages Amazon Bedrock to generate daily challenges. Amazon Cognito secures user access and authentication. The solution demonstrates how cloud-native services can deliver fast, scalable, and cost-efficient real-time applications without the need for traditional server management.\n2. Problem Statement What’s the Problem? Existing typing platforms are often limited to single-player functionality, lacking real-time multiplayer interaction or open implementation examples. Most multiplayer solutions require dedicated servers or complex infrastructure, making them costly and difficult to maintain. For developers and enthusiasts, there is no accessible, serverless example demonstrating real-time synchronization, event-driven updates, and secure user management in one integrated system.\nThe Solution This project provides a serverless, real-time typing platform that allows multiple users to connect, type simultaneously, and see instant feedback on their progress. It is designed to be lightweight, cost-efficient, and scalable.\nBenefits and Return on Investment The project serves as both a developer learning platform and a user-facing application. It demonstrates best practices in event-driven architecture, WebSocket implementation, and cloud-based scalability at minimal operational cost. For users, it provides an engaging, social typing experience. For developers, it offers an educational reference for building serverless real-time systems.\nThe platform requires no ongoing server maintenance, ensuring:\nLong-term sustainability Rapid scalability Low operational overhead 3. Solution Architecture The platform employs a serverless AWS architecture to manage a real-time typing challenge application. Data is processed and stored using a variety of AWS services, and the frontend is a modern, responsive UI. The architecture is detailed below: AWS Services Used Amazon S3: Hosts the static frontend. CloudFront: Provides a CDN for the frontend and API. AWS WAF: Acts as a firewall for CloudFront. Route 53: Manages domain and DNS routing. Amazon Cognito: Handles user sign-in and sign-up. API Gateway: Serves as the entry point for microservices and connects to private ECS via VPC Link. AWS Lambda: Powers the record and text microservices and the WebSocket API for the game microservice. Amazon RDS: Stores records and leaderboards. DynamoDB: Stores text data. AWS Bedrock (Titan Text G1 Express): Functions as the LLM model for text generation. Amazon SNS: Manages notifications and alarms. AWS CloudWatch: Handles logging and monitoring. AWS Secrets Manager: Stores database and API secrets. Amazon ECS (Fargate): Hosts the core game backend. Elastic Load Balancer: Balances game service traffic. ElastiCache (Redis): Caches real-time game data. CodePipeline: Orchestrates builds and deployments. CodeBuild: Builds Docker images and Lambda services. ECR (Elastic Container Registry): Stores Docker images. GitLab: Triggers pipeline builds. Component Design Frontend Layer: A modern, responsive UI built with React, hosted on Amazon S3, and served via Amazon CloudFront with AWS WAF and Amazon Route 53 for enhanced performance, global content delivery, and robust security. API \u0026amp; Real-Time Communication: AWS API Gateway serves as the unified gateway for both REST and WebSocket APIs, implementing rate limiting, authentication, and request validation. The WebSocket API powers real-time multiplayer interactions. Compute \u0026amp; Business Logic: AWS Lambda handles serverless processing tasks like data persistence and AI-powered text generation. Amazon ECS with AWS Fargate hosts containerized backend services, including the game server. VPC PrivateLink connects API Gateway to ECS Fargate containers, and an Elastic Load Balancer distributes traffic. Data Layer: Amazon RDS stores relational data like user profiles and leaderboards. Amazon DynamoDB manages non-relational data such as dynamically generated text. Amazon ElastiCache (Redis) provides in-memory caching for frequently accessed data. Authentication \u0026amp; User Management: Amazon Cognito handles secure user registration, authentication, and authorization, supporting social and federated login. Amazon Secret Manager secures storage for secrets like API keys. CI/CD \u0026amp; DevOps: AWS CodeBuild and AWS CodePipeline enable continuous integration and continuous deployment (CI/CD) across all application components, automating build, test, and deployment processes. 4. Technical Implementation Implementation Phases This project has two parts—setting up the cloud infrastructure and building the application—each following 4 phases:\nBuild Theory and Draw Architecture: Research and design the AWS serverless architecture. Calculate Price and Check Practicality: Use the AWS Pricing Calculator to estimate costs and adjust if needed. Fix Architecture for Cost or Solution Fit: Tweak the design to stay cost-effective and usable. Develop, Test, and Deploy: Code the application and AWS services, then test and release to production. Technical Requirements Frontend: Practical knowledge of React. Backend: Experience with AWS services including Lambda, API Gateway, ECS, Fargate, RDS, DynamoDB, and ElastiCache. CI/CD: Familiarity with CodePipeline, CodeBuild, and GitLab. Infrastructure as Code: Use AWS CDK/SDK to code interactions. 5. Timeline \u0026amp; Milestones Project Timeline\nMonth 1: Study all about AWS and AWS services. Month 2: Start planning for the project and the AWS services to implement. Month 3: Develop, implement, and launch. 6. Budget Estimation \u0026mdash;\u0026gt; Budget Estimation File \u0026lt;\u0026mdash;\nInfrastructure Costs AWS Services: Amazon Route 53: $0.90 AWS Web Application Firewall (WAF): $9.03 Amazon CloudFront: $0.40 ($0 with free tier) S3 Standard: $0.11 ($0 with free tier) Data Transfer: $0.00 ($0 with free tier) Amazon API Gateway: $15.92 ($0 with free tier) Amazon Cognito: $0.00 Network Load Balancer: $18.49 AWS Fargate: $8.88 AWS Lambda: $0.02 ($0 with free tier) Amazon ElastiCache: $16.06 Amazon RDS for PostgreSQL: $39.37 ($0 with free tier) AWS Bedrock (Workload 1): $2.63 DynamoDB: $0.03 ($0 with free tier) AWS Secrets Manager: $4.00 AWS CodePipeline: $1.00 ($0 with free tier) AWS CodeBuild: $5.00 ($0 with free tier) With free tier account: about $60/month With paid tier account: about $122/month Both amounts assume 24/7 running services; in reality, it might be much lower.\n7. Risk Assessment Risk Matrix Technical Integration: Medium impact, medium probability. Performance and Scalability: High impact, medium probability. Cost Management: Medium impact, medium probability. Security: High impact, low probability. Data Reliability: Medium impact, low probability. Operational Risks: Low impact, medium probability. Learning Curve: Medium impact, high probability. Mitigation Strategies Technical Integration: This risk is mitigated by phased implementation and adherence to AWS best practices. Performance and Scalability: Optimizing WebSocket communication, leveraging Redis caching, and monitoring with CloudWatch will help maintain responsiveness. Cost Management: These can be controlled through AWS Budgets, usage alerts, and auto-scaling policies. Security: Risks will be minimized using AWS WAF, Cognito, Secrets Manager, and strict IAM policies. Data Reliability: Clear data ownership boundaries and transactional writes will ensure consistency. Operational Risks: Risks in CI/CD pipelines will be reduced with staged deployments and rollback configurations. Learning Curve: The learning curve will be addressed through dedicated study and hands-on practice in the first project phase. Contingency Plans If the service fails: A maintenance page will be displayed to users. If multiplayer is too slow: Multiplayer mode will be temporarily disabled, but single-player will remain active. If costs spike unexpectedly: The change that caused the spike will be immediately reversed. If a new update breaks the site: The system will automatically revert to the previous working version. 8. Expected Outcomes Technical Improvements: Upon completion, the project will deliver a scalable, secure, and fully functional real-time multiplayer typing platform built entirely on AWS.\nLong-term Value: It will demonstrate best practices in serverless architecture, event-driven design, and CI/CD automation. For users, it provides a smooth, engaging multiplayer typing experience. For us, it serves as our first hands-on, practical reference model for building real-time, serverless web applications on AWS with minimal cost and maintenance.\n"
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Test the Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": "Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"
},
{
	"uri": "http://localhost:1313/internship_report/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Define and scope the first typing game project (core features, microservice boundaries, future matchmaking). Establish team foundation: shared repo, initial backlog, ER diagrams, tech stack, ownership. Standardize WPM and accuracy formulas. Prototype FastAPI service: text generation, sentence assembly, chat; validate Bedrock integration path. Set up AWS Budgets with alerting. Gain working proficiency: Lambda (function URL), VPC (subnets, gateways, peering vs transit), Flow Logs, load balancing concepts. Provision Amazon RDS; design schema and seed dataset for text retrieval. Refactor text service to DB-backed retrieval; benchmark vs prior API method. Introduce early operational practices: role assignment, benchmarking, monitoring for scalability. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Hold a team brainstorming session to define and prioritize concepts for the first project + Transfer our brainstorming notes from the project canvas into an initial task backlog on the project management board + Research and document the specific algorithms for calculating WPM and accuracy to ensure consistency +Create the shared code repository and establish the basic project structure for the chosen languages and microservices + Refine the sketched ER diagrams and begin drafting the initial database schemas + Formally assign lead responsibility for each microservice to individual team members to streamline development 09/15/2025 09/15/2025 3 - Set up AWS Budgets: + Review budget types (Cost, Usage, RI, etc.) + Define monthly cost threshold + Configure budget in the AWS console + Set up email/SNS alerts - Create a web app using AWS Lambda: + Learn Lambda and function URL fundamentals + Code a simple \u0026ldquo;Hello World\u0026rdquo; function + Configure the function and its IAM role + Enable and test the function URL endpoint - Learn and test FastAPI for microservices: + Go through the official FastAPI tutorial + Set up a local development environment + Create a proof-of-concept API + Implement and test basic endpoints using Swagger UI 09/16/2025 09/16/2025 AWS Lambda: https://ap-southeast-1.console.aws.amazon.com/lambda AWS Budgets: https://us-east-1.console.aws.amazon.com/costmanagement/ FastAPI: https://www.coursera.org/learn/packt-mastering-rest-apis-with-fastapi-1xeea/ 4 - Exploring the Fundamentals of AWS Networking and Security + Review the core concept of an Amazon VPC (Virtual Private Cloud) as an isolated section of the AWS cloud + Understand the purpose of subnets, and the distinction between public and private subnets for structuring a network + Learn the roles of an Internet Gateway for providing internet access to public subnets and a NAT Gateway for allowing private subnets to access the internet securely + Explore VPC Flow Logs as a tool for monitoring and troubleshooting network traffic within your VPC + Compare methods for connecting an on-premise data center to AWS: Site-to-Site VPN for encrypted connections over the internet and Direct Connect for a dedicated, private connection + Understand the use cases for VPC Peering (connecting two VPCs directly) versus a Transit Gateway + Grasp the overall purpose of Elastic Load Balancing (ELB) to distribute application traffic across multiple servers for high availability 09/17/2025 09/17/2025 Module 02-(01 to 03): https://www.youtube.com/watch?v=O9Ac_vGHquM https://www.youtube.com/watch?v=BPuD1l2hEQ4 https://www.youtube.com/watch?v=CXU8D3kyxIc 5 - Explore the Amazon Bedrock playground: + Review the available foundation models (e.g., Claude, Titan) + Select a model for text generation + Experiment with different prompts and parameters + Generate and analyze a sample response - Prototype the designated microservice: + Structured the service\u0026rsquo;s logic and integrated multiple APIs for generating text + Implemented the core functions for creating random sentences and managing a chat feature + Reviewed the initial build to identify limitations and outline next steps for improvement + Validate our technical approach 09/18/2025 09/18/2025 Amazon Bedrock:https://ap-southeast-1.console.aws.amazon.com/bedrock 6 - Launch and Configure an Amazon RDS Database: + Review and select a suitable database engine for the project\u0026rsquo;s needs + Configure the core instance settings, including credentials, VPC, and security group access rules + Launch the database, monitor its creation, and securely record the connection endpoint - Prototype a Database-Driven TextService: + Design a simple database schema with tables for words and sentences to store text content + Create a one-time script to populate the new RDS database with an initial dataset + Refactor the TextService to fetch data from the database instead of external APIs and benchmark the performance improvement + Run a simple benchmark to compare the response times between the old API-based method and the new database query method 09/19/2025 09/19/2025 Aurora and RDS: https://ap-southeast-1.console.aws.amazon.com/rds Week 2 Achievements: Defined first typing game scope:\nCore features Microservice boundaries Backlog seeded Ownership assigned. Standard WPM and accuracy formulas researched and documented.\nShared repository initialized with baseline multi-language structure.\nER diagrams refined and initial relational schema drafted.\nFastAPI prototype delivered (text generation, sentence assembly, chat) and verified via Swagger UI.\nAWS Budgets configured with monthly threshold and alerting.\nCore AWS foundations learned:\nLambda (function URL) VPC (subnets, IGW, NAT, Flow Logs) Peering vs transit gateway Load balancing concepts Amazon Bedrock models explored; candidate model and prompt approach validated.\nRDS instance launched, schema created, seed dataset loaded.\nTextService refactored to DB-backed retrieval with initial performance improvement benchmarked.\nEarly operational practices introduced: role ownership, benchmarking focus, scalability considerations.\n"
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.3-s3-vpc/",
	"title": "Access S3 from VPC",
	"tags": [],
	"description": "",
	"content": "Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "
},
{
	"uri": "http://localhost:1313/internship_report/4-eventparticipated/4.3-event3/",
	"title": "Reinventing DevOps with AWS Generative AI",
	"tags": [],
	"description": "",
	"content": "Summary Report Event Objectives Share the current context of DevOps and the impact of Generative AI. Present real-world case studies and practical frameworks for integrating AI into DevOps. Provide a live demonstration of AWS Generative AI tools that enhance the development lifecycle. Discuss the evolving role of DevOps engineers and the skills needed for the future. Speakers Le Thanh Duc – Cloud Delivery Manager, CMC Global Du Quoc Thanh – Technical Leader, CMC Global Van Hoang Kha – Cloud Engineer, AWS Community Builder Key Highlights The Evolution from DevOps to DevSecOps The modern DevOps mindset integrates security from the start, making it a shared responsibility across development, security, and operations teams. Shifting from a reactive, post-development security model to a proactive approach where security is embedded in every phase. This cultural shift is the core of DevSecOps, aiming to balance development speed with system security. Phases of a Secure DevOps Lifecycle A comprehensive, seven-phase approach to embedding security throughout the pipeline:\nPlan: Define security requirements and perform threat modeling. Code: Use static analysis (SAST) tools to detect vulnerabilities early. Build: Automate security checks, dependency scans, and configuration validation. Test: Integrate penetration testing and compliance auditing. Deploy: Scan Infrastructure as Code (IaC) to ensure secure environments. Operate: Automate patching, incident response, and remediation. Monitor: Use real-time analytics and AI-powered anomaly detection for proactive defense. Leveraging AI in the DevOps Toolchain Automation: AI automates repetitive tasks like code review, log analysis, vulnerability scanning, and filtering false positives. Enhanced Security: AI-driven tools can prioritize critical risks, suggest fixes, and detect anomalous behavior in runtime environments. Efficiency: AI assists in generating documentation, reports, and compliance policies, reducing manual workload. Tooling Examples: The session highlighted tools like SonarQube, Checkov, Prometheus, and GitHub Actions, along with AI\u0026rsquo;s role in enhancing their capabilities. AWS Tools for AI-Enhanced DevOps Amazon CodeGuru: A service demonstrated to scan code for vulnerabilities (e.g., SQL injection, secret leaks) and provide actionable recommendations for fixes. AWS Managed Control Plane (MCP) \u0026amp; Base (MCB): Tools for automating security compliance and updates for Terraform and Kubernetes (EKS) configurations. Cost Optimization: AI/ML services like AWS Cost Anomaly Detection and Compute Optimizer help predict resource needs and reduce waste. Key Takeaways Security Mindset Proactive Integration: Always start with security in mind, embedding it into the earliest stages of planning and development, not as an afterthought. Shared Responsibility: Foster a culture where developers, operations, and security teams are collectively responsible for security. Continuous Improvement: Use feedback loops from monitoring and incidents to continuously refine security processes. Technical Architecture Automated Security Pipeline: Embed automated security checks at every stage of the CI/CD pipeline, from code scanning to deployment. Observability: Implement robust monitoring, logging, and alerting systems (e.g., Prometheus, Grafana, Loki) to gain real-time insights into system health and security. Infrastructure as Code (IaC) Security: Utilize tools to scan IaC configurations to prevent misconfigurations before they reach production. AI Integration Strategy Phased Approach: Select and adopt AI tools that fit specific project needs to avoid performance overhead and unnecessary complexity. Human-in-the-Loop: View AI as a powerful assistant that enhances human capabilities, not as a replacement. Human oversight and judgment remain critical. Measure ROI: The integration of AI should be measured by its ability to increase development velocity, improve security posture, and reduce manual effort. Applying to Work Enhance CI/CD: Integrate automated static analysis (SAST) and dependency scanning tools into current pipelines. Adopt IaC Scanning: Implement tools like Checkov to validate Terraform or other IaC scripts. Pilot AWS AI Tools: Experiment with Amazon CodeGuru on a small-scale project to review code quality and security. Improve Monitoring: Leverage AI-powered anomaly detection to get proactive alerts on potential issues. Automate Documentation: Use AI to assist in generating and maintaining project documentation and reports. Event Experience Attending the \u0026ldquo;Reinventing DevOps with AWS Generative AI\u0026rdquo; session was highly valuable, offering a comprehensive overview of how AI is reshaping security and efficiency in software development. Key experiences included:\nLearning from highly skilled speakers Experts from CMC Global and AWS Vietnam shared deep insights from their extensive experience in cloud and DevOps. Through real-world case studies from clients in the Philippines and Singapore, I gained a practical understanding of implementing secure CI/CD pipelines. Hands-on technical exposure The live demonstration of Amazon CodeGuru was particularly insightful, showing how AI can concretely identify vulnerabilities and suggest code fixes in real-time. Leveraging modern tools Explored a modern DevOps toolchain, including SonarQube, Checkov, Prometheus, and GitLab CI, and understood how AI integrates with them. Learned how to use AI for infrastructure management and compliance with tools like AWS MCP and MCB. Networking and discussions The interactive Q\u0026amp;A session offered a chance to dive deeper into specific topics, such as the evolution of DevOps roles, AI\u0026rsquo;s limitations, and career advice for cloud architects. Discussions reinforced the importance of balancing AI automation with human expertise and critical thinking. Lessons learned Shifting to a DevSecOps culture is essential for building secure and reliable applications at speed. AI tools like Amazon CodeGuru can significantly boost productivity and security, but they require human oversight to verify and implement suggestions effectively. Modernization requires a clear strategy; a phased approach to adopting new tools and processes is less risky and more effective. Overall, the event provided not only technical knowledge but also reshaped my thinking about the future of DevOps, the indispensable role of integrated security, and the collaborative potential between AI and human engineers.\n"
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Test the Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "
},
{
	"uri": "http://localhost:1313/internship_report/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog I have translated:\nBlog 1 - How We Built a Flywheel to Steadily Improve Security for Amazon RDS This blog details the process an AWS security team undertook to secure a new feature, PL/Rust, on Amazon Relational Database Service (Amazon RDS). The author, a principal security engineer, explains how the team moved beyond a simple implementation to build a comprehensive, self-improving security system—a \u0026ldquo;flywheel\u0026rdquo;—that combines technology, process, and testing to protect customers.\nBlog 2 - Create an SSL connection to Amazon RDS for Db2 in Java without KeyStore or Keytool This blog outlines a simplified method for establishing a secure SSL database connection in Java, specifically for Amazon Relational Database Service (Amazon RDS) for Db2. The approach allows developers to bypass the traditional complexities associated with the keytool utility and the management of Java KeyStores. The primary benefits of this technique include its simplicity, its suitability for automated environments like CI/CD pipelines, and its ability to maintain strong security through proper TLS 1.2 negotiation and server certificate validation.\nBlog 3 - Enhance the local testing experience for serverless applications with LocalStack This blog announces and explains new capabilities designed to simplify the local testing experience for serverless applications. Through an integration with AWS Partner, LocalStack, the AWS Toolkit for Visual Studio Code now provides a more streamlined way for developers to build, test, and debug their serverless applications without leaving their development environment.\n"
},
{
	"uri": "http://localhost:1313/internship_report/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Complete foundational AWS hands-on labs (Site-to-Site VPN, EC2 fundamentals). Progress through and finish all four modules of the AWS Cloud Technical Essentials course. Strengthen practical usage of AWS Console and CLI (configuration, key management, region/service discovery). Collaborate with product/design to review and document TypeRush Figma UI/UX for future implementation. Evaluate storage options and decide on a scalable NoSQL approach for TextService data. Prototype and integrate MongoDB (environment setup, seeding, service refactor, validation). Build effective communication cadence with First Cloud Journey team members. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Lab 03: AWS Site-to-Site VPN: + Create a complete Site-to-Site VPN environment, including a new VPC, an EC2 instance to act as a customer gateway, a Virtual Private Gateway, and the VPN connection itself. + Configure and test the VPN tunnel connectivity. - Lab 04: Amazon EC2 Fundamentals: + Launch and connect to both Microsoft Windows Server and Amazon Linux EC2 instances. + Deploy a sample \u0026ldquo;AWS User Management\u0026rdquo; application on both Windows and Linux environments to practice basic CRUD operations. + Explore core EC2 features like modifying instance types, managing EBS snapshots, and creating custom AMIs. 09/22/2025 09/22/2025 VPN Lab (Lab 03): https://000003.awsstudygroup.com/ EC2 Lab (Lab 04): https://000004.awsstudygroup.com/ 3 - Getting start with AWS Cloud Technical Essentials course, covering 2 modules: + Module 1: Cloud Foundations \u0026amp; IAM - Define cloud computing and its value proposition. - Differentiate between on-premises and cloud workloads. - Create an AWS account and review different methods for interacting with AWS services. - Describe the AWS Global Infrastructure, including Regions and Availability Zones. - Learn and apply best practices for AWS Identity and Access Management (IAM). + Module 2: Compute \u0026amp; Networking - Review the basic components of Amazon EC2 architecture. - Differentiate between containers and virtual machines. - Discover the features and advantages of serverless technologies. - Learn basic networking concepts and the features of Amazon Virtual Private Cloud (VPC). - Create a VPC. 09/23/2025 09/23/2025 AWS Cloud Technical Essentials: https://www.coursera.org/learn/aws-cloud-technical-essentials 4 - Collaborate and Document Figma Design for TypeRush UI/UX: + Participate in a dedicated cross-functional design review meeting to comprehensively examine the latest Figma mockups provided by the UI/UX team. + Systematically analyze each key screen (e.g., login/registration, main game interface, post-game score summary, settings menu) to grasp the visual hierarchy, component states, and intended user interactions. + Compile a list of initial technical feasibility questions and potential UI implementation considerations to facilitate further discussion and alignment between design and engineering. + Begin translating key design elements into preliminary front-end component requirements or user stories, setting the groundwork for future development sprints. - Discuss Text Service Storage with Team Lead \u0026amp; Confirm NoSQL Choice: + Engage in a focused discussion with the team leader regarding the optimal database solution for the TextService (e.g., for storing words and sentences). + Present the pros and cons of relational (SQL) vs. non-relational (NoSQL) options in the context of our data structure and anticipated access patterns. + Confirm the decision to proceed with a NoSQL database as the chosen storage solution due to its flexibility and scalability for text content. 09/24/2025 09/24/2025 5 - Integrate and Test MongoDB with TextService Prototype: + Set up MongoDB Environment: Set up a local instance using Docker. + Refactor Data Seeding Script: Modify the population script to insert the words and sentences into MongoDB collections as documents. + Rewrite Service Logic: Update the TextService data retrieval methods to query the MongoDB collections instead of the previous data source. + Verify Integration: Thoroughly test the refactored service to confirm that it can successfully connect, write to, and read from the MongoDB database. 09/25/2025 09/25/2025 6 - Completing AWS Cloud Technical Essentials course, with 2 final modules: + Module 3: Storage \u0026amp; Databases - Differentiate between file, block, and object storage models. - Explain core Amazon S3 concepts like buckets and objects, then create an S3 bucket. - Describe the function and use cases of Amazon EBS with EC2. - Explore the various database services available on AWS. - Understand the function of Amazon DynamoDB and create a DynamoDB table. + Module 4: Monitoring \u0026amp; High Availability - Define the benefits of monitoring and the role of Amazon CloudWatch. - Understand how to optimize solutions for performance and cost on AWS. - Describe the function of Elastic Load Balancing (ELB) to route and distribute traffic. - Differentiate between vertical scaling (scaling up) and horizontal scaling (scaling out). - Configure a solution for high availability. 09/26/2025 09/26/2025 AWS Cloud Technical Essentials: https://www.coursera.org/learn/aws-cloud-technical-essentials Week 3 Achievements: AWS Infrastructure Labs:\nBuilt full Site-to-Site VPN (VPC, customer gateway EC2, virtual private gateway, tunnel validation) Executed EC2 fundamentals (Windows \u0026amp; Linux instances, CRUD app deployment, snapshots, custom AMIs) Finished all four modules of AWS Cloud Technical Essentials (Foundations/IAM, Compute \u0026amp; Networking, Storage \u0026amp; Databases, Monitoring \u0026amp; High Availability).\nAWS Console \u0026amp; CLI Proficiency:\nAccount setup, credential \u0026amp; config management Region \u0026amp; service exploration Key pair handling and resource inspection workflows TypeRush UI/UX Documentation:\nAnalyzed Figma screens (login, game, score summary, settings) and navigation flows Captured component states \u0026amp; feasibility questions Drafted initial user stories / component requirements TextService Storage Architecture:\nEvaluated SQL vs NoSQL trade*offs for word/sentence storage Selected NoSQL approach for flexibility \u0026amp; scalability MongoDB Integration Prototype:\nDockerized local MongoDB environment Converted seeding script to insert documents Refactored service data layer to query MongoDB Validated read/write operations end-to-end "
},
{
	"uri": "http://localhost:1313/internship_report/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "During my internship, I participated in [x] events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nKick-off AWS FCJ Workforce Event Name: Kick-off AWS FCJ Workforce - FPTU OJT FALL 2025\nDate \u0026amp; Time: 08:30, September 06, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nGist: Vision talks on AWS careers (Cloud, DevOps, AI/ML, Security, Data), alumni sharing, networking, and interactive Q\u0026amp;A.\nOutcomes and value gained: Mindset reset to prioritize long-term growth; clearer view of DevOps/cloud paths; action to engage AWS Builders community and share takeaways with the team.\nData Science On AWS Event Name: Data Science On AWS\nDate \u0026amp; Time: 09:30, October 16, 2025\nLocation: FPT University HCMC, Hi-Tech Park, Thu Duc, Ho Chi Minh City\nRole: Attendee\nGist: End-to-end Data Science pipeline on AWS (S3 → Glue → SageMaker), live demos (IMDb ETL, Sentiment model), and overview of managed AI services (Transcribe, Comprehend, Rekognition, Personalize).\nOutcomes and value gained: New skills with Glue ETL and SageMaker training/deploy; stronger data-first mindset; contribution plan to automate ETL, pilot sentiment analysis, and streamline MLOps.\nReinventing DevOps with AWS Generative AI Event Name: Reinventing DevOps with AWS Generative AI\nDate \u0026amp; Time: 19:30, October 16, 2025\nLocation: Online at CMC Global on Microsoft Teams\nRole: Attendee\nGist: DevOps → DevSecOps evolution with a seven-phase secure lifecycle, AI-augmented toolchain, and live demo of Amazon CodeGuru; emphasis on IaC security and observability.\nOutcomes and value gained: New skills in SAST/dependency scans, IaC scanning (e.g., Checkov), and AI-powered anomaly detection; contribution plan to add CI/CD security gates, pilot CodeGuru, and enhance monitoring/documentation.\nGenerative AI with Amazon Bedrock Event Name: Generative AI with Amazon Bedrock\nDate \u0026amp; Time: 8:30, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nGist: High-level overview of Generative AI on AWS with Amazon Bedrock, using Foundation Models, advanced Prompt Engineering (Zero-Shot, Few-Shot, Chain-of-Thought), and RAG with Amazon Titan and vector databases, plus an introduction to Agentic AI and the new Amazon Bedrock AgentCore for production-ready AI Agents.\nOutcomes and value gained: Clear understanding of how to build RAG-powered applications, design better prompts, and move from AI prototypes to secure, scalable production systems, with concrete next steps using aws-samples/amazon-bedrock-samples and Bedrock_AgentCore to build internal AI assistants grounded in private data.\nDevOps on AWS Event Name: DevOps on AWS\nDate \u0026amp; Time: 8:30, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nGist: End-to-end DevOps on AWS: Infrastructure as Code (IaC) with AWS CloudFormation and AWS CDK, CI/CD pipelines using AWS CodeCommit, CodeBuild, CodeDeploy, and CodePipeline, containerization with Docker and the AWS container ecosystem (Amazon ECR, ECS, EKS, AWS Fargate, App Runner), plus monitoring and observability via Amazon CloudWatch and AWS X-Ray.\nOutcomes and value gained: Stronger DevOps mindset and practical know-how to replace manual \u0026ldquo;ClickOps\u0026rdquo; with automated IaC, standardize CI/CD, and adopt container-first microservices, with clear actions to codify current infrastructure, containerize applications, and improve observability using CloudWatch dashboards and X-Ray traces.\n"
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.4-s3-onprem/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "On-premises DNS Simulation",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"
},
{
	"uri": "http://localhost:1313/internship_report/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Learn to configure Amazon RDS databases with VPC setup, security groups, and backup operations. Study scalable web application deployment using Auto Scaling Groups and Application Load Balancers. Understand monitoring techniques with CloudWatch metrics, logs, alarms, and dashboards. Explore hybrid DNS solutions using Route 53 Resolver for enterprise networking. Develop AWS CLI skills for automated resource management across S3, EC2, VPC, and IAM. Practice building CI/CD pipelines with CodeCommit, CodeBuild, CodeDeploy, and CodePipeline. Implement automated backup strategies using AWS Backup with lifecycle policies. Learn containerized application deployment with Docker on AWS and container registries. Study VM migration workflows including import/export operations between environments. Build serverless applications using AWS Lambda and API Gateway with IAM configuration. Understand security monitoring using AWS Security Hub with multi-service integration. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Configure and Manage a Relational Database with Amazon RDS: + Create a VPC, security groups for EC2 and RDS, and a DB subnet group + Launch an EC2 instance and create an RDS database instance + Deploy a sample application on the EC2 instance to connect to the RDS database + Perform backup and restore operations for the RDS instance + Practice resource cleanup by deleting the created resources - Deploy and configure a scalable web application using Auto Scaling: + Set up the necessary network infrastructure, including a VPC, subnets, and security groups + Create an EC2 Launch Template to define the configuration for new instances + Configure a Target Group and an Application Load Balancer to manage and distribute incoming traffic + Create and configure an Auto Scaling Group to automatically manage the number of EC2 instances + Test various scaling solutions, including manual, scheduled, and dynamic scaling policies + Clean up all created AWS resources to avoid ongoing charges - Monitor and Analyze AWS Resources with CloudWatch: + Explore and analyze metrics from various AWS services using search and math expressions + Investigate and query log data using CloudWatch Logs Insights + Create a Metric Filter to extract data from log events for monitoring + Configure a CloudWatch Alarm to trigger notifications based on a specific metric threshold + Build a custom CloudWatch Dashboard to visualize key metrics and alarms + Perform a cleanup of all created resources, including alarms and dashboards 29/09/2025 29/09/2025 Amazon RDS: https://000005.awsstudygroup.com/ Auto Scaling Group: https://000006.awsstudygroup.com/ CloudWatch Metrics https://000008.awsstudygroup.com/ 3 - Implement a Hybrid DNS Solution with Route 53 Resolver: + Prepare the lab environment by deploying foundational infrastructure using a CloudFormation template + Deploy a Microsoft Active Directory instance to simulate an on-premises DNS server + Create a Route 53 Resolver outbound endpoint to forward DNS queries from the VPC + Configure Route 53 Resolver rules to direct DNS queries to the appropriate resolver + Set up a Route 53 Resolver inbound endpoint to allow the on-premises network to query VPC resources + Test the hybrid DNS name resolution and then clean up all deployed resources - Manage AWS Services using the Command Line Interface (CLI): + Install and configure the AWS CLI with an access key, secret key, and default region + Practice using the CLI to view and describe resources in various services like S3, SNS, and IAM + Perform Amazon S3 operations, such as creating buckets and managing objects, via the command line + Create and manage VPC components, including an Internet Gateway, using CLI commands + Launch, describe, and terminate an Amazon EC2 instance entirely from the command line + Clean up all resources created during the lab to avoid incurring costs 30/09/2025 30/09/2025 Route 53 Resolver: https://cloudjourney.awsstudygroup.com/ AWS CLI: https://000011.awsstudygroup.com/ 4 - Build a CI/CD Pipeline to Automate Application Deployment: + Create a version control repository using AWS CodeCommit to store the application\u0026rsquo;s source code + Configure a build project with AWS CodeBuild to compile, test, and package the application + Set up a deployment group and application in AWS CodeDeploy to manage the deployment process + Create a unified CI/CD pipeline with AWS CodePipeline to orchestrate the entire workflow + Test the end-to-end pipeline by pushing a code change and verifying the automated deployment + Clean up all AWS resources created during the lab to avoid unnecessary charges - Automate EC2 Instance Backups with AWS Backup: + Deploy the necessary infrastructure, including a new VPC and an EC2 instance, using a CloudFormation template + Create a backup plan in AWS Backup to define backup frequency, retention policies, and lifecycle rules + Configure notification settings to receive alerts on the status of backup jobs + Test the backup and restore process to ensure data can be successfully recovered + Clean up all resources, including the CloudFormation stack and any created backups 01/10/2025 01/10/2025 AWS IAM Identity Center: https://000012.awsstudygroup.com/ AWS Backup: https://000013.awsstudygroup.com/ 5 - Deploy a Dockerized Application on AWS: + Configure the necessary AWS infrastructure, including a VPC, security groups, and IAM roles + Launch an Amazon RDS instance to serve as the application\u0026rsquo;s database + Set up an EC2 instance and install the required dependencies for running the application + Deploy and test the application on the EC2 instance using a Docker image + Redeploy the application using Docker Compose to manage multiple containers + Push the Docker image to a container registry, such as Amazon ECR or Docker Hub + Clean up all created AWS resources to avoid incurring further charges - Migrate a Virtual Machine to and from AWS: + Export a virtual machine from an on-premises environment + Upload the exported virtual machine image to an Amazon S3 bucket + Import the virtual machine from S3 into AWS to create a new Amazon Machine Image (AMI) + Deploy a new EC2 instance from the imported AMI + Export an existing EC2 instance back to an S3 bucket + Clean up all created resources, including the S3 bucket and EC2 instance 02/10/2025 02/10/2025 Docker on AWS: https://000015.awsstudygroup.com/ VM Import/Export: https://000014.awsstudygroup.com/ 6 - Deploy a Serverless Application with Lambda and API Gateway: + Prepare and zip the Lambda deployment package, ensuring it includes the function\u0026rsquo;s source code and all required dependencies + Define and create an IAM role with the necessary execution permissions and a trust policy that allows Lambda to assume the role + Create the Lambda function in the AWS Console, specifying the runtime and uploading the zipped deployment package + Build an HTTP API endpoint using Amazon API Gateway and configure its integration to invoke the Lambda function + Deploy the API Gateway to a stage and test the end-to-end serverless application by accessing the public URL + Clean up all associated resources, including the API Gateway, the Lambda function, and the IAM role, to prevent ongoing costs - Aggregate and Prioritize Security Findings with AWS Security Hub: + Enable AWS Security Hub to begin aggregating security data from various AWS services + Review the integrated dashboard to visualize security alerts and findings + Organize and prioritize security detections from services like Amazon GuardDuty, Inspector, and Macie + Explore the summarized risks presented in interactive charts and tables 03/10/2025 03/10/2025 AWS Lambda: https://000016.awsstudygroup.com/ AWS Security Hub: https://000018.awsstudygroup.com/ Week 4 Achievements: Successfully configured and managed Amazon RDS databases:\nVPC setup with security group configuration DB subnet group creation and EC2-RDS integration Application deployment with database connectivity Backup and restore operations implementation Deployed scalable web applications using Auto Scaling infrastructure:\nNetwork infrastructure setup with VPC, subnets, and security groups EC2 Launch Template configuration for standardized instances Application Load Balancer and Target Group implementation Auto Scaling Group configuration with scaling policies Implemented comprehensive monitoring with CloudWatch services:\nMetrics exploration using search and math expressions Log data analysis with CloudWatch Logs Insights Custom alarm configuration with notification thresholds Interactive dashboard development for visualization Configured hybrid DNS solutions using Route 53 Resolver:\nCloudFormation-based infrastructure deployment Microsoft Active Directory integration for simulation Outbound and inbound endpoint configuration Bidirectional name resolution testing Developed AWS CLI proficiency for automated resource management:\nCLI installation and configuration with access credentials Multi-service resource management (S3, SNS, IAM) S3 operations including bucket and object management EC2 instance lifecycle management through CLI Built automated CI/CD pipelines using AWS developer services:\nVersion control repository setup with AWS CodeCommit Build project configuration with AWS CodeBuild Deployment automation using AWS CodeDeploy End-to-end pipeline orchestration with CodePipeline Implemented automated backup strategies with AWS Backup:\nInfrastructure deployment using CloudFormation templates Backup plan creation with lifecycle policies Notification configuration for backup job monitoring Disaster recovery testing through backup processes Deployed containerized applications using Docker on AWS:\nAWS infrastructure configuration including VPC and IAM roles RDS database integration for containerized applications Docker image deployment and testing on EC2 instances Container registry operations with Amazon ECR Completed VM migration workflows between environments:\nVirtual machine export from on-premises systems S3-based image storage and transfer operations VM import processes for AMI creation EC2 instance deployment from imported images Built serverless applications using AWS Lambda and API Gateway:\nLambda deployment package preparation and configuration IAM role creation with execution permissions Function deployment and runtime configuration API Gateway endpoint creation and Lambda integration Configured centralized security monitoring with AWS Security Hub:\nSecurity Hub enablement for multi-service data aggregation Dashboard utilization for security alert visualization Security finding organization from GuardDuty, Inspector, and Macie Risk analysis using interactive charts and tables "
},
{
	"uri": "http://localhost:1313/internship_report/4-eventparticipated/4.5-event5/",
	"title": "DevOps on AWS",
	"tags": [],
	"description": "",
	"content": "Summary Report Event Objectives Instill the DevOps mindset, covering its culture, principles, and key performance metrics. Provide a deep dive into building CI/CD pipelines using native AWS DevOps services. Teach Infrastructure as Code (IaC) principles using AWS CloudFormation and the AWS CDK. Explore the AWS container ecosystem, including Docker, Amazon ECR, ECS, EKS, and App Runner. Demonstrate how to implement comprehensive monitoring and observability using CloudWatch and AWS X-Ray. Speakers Truong Quang Tinh - Platform Engineer (TymeX), AWS Community Builder Bao Huynh - AWS Community Builder Thinh Nguyen - AWS Community Builder Vi Tran - AWS Community Builder Van Hoang Kha – Cloud Engineer, AWS Community Builder Long Huynh - AWS Community Builder Quy Pham - AWS Community Builder Nghiem Le - AWS Community Builder Key Highlights From Manual Operations to Infrastructure as Code (IaC) The workshop highlighted the pitfalls of \u0026ldquo;ClickOps\u0026rdquo; (manual console-based management), such as being slow, error-prone, and difficult to replicate. AWS CloudFormation: Introduced as the native IaC solution, using YAML/JSON templates to define and manage AWS resources in \u0026ldquo;Stacks\u0026rdquo; and its ability to detect configuration drift. AWS Cloud Development Kit (CDK): Presented as a developer-centric IaC framework that allows defining infrastructure in familiar programming languages (e.g., Python, TypeScript), using reusable \u0026ldquo;Constructs\u0026rdquo; to accelerate development. Building a Full CI/CD Pipeline A complete, automated pipeline was demonstrated using the suite of AWS developer tools: AWS CodeCommit: For secure source control. AWS CodeBuild: For automated builds and testing. AWS CodeDeploy: For managing complex deployments like Blue/Green and Canary releases. AWS CodePipeline: To orchestrate the entire release process from source to deployment. Containerization and Orchestration The session covered the fundamentals of containerization with Docker and the importance of a container registry like Amazon ECR for storing and scanning images. A detailed comparison of orchestration services was provided: Amazon ECS: An AWS-native, simpler solution deeply integrated with the AWS ecosystem, ideal for teams wanting lower operational overhead. Amazon EKS: A managed Kubernetes service that aligns with the open-source standard, offering greater flexibility and multi-cloud portability at the cost of higher complexity. AWS Fargate \u0026amp; App Runner: Serverless compute options that remove the need to manage underlying servers for containers, simplifying deployment and operations. Monitoring and Observability The importance of full-stack observability was emphasized for maintaining and debugging distributed systems. Amazon CloudWatch: Used for collecting metrics, logs, and setting up alarms and dashboards. AWS X-Ray: Demonstrated for distributed tracing to analyze and debug performance bottlenecks in microservices architectures. Key Takeaways Design Mindset Automate Everything: Transition from manual \u0026ldquo;ClickOps\u0026rdquo; to a fully automated IaC approach to ensure consistency, speed, and reliability. Infrastructure as Code is Non-Negotiable: IaC is the foundation for modern DevOps, enabling collaboration, versioning, and reproducibility of environments. Choose the Right Tool for the Team: The choice between CloudFormation, CDK, ECS, and EKS should be based on team skills, ecosystem needs, and the desired balance between simplicity and control. Technical Architecture CI/CD Pipelines: Every project should have an automated pipeline that handles code integration, testing, and deployment to ensure rapid and safe releases. Container-First for Microservices: Use containers to package applications and their dependencies, and an orchestrator (ECS or EKS) to manage them at scale. Full-Stack Observability: Implement a robust monitoring strategy with metrics, logs (CloudWatch), and distributed tracing (X-Ray) to gain deep insights into application performance and health. Modernization Strategy Phased Adoption: Introduce DevOps practices incrementally. Start by converting one manual process to IaC or building a CI/CD pipeline for a single service. Leverage Serverless: Use serverless options like AWS Fargate and App Runner to reduce operational complexity and allow teams to focus on application logic rather than infrastructure management. Measure What Matters: Focus on key DevOps metrics like Deployment Frequency, Lead Time for Changes, and Mean Time to Recovery (MTTR) to drive continuous improvement. Applying to Work Automate a Deployment: Convert a manually deployed application to use an AWS CodePipeline workflow. Codify Infrastructure: Define an existing S3 bucket or EC2 instance using an AWS CloudFormation template or a CDK application. Containerize an Application: Create a Dockerfile for a web application and push the image to Amazon ECR. Pilot a Container Service: Deploy a simple containerized application using AWS App Runner or ECS with the Fargate launch type. Improve Observability: Create a CloudWatch Dashboard for a critical application and configure alarms for key metrics like CPU utilization and error rates. Event Experience Attending the \u0026ldquo;DevOps on AWS\u0026rdquo; workshop was extremely valuable, offering a comprehensive and practical guide to implementing modern DevOps practices on the cloud.\nLearning from highly skilled speakers The AWS Community Builders provided deep, practical knowledge, breaking down complex topics into understandable concepts. Hands-on technical exposure The multiple live demos, including a full CI/CD pipeline walkthrough and a microservices deployment on ECS, provided a clear, real-world context for the tools and services discussed. Leveraging modern tools The workshop provided a thorough exploration of the modern AWS DevOps toolkit, from advanced IaC with the CDK to serverless containers with App Runner. Networking and discussions The Q\u0026amp;A sessions offered opportunities to discuss career pathways and the AWS certification roadmap, providing valuable guidance for professional development. Lessons learned Adopting IaC is the single most impactful step towards achieving a mature DevOps practice. AWS provides a complete, integrated toolset to build a sophisticated DevOps platform, with options suitable for teams of all sizes and skill levels. Effective monitoring and observability are not optional; they are critical for operating reliable and performant applications in the cloud. Some event photos "
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"
},
{
	"uri": "http://localhost:1313/internship_report/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Implement advanced AWS networking with VPC Peering and Transit Gateway for multi-VPC connectivity. Deploy full-stack applications using EC2, RDS, Auto Scaling, and CloudFront integration. Create serverless cost optimization solutions with AWS Lambda for automated EC2 management. Build CI/CD pipelines using AWS Developer Tools for automated application deployment. Configure hybrid cloud storage with AWS Storage Gateway for on-premises integration. Manage enterprise file systems with Amazon FSx and secure web applications using AWS WAF. Organize AWS resources effectively using Tags and Resource Groups for governance. Develop proficiency with both AWS Management Console and CLI operations. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Set up VPC Peering between two VPCs: + Initialize CloudFormation Template to create the initial environment + Create Security Groups to control traffic to the EC2 instances + Create EC2 instances in each VPC for testing connectivity + Update Network ACLs to allow traffic between the peered VPCs + Create and accept the VPC Peering connection + Configure Route Tables to direct traffic between the peered VPCs + Enable Cross-Peer DNS for name resolution between VPCs - Implement a scalable network architecture with AWS Transit Gateway: + Generate a Key Pair for secure access to instances + Initialize the environment using a CloudFormation Template + Create a Transit Gateway to act as a central network hub + Attach VPCs to the Transit Gateway to enable communication + Configure Transit Gateway Route Tables to control traffic flow + Update VPC Route Tables to route traffic through the Transit Gateway 10/06/2025 10/06/2025 VPC Peering: https://000019.awsstudygroup.com/ AWS Transit Gateway: https://000020.awsstudygroup.com/ 3 - Deploy WordPress on AWS Cloud: + Prepare VPC and Subnets for the network infrastructure + Create Security Groups for EC2 and Database instances to control access + Launch an EC2 instance to host the WordPress application + Launch a Database instance using Amazon RDS for the WordPress database + Install and configure WordPress on the EC2 instance + Implement Auto Scaling for the WordPress instance + Perform database backup and restore operations + Set up CloudFront for the web server to improve performance and security - Optimize EC2 Costs with Lambda: + Create tags for EC2 instances to identify them for cost optimization + Create an IAM Role for the Lambda function to grant necessary permissions + Create a Lambda function to automatically stop and start EC2 instances + Test the Lambda function to ensure it is working correctly 10/07/2025 10/07/2025 WordPress on AWS: https://000021.awsstudygroup.com/ Lambda Cost Optimization: https://000022.awsstudygroup.com/ 4 - Automate Application Deployment with CI/CD Pipeline: + Prepare the necessary resources for the CI/CD pipeline + Install and configure the CodeDeploy Agent on the EC2 instances + Create a CodeCommit repository to store the application source code + Configure CodeBuild to compile and build the application + Set up CodeDeploy to automate the deployment process + Create a CodePipeline to orchestrate the entire CI/CD workflow + Troubleshoot any issues that may arise during the pipeline execution - Utilize AWS Storage Gateway for hybrid cloud storage: + Create an S3 Bucket to store data in the cloud + Set up an EC2 instance to host the Storage Gateway + Create a Storage Gateway to connect on-premises environments with AWS storage + Create File Shares on the Storage Gateway for file-based access + Mount the File Shares on an on-premises machine to access cloud storage 10/08/2025 10/08/2025 AWS CodePipeline: https://000023.awsstudygroup.com/ Storage Gateway: https://000024.awsstudygroup.com/ 5 - Manage Amazon FSx for Windows File Server: + Create the initial environment for the file server + Create both SSD and HDD Multi-AZ file systems + Create new file shares on the file systems + Test and monitor the performance of the file systems + Enable data deduplication and shadow copies for storage optimization and data protection + Manage user sessions, open files, and user storage quotas + Enable Continuous Access share for high availability + Scale throughput and storage capacity as needed + Delete the environment to clean up resources + Reference the AWS CLI for managing the file server - Implement AWS Web Application Firewall (WAF): + Create an S3 Bucket and deploy a sample web application for testing + Use AWS WAF with managed rules to protect against common threats + Create custom and advanced custom rules for specific security needs + Test new rules to ensure they are working correctly + Log requests for monitoring and analysis + Clean up all created resources to avoid ongoing charges 10/09/2025 10/09/2025 Amazon FSx: https://000025.awsstudygroup.com/ AWS WAF: https://000026.awsstudygroup.com/ 6 - Manage AWS resources using Tags and Resource Groups: + Understand and use tags on the AWS Management Console + Create an EC2 instance with tags + Add or remove tags from existing resources + Filter resources based on tags + Learn to use tags with the AWS Command Line Interface (CLI) + Add tags to an existing EC2 resource using the CLI + Add tags to a new resource during creation using the CLI + Describe tagged resources using the CLI + Create and manage a Resource Group to organize AWS resources + Create a tag-based Resource Group + View and manage resources within a Resource Group 10/10/2025 10/10/2025 AWS Tags \u0026amp; Resource Groups: https://000027.awsstudygroup.com/ Week 5 Achievements: Successfully implemented advanced AWS networking solutions:\nVPC Peering for direct inter-VPC communication Transit Gateway for centralized network hub architecture Cross-VPC DNS resolution and routing configuration Deployed and optimized full-stack cloud applications:\nWordPress application with RDS database integration Auto Scaling and CloudFront for performance and availability Serverless cost optimization using Lambda functions Built comprehensive DevOps automation workflows:\nEnd-to-end CI/CD pipelines with AWS Developer Tools Automated deployment processes with CodePipeline integration Hybrid cloud storage solutions using Storage Gateway Configured enterprise-grade file systems and security:\nAmazon FSx for Windows File Server with Multi-AZ deployment AWS WAF implementation with custom security rules Performance monitoring and storage optimization techniques Established effective AWS resource management practices:\nResource tagging strategies for cost tracking and organization Resource Groups for streamlined resource governance Proficient use of both AWS Console and CLI operations Gained hands-on experience with Infrastructure as Code using CloudFormation templates for consistent environment provisioning.\n"
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "
},
{
	"uri": "http://localhost:1313/internship_report/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Learn to manage EC2 access control using IAM services with resource tags and permission boundaries. Set up and configure monitoring tools including Grafana and AWS CloudWatch. Implement AWS Systems Manager for patch management and remote command execution. Optimize EC2 instances through right-sizing practices and AWS Compute Optimizer. Apply encryption to S3 data using AWS KMS and configure audit logging. Analyze AWS costs and usage patterns with Cost Explorer. Build a data pipeline and lake using S3, Kinesis, Glue, Athena, and QuickSight. Automate infrastructure provisioning with AWS CloudFormation templates. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Manage access to EC2 services with resource tags through IAM services: + Create an IAM user for preparation + Create a custom IAM Policy to define specific permissions + Set up an IAM Role to be assumed by users or services + Verify the policy by switching roles and testing access - Getting started with Grafana basic: + Create a VPC and subnet to establish a network environment + Configure a Security Group to control inbound and outbound traffic + Launch an EC2 instance to host the monitoring application + Create an IAM User and Role for secure access to AWS resources + Assign the IAM Role to the EC2 instance + Install Grafana on the EC2 instance + Set up monitoring dashboards within Grafana 10/13/2025 10/13/2025 IAM services: https://000028.awsstudygroup.com/ Grafana basic: https://000029.awsstudygroup.com/ 3 - Limit user permissions with IAM Permission Boundary: + Perform preparatory steps for the exercise + Create a restriction policy to define the maximum allowable permissions + Create a new IAM user with limited permissions + Test the IAM user\u0026rsquo;s limits to verify the permission boundaries - Manage patches and run commands on multiple servers with AWS System Manager: + Create a VPC and Subnet for the network environment + Launch a public Windows EC2 instance + Create an IAM Role with necessary permissions + Assign the IAM Role to the EC2 instance + Configure and use Patch Manager to handle server patching + Use Run Command to execute commands on the servers 10/14/2025 10/14/2025 IAM permission boundary: https://000030.awsstudygroup.com/ AWS Systems Manager: https://000031.awsstudygroup.com/ 4 - Implement right-sizing practices for Amazon EC2: + Get acquainted with Amazon CloudWatch for monitoring + Create and attach an IAM Role for the CloudWatch Agent + Install the CloudWatch Agent on an EC2 instance + Use AWS Compute Optimizer to analyze and optimize EC2 configurations - Encrypt data at rest in S3 using AWS KMS: + Create necessary IAM policies, roles, groups, and users + Set up a Key Management Service (KMS) key + Create an S3 bucket and upload data + Configure AWS CloudTrail for logging and Amazon Athena for querying data + Test and share encrypted data stored in S3 10/15/2025 10/15/2025 EC2 right-sizing: https://000032.awsstudygroup.com/ S3 encryption with KMS: https://000033.awsstudygroup.com/ 5 - Visualize and analyze costs with AWS Cost Explorer: + View cost and usage data by service and by account + Analyze the scope and effectiveness of Savings Plans and Reserved Instances + Evaluate cost elasticity + Create custom reports for EC2 instances + Use Cost Explorer for in-depth cost analysis + Review data transfer costs for common architectures - Build a data lake on AWS: + Create an IAM Role and Policy for necessary permissions + Set up an S3 bucket for data storage + Create a Kinesis Data Firehose delivery stream for data collection + Use a Glue Crawler to create a data catalog + Perform data transformation + Analyze data using Amazon Athena + Visualize data with Amazon QuickSight 10/16/2025 10/17/2025 AWS Cost Explorer: https://000034.awsstudygroup.com/ Data lake on AWS: https://000035.awsstudygroup.com/ 6 - Study AWS CloudWatch for monitoring and observability: + Explore CloudWatch Metrics, including viewing, searching, and using expressions + Work with CloudWatch Logs, Logs Insights, and Metric Filters + Configure CloudWatch Alarms to trigger notifications + Create CloudWatch Dashboards for visualizing data - Automate infrastructure with AWS CloudFormation: + Create IAM Users and Roles for preparation + Develop a basic CloudFormation template to provision resources + Explore advanced features like Custom Resources with Lambda + Use Mappings, Stacksets, and Drift Detection for complex deployments 10/17/2025 10/17/2025 AWS CloudWatch: https://000036.awsstudygroup.com/ AWS CloudFormation: https://000037.awsstudygroup.com/ Week 6 Achievements: Successfully managed EC2 access control through IAM services:\nConfigured resource-based access policies using tags Implemented permission boundaries to limit user capabilities Set up monitoring and observability infrastructure:\nDeployed Grafana on EC2 for custom dashboards Configured CloudWatch Metrics, Logs, and Alarms for resource monitoring Implemented AWS Systems Manager capabilities:\nUsed Patch Manager for automated server updates Executed remote commands across multiple instances with Run Command Applied EC2 optimization and cost management practices:\nInstalled and configured CloudWatch Agent for detailed metrics Analyzed instance configurations using AWS Compute Optimizer Reviewed cost patterns and trends with Cost Explorer Secured data storage with encryption:\nCreated and managed KMS keys for S3 encryption Set up CloudTrail and Athena for audit logging and analysis Built a complete data lake pipeline:\nConfigured Kinesis Data Firehose for streaming data ingestion Created data catalogs with Glue Crawler Performed data analysis with Athena and visualization with QuickSight Automated infrastructure deployment with CloudFormation:\nDeveloped templates for resource provisioning Explored advanced features including Lambda-backed custom resources and Stacksets "
},
{
	"uri": "http://localhost:1313/internship_report/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Deepen practical skills with Amazon DynamoDB, including core operations, advanced design patterns, and global architectures. Configure identity federation, IAM roles, and cost optimization strategies across AWS and Azure AD. Deploy and manage applications using Lightsail, containers, Step Functions, and IAM roles for secure access. Use Cloud9, Elastic Beanstalk, and CI/CD tools to deploy and automate application delivery pipelines. Strengthen AWS security posture with foundational IAM practices, detective controls, incident response, and infrastructure protection. Build and automate microservices-based architectures using Lambda, DynamoDB, Step Functions, and CodeStar. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Explore Amazon DynamoDB Core Concepts + Create DynamoDB tables and load sample data + Use AWS CLI to read, query, scan, insert, and update data + Explore DynamoDB table data and Global Secondary Indexes using AWS Console + Perform backup and restore operations (Point-In-Time Recovery and on-demand backups) - Implement Advanced Design Patterns for DynamoDB + Analyze DynamoDB capacity units and partitioning + Compare sequential and parallel table scans for performance + Implement GSI write sharding, key overloading, and sparse GSIs + Use composite keys and adjacency lists for complex querying - Utilize Change Data Capture for Amazon DynamoDB + Configure and process data changes using DynamoDB Streams + Set up Lambda function to handle stream events + Implement change data capture using Kinesis Data Streams - Build a Global Serverless Application with Amazon DynamoDB + Deploy backend resources for serverless application + Configure and explore DynamoDB Global Tables for multi-region replication + Interact with sample global application interface - Model Game Player Data with Amazon DynamoDB + Plan data model based on entity relationships and access patterns + Design primary key and create table structure + Implement and query sparse Global Secondary Index to find open games + Add inverted index to retrieve past games for a user - Perform Cost and Performance Analysis with AWS Glue and Amazon Athena + Prepare and build database using AWS Glue crawlers + Use Amazon Athena to analyze cost and usage report data + Implement tagging strategies for cost allocation analysis 10/20/2025 10/20/2025 CDK basic: https://000038.awsstudygroup.com/ Amazon DynamoDB Immersion: https://000039.awsstudygroup.com/ Analysis with Glue and Athena: https://000040.awsstudygroup.com/ 3 - Configure IAM Federation from Microsoft Azure Active Directory + Prepare Azure environment by creating an account, initializing Active Directory, and creating users + Create and configure an Enterprise Application in Azure to connect with AWS + Set up an Identity Provider and corresponding IAM Roles in AWS for Azure AD to assume + Sync IAM Roles to Azure and provision user access to AWS + Test federated user access from Azure AD to the AWS console - Analyze and Optimize AWS Costs with Savings Plans and Reserved Instances + Understand the different types of Savings Plans and how they compare to Reserved Instances + Use AWS recommendations to identify opportunities for cost savings + Purchase a Savings Plan to reduce EC2 server costs + Explore Reserved Instance types and purchasing options + Review Reserved DB Instances for Amazon RDS - Perform Database Schema Conversion and Migration + Prepare the environment by creating a key pair and installing the Schema Conversion Tool on an EC2 instance + Configure the source database, either Oracle or SQL Server + Use the Schema Conversion Tool to convert the source schema to a format compatible with the target database + Create and configure a DMS replication instance, endpoints, and migration task + Execute the data migration and replicate ongoing data changes + Explore and test DMS Serverless for automated scaling during migration + Monitor DMS migrations using CloudWatch, event notifications, and task logs - Configure IAM Roles and Conditions + Create an IAM Group and IAM User to manage permissions + Configure an Admin IAM Role and set up a switch role + Restrict role access by applying conditions based on IP address and time - Deploy and Manage Applications with Amazon Lightsail + Deploy a database and a WordPress instance on Lightsail + Configure networking and application settings for a WordPress deployment + Deploy Prestashop and Akaunting instances + Secure the application, create a snapshot for backup, and upgrade to a larger instance + Set up an alarm for resource monitoring 10/21/2025 10/21/2025 IAM Federation with Azure AD: https://000041.awsstudygroup.com/ AWS Cost Optimization: https://000042.awsstudygroup.com/ Database Migration with DMS: https://000043.awsstudygroup.com/ IAM Roles and Conditions: https://000044.awsstudygroup.com/ Amazon Lightsail Applications: https://000045.awsstudygroup.com/ 4 - Run Applications with Amazon Lightsail Containers + Create a container service in Lightsail + Deploy a public container image to the service + Create a Lightsail instance, configure the AWS CLI, and install Docker + Build and push a custom container image from the instance + Implement a new deployment using the custom image - Get Started with AWS Step Functions + Create a Cloud9 instance and deploy sample services + Initialize a Step Functions workflow to orchestrate Lambda functions using Task states + Implement branching logic with Choice states and manage state input/output + Introduce a pause in the workflow that waits for a token to resume + Implement error handling using retry and catch features + Perform parallel work using a Parallel state - Authorize Application Access to AWS Services with an IAM Role + Create an EC2 instance and an S3 bucket for the application environment + Generate and use an IAM user access key to provide permissions (and understand its drawbacks) + Create an IAM role for EC2 with specific permissions to S3 + Attach the IAM role to the EC2 instance to grant secure access to the S3 bucket 10/22/2025 10/22/2025 Lightsail Containers: https://000046.awsstudygroup.com/ AWS Step Functions: https://000047.awsstudygroup.com/ IAM Roles for Applications: https://000048.awsstudygroup.com/ 5 - Get Started with AWS Cloud9 + Create a Cloud9 instance + Learn basic features like using the command line and working with files + Use the AWS CLI from within the Cloud9 environment - Deploy a Monolithic Application with Elastic Beanstalk + Set up a key pair, CloudFormation stack, and database + Connect to the Windows instance for configuration + Download and test the project locally using Eclipse IDE + Deploy and update the application on Elastic Beanstalk + Query the application\u0026rsquo;s API to test functionality - Configure an Automated Application Release Pipeline + Prepare the environment and configure an AWS CodeStar project + Connect Eclipse IDE with AWS CodeCommit for source control + Replace the sample application and trigger deployment via the pipeline + Deploy a Windows Service using AWS CodeDeploy to an EC2 instance + Monitor the service using the Eclipse IDE and other tools - Study Foundational AWS Security Principles + Analyze the process for securing the AWS account root user and enforcing multi-factor authentication + Practice configuring a dedicated IAM administrator user and group to avoid using the root account for daily tasks + Define and apply a strong IAM user password policy to enforce complexity and rotation requirements + Examine the use of Service Control Policies (SCPs) within an AWS Organization to apply preventative guardrails at the OU level - Analyze Identity and Access Management Configurations + Use IAM Access Analyzer to review and validate permissions for IAM users and roles, ensuring adherence to least privilege + Construct and test an IAM role to enable secure, temporary cross-account access without sharing long-term credentials + Evaluate resource policies for public and cross-account access to identify unintended external exposure + Generate and interpret IAM service-last-accessed reports to identify and remove unused or excessive permissions 10/23/2025 10/23/2025 AWS Cloud9: https://000049.awsstudygroup.com/ Elastic Beanstalk: https://000050.awsstudygroup.com/ CI/CD Pipeline: https://000051.awsstudygroup.com/ AWS Well-Architected Security Workshop: https://catalog.workshops.aws/well-architected-security 6 - Create a Microservice + Prepare and configure Eclipse IDE + Create, test, and upload a Lambda function locally and on AWS Lambda + Implement and automate a serverless ImageManager Lambda function + Configure orchestration with CodeStar for CI/CD pipeline deployment - Refactor Data and Workflows + Create a Key Pair and a CloudFormation stack + Create a Scan \u0026amp; Query Microservice using a new DynamoDB Table and a Global Secondary Index + Automate the microservice using AWS CodeStar and redeploy the new source from Eclipse IDE + Create an API for the microservice and update IAM policies for deployment + Create a Calculator Microservice using AWS Step Functions and Lambda - Implement Detective Controls and Incident Response Procedures + Deploy resources and analyze Amazon GuardDuty findings to understand its threat detection capabilities + Consolidate and prioritize security findings from various AWS services using AWS Security Hub + Configure automated response and remediation workflows in Security Hub to address common security findings + Conduct a root cause analysis of a security event by visualizing resource relationships and activity in Amazon Detective - Examine Infrastructure Protection Mechanisms + Design a VPC with appropriate subnetting and security groups to create a secure network boundary + Deploy AWS Network Firewall to inspect and filter traffic flows between subnets and to/from the internet + Configure AWS WAF rules to protect web applications from common exploits, such as cross-site scripting (XSS) + Understand the mechanisms of AWS Shield Advanced for mitigating sophisticated DDoS attacks - Review Data Protection and Encryption Strategies + Utilize Amazon Macie to discover and classify sensitive data residing within Amazon S3 buckets + Implement encryption at rest by applying customer-managed keys from AWS KMS to services like S3 and EBS + Secure data in transit by provisioning and deploying an SSL/TLS certificate from AWS Certificate Manager to a load balancer + Demonstrate secure application credential management by storing and rotating secrets with AWS Secrets Manager 10/24/2025 10/24/2025 Create Microservice: https://000052.awsstudygroup.com/ Refactor Data and Workflows: https://000053.awsstudygroup.com/ AWS Well-Architected Security Workshop: https://catalog.workshops.aws/well-architected-security Week 7 Achievements: Gained hands-on experience with Amazon DynamoDB by creating tables, loading data, working with GSIs and composite keys, and enabling backups and change data capture with Streams and Kinesis.\nApplied cost and analytics practices using DynamoDB performance analysis and AWS Glue + Athena to review cost and usage data and experiment with tagging for allocation.\nConfigured identity federation and IAM governance:\nSet up Azure AD federation to AWS, synced IAM roles, and tested federated console access. Created IAM groups, users, and roles with conditions such as IP address and time-based restrictions. Optimized AWS cost and database migrations by reviewing Savings Plans and Reserved Instances, and practicing schema conversion and data migration with AWS SCT and DMS, including monitoring and Serverless options.\nDeployed and managed applications on Lightsail and containers by running WordPress, Prestashop, Akaunting, and Lightsail container services, including snapshots, alarms, and Docker-based custom images.\nOrchestrated workflows and application access:\nBuilt workflows with AWS Step Functions using Task, Choice, Wait, Retry/Catch, and Parallel states. Compared IAM access keys with IAM roles and attached an EC2 role for secure S3 access. Used Cloud9 and Elastic Beanstalk for application lifecycle, from creating a Cloud9 environment and running CLI tasks to deploying and updating a monolithic app on Elastic Beanstalk and wiring a release pipeline with CodeStar, CodeCommit, and CodeDeploy.\nStrengthened account and organizational security:\nReviewed root account protection, MFA, and a dedicated IAM admin user and group. Defined password policies, explored SCPs, and used IAM Access Analyzer and last-accessed data to refine permissions. Built and refined microservices and security controls by creating Lambda-based microservices with DynamoDB and Step Functions, automating deployment with CodeStar, and deploying GuardDuty, Security Hub, Detective, and multiple network/data protection services (VPC design, Network Firewall, WAF, Shield Advanced, Macie, KMS, ACM, Secrets Manager).\n"
},
{
	"uri": "http://localhost:1313/internship_report/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Apply Infrastructure as Code with AWS CloudFormation to deploy, evolve, and scale application infrastructure. Improve application reliability through resiliency testing, Auto Scaling, and automated recovery mechanisms. Build and secure modern application patterns, including serverless SPAs with authentication and performance tracing. Explore AI, storage, and content delivery services such as Amazon Polly, Rekognition, Lex, S3, and CloudFront. Monitor workloads using CloudWatch dashboards across different operating systems and prepare for assessment. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Implement Infrastructure as Code using AWS CloudFormation + Deploy a foundational VPC and networking infrastructure using a CloudFormation template + Launch a multi-tier web application stack onto the created VPC with a separate CloudFormation template + Explore the running web application\u0026rsquo;s architecture, including the load balancer, auto-scaling group, and EC2 instances + Analyze the CloudFormation stack\u0026rsquo;s resources, outputs, and template to understand how the infrastructure was provisioned - Implement Advanced Health Checks and Dependency Management + Deploy a web application stack using a CloudFormation template to establish a baseline + Analyze application behavior by simulating a dependency failure and observing the impact + Configure a deep health check for the load balancer to ensure it accurately reflects the application\u0026rsquo;s true health + Modify the system to implement a \u0026ldquo;fail open\u0026rdquo; mechanism, allowing the application to maintain partial functionality during a dependency outage - Manage Infrastructure Evolution with AWS CloudFormation + Deploy a baseline infrastructure stack and understand its deployed resources + Update the running stack by modifying CloudFormation parameters + Evolve the infrastructure by adding an S3 bucket to the CloudFormation template + Further modify the stack by adding a new EC2 instance with specific configurations + Implement a multi-region deployment by launching the same CloudFormation stack in a different AWS region - Refactor a Monolithic Application to Microservices + Prepare the development environment and connect to the Windows instance + Analyze the existing monolithic application\u0026rsquo;s structure and dependencies + Create and deploy the \u0026ldquo;Advert\u0026rdquo; microservice + Create and deploy the \u0026ldquo;Invoice\u0026rdquo; microservice + Create and deploy the \u0026ldquo;ShoppingCart\u0026rdquo; microservice + Create and deploy the \u0026ldquo;Order\u0026rdquo; microservice + Create and deploy the \u0026ldquo;User\u0026rdquo; microservice + Offload static content by creating and deploying a \u0026ldquo;Static\u0026rdquo; microservice + Test the overall functionality of the newly refactored microservices-based solution 10/27/2025 10/27/2025 AWS Well-Architected Reliability Workshop: https://catalog.workshops.aws/well-architected-reliability Refactoring to Microservices: https://000054.awsstudygroup.com/ 3 - Implement Resiliency Testing with AWS Fault Injection Simulator + Create a new IAM role and policy to grant AWS FIS permissions to inject faults + Define and create an experiment template in AWS FIS to target specific resources + Run the fault injection experiment and observe the impact on the application + Analyze the experiment\u0026rsquo;s output and logs to understand the system\u0026rsquo;s response - Configure Auto Scaling to Handle Load and Recover from Failure + Set up a launch template for the EC2 instances in the web server tier + Create a target group for the Application Load Balancer + Configure an Auto Scaling group to manage the web server instances + Deploy a load testing application to generate traffic and test scaling + Observe and verify the Auto Scaling group\u0026rsquo;s response to the simulated load - Automate Component Replacement using Health Checks + Manually terminate a running EC2 instance to simulate a failure + Verify that the Auto Scaling group detects the failure and launches a replacement instance + Check the Application Load Balancer\u0026rsquo;s target group to confirm the new instance is registered and healthy - Prepare the Necessary AWS Environment for the Workshop + Create a new EC2 Key Pair for instance access + Deploy the foundational infrastructure using a CloudFormation stack + Establish a connection to the Windows Instance to configure the environment - Build and Deploy a Serverless Single Page Application + Create a DynamoDB table to store application data + Manually build and deploy a serverless microservice using AWS Lambda + Create and configure an API with Amazon API Gateway to expose the microservice + Set up a CI/CD pipeline with AWS CodeStar to automate API deployment + Configure and deploy the Single Page Application website + Develop a client to interact with and consume the API - Configure Application Authentication and Authorization + Integrate Amazon Cognito User Pools to add authentication to the SPA + Secure the microservice by implementing authentication checks + Deploy and test the complete authentication and authorization flow + Implement user sign-up and sign-in functionalities - Analyze Application Performance with AWS X-Ray + Integrate AWS X-Ray to trace requests and identify performance bottlenecks 10/28/2025 10/28/2025 AWS Well-Architected Reliability Workshop: https://catalog.workshops.aws/well-architected-reliability Serverless Web Application: https://000055.awsstudygroup.com/ 4 - Integrate Speech Capabilities using Amazon Polly + Prepare the environment and explore the Amazon Polly console + Generate speech and speech marks using the AWS CLI + Utilize the AWS Polly SDK for Java to synthesize speech - Add Object Recognition Features with Amazon Rekognition + Prepare the environment for Rekognition tasks + Detect objects within images using the Rekognition service + Implement facial recognition and experiment with a sample application - Implement a Chatbot using Amazon Lex + Deploy the base application and APIs + Create and enhance a Lex chatbot for the application + Develop a Lambda handler for the bot and configure it for fulfillment + Publish the Lex chatbot for use - Host and Accelerate a Static Website with Amazon S3 and CloudFront + Create an S3 bucket and load data for the website + Enable the static website hosting feature on the S3 bucket + Configure public access blocks and object permissions + Set up an Amazon CloudFront distribution to accelerate content delivery - Manage S3 Bucket Features for Data Protection and Replication + Enable and test bucket versioning to preserve object history + Practice moving objects within S3 + Configure multi-region object replication for disaster recovery 10/29/2025 10/29/2025 AI Services Integration: https://000056.awsstudygroup.com/ S3 and CloudFront: https://000057.awsstudygroup.com/ 5 - Monitor AWS Resources using CloudWatch Dashboards + Create a CloudWatch dashboard to visualize metrics + Add metric widgets to the dashboard for monitoring + Add a CloudWatch Logs Insights widget to the dashboard - Monitor a Windows EC2 Instance using CloudWatch + Deploy a VPC and associated networking components + Launch and configure a Windows EC2 instance + Create a custom CloudWatch dashboard for the instance + Add key performance metrics to the dashboard + Generate a synthetic load to observe metric changes - Monitor a Linux EC2 Instance using CloudWatch + Deploy a VPC and necessary networking components + Launch a Linux EC2 instance with a web server + Create a CloudWatch dashboard for monitoring + Add CPU and network metrics to the dashboard + Generate load on the instance to observe performance 10/30/2025 10/30/2025 AWS Well-Architected Performance Efficiency Workshop: https://catalog.workshops.aws/well-architected-performance-efficiency/ 6 - EXAM DAY 10/31/2025 10/31/2025 Week 8 Achievements: Practiced Infrastructure as Code with CloudFormation by deploying a foundational VPC, multi-tier web stack, and iteratively evolving the stack with additional resources such as S3 buckets, EC2 instances, and multi-region deployments.\nImproved understanding of resiliency and scaling:\nUsed AWS Fault Injection Simulator to design and run experiments that injected faults and observed application behavior. Configured Auto Scaling groups, launch templates, and load balancers to handle load changes and recover from failures, including automated instance replacement based on health checks. Refactored and extended application architectures:\nBroke down a monolithic application into multiple microservices (Advert, Invoice, ShoppingCart, Order, User, Static) and validated end-to-end functionality. Built and deployed a serverless Single Page Application backed by API Gateway, Lambda, and DynamoDB, with a CI/CD pipeline using CodeStar. Added authentication and authorization with Amazon Cognito and integrated AWS X-Ray to trace requests and analyze performance. Explored AI and content services:\nIntegrated Amazon Polly for text-to-speech through the console, CLI, and SDK. Used Amazon Rekognition for image object detection and basic facial recognition scenarios. Implemented a chatbot with Amazon Lex, including deployment, enhancement, Lambda-based fulfillment, and publishing. Hosted a static website on S3, secured it with appropriate access settings, and accelerated delivery with CloudFront; practiced S3 features like versioning and cross-region replication. Enhanced monitoring and observability skills:\nBuilt CloudWatch dashboards with metric widgets and Logs Insights to visualize application and infrastructure health. Monitored both Windows and Linux EC2 instances by tracking CPU, network, and other key metrics under synthetic load. Completed the scheduled assessment for the week and consolidated knowledge from reliability, serverless, AI services, and monitoring topics.\n"
},
{
	"uri": "http://localhost:1313/internship_report/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Design and integrate conversational and event-driven components using Amazon Lex and Amazon SNS. Work with managed data and caching services such as DynamoDB and ElastiCache, and automate deployments for EKS-based workloads. Apply governance and scalability practices through Service Quotas, IAM-based usage controls, and EKS Blueprints. Build and operate serverless and containerized applications, and evaluate storage performance across S3 and EFS. Implement S3 security controls and build a basic data lake pipeline using Glue, Athena, and QuickSight. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Configure an Amazon Lex Chatbot + Deploy the base application and necessary APIs for the chatbot + Create a new chatbot instance in Amazon Lex + Enhance the chatbot\u0026rsquo;s functionality and conversational flow + Develop a Lambda function to handle the bot\u0026rsquo;s fulfillment logic + Publish the chatbot to make it available for use - Implement a Publish/Subscribe Messaging Model with Amazon SNS + Deploy the initial infrastructure using a SAM template + Create an Amazon SNS Topic to handle message publication + Develop a customer notification service to subscribe to the topic + Create a customer accounting service as another subscriber + Implement an \u0026ldquo;extraordinary rides\u0026rdquo; service with message filtering + Update the primary management service to publish messages to the SNS topic + Test the fan-out functionality and message filtering rules 11/03/2025 11/03/2025 Amazon Lex Chatbot: https://000058.awsstudygroup.com/ Messaging with Amazon SNS: https://000059.awsstudygroup.com/ 3 - Work with Amazon DynamoDB + Create a table and perform basic data operations like write, read, and update + Query data from the table + Create and query a Global Secondary Index + Manage a table using AWS CloudShell + Interact with DynamoDB using Python, covering operations like creating tables, managing data, and performing queries and scans - Work with Amazon ElastiCache for Redis + Create an ElastiCache subnet group + Create an ElastiCache cluster with both cluster mode disabled and enabled + Grant access and connect to a cluster node + Use the AWS SDK to connect to an ElastiCache cluster + Perform data operations using the AWS SDK, such as get/set strings, manage hashes, use publish/subscribe, and interact with streams - Build a CI/CD Pipeline for an EKS Cluster + Prepare the environment with an IAM User, a Cloud9 workspace, and install Kubernetes tools + Configure an IAM role for the EKS cluster + Create an EKS cluster and test the deployment of a sample application + Set up AWS CodePipeline and CodeBuild with necessary service roles and permissions + Create a pipeline to automate the deployment process from a source code repository + Test the CI/CD pipeline by making a code change 11/04/2025 11/04/2025 Amazon DynamoDB Workshop: https://000060.awsstudygroup.com/ Amazon ElastiCache Workshop: https://000061.awsstudygroup.com/ EKS CI/CD Workshop: https://000062.awsstudygroup.com/ 4 - Manage Service Quotas + View and manage service quotas from a central location + Request a service quota increase through the Service Quotas console - Implement Resource Usage and Cost Management with IAM + Create IAM groups and users for managing access + Apply a policy to restrict resource creation by AWS Region + Implement a policy to limit the allowed EC2 instance families + Update policies to restrict allowed EC2 instance sizes + Create and assign a policy to limit the types of EBS volumes that can be created - Deploy and Manage an EKS Cluster using EKS Blueprints + Prepare the environment by creating a VPC, an EC2 instance, and installing necessary tools + Configure an IAM role for the EKS Blueprints setup + Create an EKS Blueprints and a CDK project + Build a deployment pipeline to create and manage the cluster + Manage team access to the cluster using Infrastructure as Code (IaC) + Configure and test add-ons such as the Cluster Autoscaler + Deploy workloads to the cluster using ArgoCD 11/05/2025 11/05/2025 Service Quotas Workshop: https://000063.awsstudygroup.com/ IAM Resource Management: https://000064.awsstudygroup.com/ EKS Blueprints Workshop: https://000065.awsstudygroup.com/ 5 - Build a Serverless Web Application using Lambda and API Gateway + Create a Cloud9 instance and a CodeCommit repository for the project + Deploy the frontend application using AWS Amplify Console + Deploy the backend infrastructure, including Lambda functions and an API Gateway + Populate a DynamoDB table with sample data + Implement a system for handling ride times with backend and frontend deployments + Develop a photo processing system using Lambda for chroma keying and photo-compositing - Transition a Monolithic Application to Microservices using Docker and AWS Fargate + Create a CloudFormation stack and set up the environment + Containerize a monolithic application using Docker + Deploy the containerized application using AWS Fargate + Configure an Application Load Balancer and an ECS Service + Create a new task definition revision and update the ECS service + Build a Docker image for a new microservice and create its task definition and ECS service - Evaluate Storage Performance on AWS + Launch a CloudFormation template and configure security groups + Connect to an EC2 instance for testing + Optimize S3 throughput and test sync commands for efficient data transfer + Analyze and optimize S3 performance for small file and copy operations + Optimize EFS IOPS and evaluate the impact of I/O size and sync frequency + Investigate the effects of multi-threading on EFS performance 11/06/2025 11/06/2025 Serverless Web Application: https://000066.awsstudygroup.com/ Microservices with Fargate: https://000067.awsstudygroup.com/ Storage Performance Evaluation: https://000068.awsstudygroup.com/ 6 - Implement S3 Security Best Practices + Launch a CloudFormation template and secure network access + Generate access keys and connect to an EC2 instance + Require HTTPS and SSE-S3 encryption for data in transit and at rest + Block public ACLs and configure S3 Block Public Access settings + Restrict access to an S3 VPC Endpoint + Use AWS Config to detect public buckets + Utilize Amazon Access Analyzer for S3 - Build a Data Lake with Your Data + Create a Cloud9 instance and prepare a dataset for upload to S3 + Use AWS DataBrew for data profiling, cleaning, and transformation + Ingest data with AWS Glue by configuring roles and creating a data catalog + Transform data to Parquet format + Query data using Amazon Athena with basic and advanced queries like joins and views + Visualize data with Amazon QuickSight by connecting datasets and building a dashboard 11/07/2025 11/07/2025 S3 Security Best Practices: https://000069.awsstudygroup.com/ Data Lake Workshop: https://000070.awsstudygroup.com/ Week 9 Achievements: Designed conversational and messaging workflows by configuring an Amazon Lex chatbot, wiring Lambda for fulfillment, and implementing a publish/subscribe model with Amazon SNS and multiple subscriber services.\nWorked with managed data and cache services:\nPracticed core DynamoDB operations, Global Secondary Indexes, CloudShell table management, and Python-based interactions. Created and connected to ElastiCache for Redis clusters, using the SDK to perform operations such as strings, hashes, publish/subscribe, and streams. Automated Kubernetes-based deployments by provisioning EKS clusters, configuring IAM roles, and building CI/CD pipelines with CodePipeline and CodeBuild to deploy sample applications from source changes.\nApplied governance and quota management:\nReviewed and adjusted AWS Service Quotas for capacity needs. Implemented IAM policies to control resource usage by Region, EC2 instance family/size, and EBS volume types. Used EKS Blueprints and Infrastructure as Code to prepare networking, configure cluster roles, manage team access, and deploy add-ons like Cluster Autoscaler and workloads via ArgoCD.\nBuilt and evaluated application architectures:\nDeveloped a serverless web application using Lambda, API Gateway, DynamoDB, Amplify, and CodeCommit, including ride time handling and photo processing flows. Transitioned a monolithic application to microservices with Docker and AWS Fargate, fronted by an Application Load Balancer and ECS services. Analyzed storage performance on AWS by testing S3 throughput and sync patterns, exploring small-file operations, and tuning EFS IOPS, I/O size, sync frequency, and multi-threading impact.\nStrengthened S3 security posture:\nEnforced HTTPS and SSE-S3 encryption, blocked public access via ACLs and Block Public Access, and restricted traffic to an S3 VPC Endpoint. Used AWS Config and Access Analyzer to detect and review potentially public or misconfigured buckets. Built a simple data lake workflow by staging data in S3, transforming it with DataBrew and Glue into Parquet, querying with Athena, and creating visualizations in QuickSight.\n"
},
{
	"uri": "http://localhost:1313/internship_report/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Deploy and manage applications with Red Hat OpenShift Service on AWS (ROSA) and implement a basic CI/CD workflow. Build and analyze an analytics platform on AWS using streaming ingestion, Glue, EMR, Athena, QuickSight, and Redshift. Create business dashboards and monitor network and account activity with Amazon QuickSight, VPC Flow Logs, and delegated billing access. Develop and deploy cloud applications using AWS CDK, event-driven architectures with SNS/SQS, and Lambda-based serverless patterns. Build serverless web application stacks with API Gateway, Lambda, DynamoDB, Cognito, and CloudFront, including SSL and authentication. Implement order processing and CI/CD pipelines for serverless applications using SQS, SNS, SAM, and CodePipeline. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Deploy Applications with Red Hat OpenShift Service on AWS (ROSA) + Enable ROSA and install the necessary command-line tools + Create and configure an OpenShift cluster on AWS + Deploy a sample application onto the OpenShift cluster + Set up a basic CI/CD workflow using CodeCommit, CodeBuild, and CodePipeline + Configure a CI/CD workflow specifically for the application deployment - Build an Analytics Platform on AWS + Ingest and store streaming data using Amazon Kinesis Firehose + Catalog data using AWS Glue Crawlers to populate the Glue Data Catalog + Transform data using AWS Glue interactive sessions, Glue Studio, and DataBrew + Process large-scale data with Amazon EMR + Analyze data interactively with Amazon Athena and in real-time with Kinesis Data Analytics + Visualize data by creating dashboards in Amazon QuickSight + Serve data using AWS Lambda and build a data warehouse with Amazon Redshift 11/10/2025 11/10/2025 ROSA Hands-on Lab: https://000071.awsstudygroup.com/ Analytics Platform: https://000072.awsstudygroup.com/ 3 - Getting Started with Amazon QuickSight + Prepare data and build an initial dashboard with various chart types like line charts, pie charts, and pivot tables + Enhance the dashboard by applying formatting, adding new visuals, and including detailed data tables + Create an interactive dashboard by configuring filters, filter actions, and navigation actions before publishing - Monitor Network Infrastructure with VPC Flow Logs + Create and enable VPC Flow Logs to capture IP traffic information for a VPC + Publish flow log data to Amazon CloudWatch Logs + Monitor and analyze the collected network traffic data to diagnose security group rules and understand traffic patterns - Delegate Access to the AWS Billing Console + Create an IAM user group and enable access to the billing console + Define a custom IAM policy to grant specific billing and cost management permissions + Attach the policy to the IAM user group to delegate access + Test the configured permissions by having a user from the group access the billing console 11/11/2025 11/11/2025 Amazon QuickSight Guide: https://000073.awsstudygroup.com/ VPC Flow Logs Lab: https://000074.awsstudygroup.com/ AWS Billing Console: https://000075.awsstudygroup.com/ 4 - Develop an Application using AWS CDK + Set up the environment by creating an IAM Role and launching an EC2 instance + Configure the development environment using VSCode + Use CDK to create an application architecture with API Gateway, ELB, and ECS + Integrate Lambda and S3 into the architecture + Implement nested stacks for better organization and reusability - Build an Event-driven Architecture with SNS and SQS + Deploy the necessary infrastructure and configure an event generator + Implement a simple publish/subscribe model using an SNS topic and an SQS queue + Apply message filtering to an SNS subscription to route specific messages + Use advanced message filtering techniques to handle more complex routing logic - Develop a Serverless Application with AWS Lambda + Create a Lambda function for image processing triggered by S3 events + Set up an S3 bucket and an IAM policy for the Lambda function + Test the Lambda function\u0026rsquo;s operations for image resizing + Create a DynamoDB table to store data + Write data to the DynamoDB table using a Lambda function 11/12/2025 11/12/2025 AWS CDK Development: https://000076.awsstudygroup.com/ Event-driven Architecture Lab: https://000077.awsstudygroup.com/ Serverless Lambda Guide: https://000078.awsstudygroup.com/ 5 - Build a Serverless Frontend to Call API Gateway + Deploy the front-end application + Create a DynamoDB table + Deploy Lambda functions for writing, listing, and deleting items + Configure API Gateway methods and enable CORS + Test the APIs using Postman and the front-end - Deploy a Serverless Application with SAM + Deploy the front-end application + Create a DynamoDB table + Deploy Lambda functions for listing, writing, deleting, and resizing images + Configure GET, POST, and DELETE methods in API Gateway + Test the APIs using Postman and the front-end - Implement Serverless Authentication with Amazon Cognito + Create a Cognito User Pool + Create an API and a corresponding Lambda function + Test the authentication flow with a front-end application - Configure SSL for a Serverless Application + Create a domain and a Route 53 hosted zone + Request an SSL certificate using AWS Certificate Manager + Create a CloudFront distribution to serve the application over HTTPS 11/13/2025 11/13/2025 Serverless Frontend with API Gateway: https://000079.awsstudygroup.com/ Serverless Application with SAM: https://000080.awsstudygroup.com/ Cognito Authentication Lab: https://000081.awsstudygroup.com/ SSL Configuration: https://000082.awsstudygroup.com/ 6 - Implement Order Processing with SQS and SNS + Create an SQS queue and an SNS topic + Create a DynamoDB table for orders + Develop Lambda functions for checking out, managing, handling, and deleting orders + Test the order processing flow - Build a CI/CD Pipeline for a Serverless Application + Create a Git repository for the SAM pipeline + Configure a SAM pipeline with AWS CodePipeline + Create a Git repository for the front-end + Build a pipeline for the front-end deployment + Test the web application\u0026rsquo;s functionality 11/14/2025 11/14/2025 Order Processing with SQS and SNS: https://000083.awsstudygroup.com/ CI/CD Pipeline for Serverless: https://000084.awsstudygroup.com/ Week 10 Achievements: Deployed applications with Red Hat OpenShift Service on AWS by enabling ROSA, provisioning a cluster, deploying a sample workload, and wiring a basic CI/CD workflow using CodeCommit, CodeBuild, and CodePipeline.\nBuilt and explored an analytics platform on AWS:\nIngested streaming data with Kinesis Firehose and cataloged it using AWS Glue Crawlers. Transformed and processed data with Glue (Studio and interactive sessions), DataBrew, EMR, and analyzed it with Athena and Kinesis Data Analytics. Visualized insights in QuickSight dashboards and served data with Lambda and Redshift-based warehousing. Created visualizations and improved observability:\nDesigned interactive QuickSight dashboards with multiple chart types, formatting, filters, and navigation. Enabled and reviewed VPC Flow Logs in CloudWatch Logs to understand traffic patterns and validate security group behavior. Delegated access to the AWS Billing Console via IAM groups and custom billing policies, then verified the delegated access. Developed infrastructure and applications with AWS CDK and event-driven patterns:\nSet up a CDK development environment on EC2 with VSCode, IAM roles, and used CDK to define architectures with API Gateway, ELB, ECS, Lambda, and S3. Implemented SNS + SQS event-driven flows, including basic pub/sub and advanced message filtering. Created Lambda-based image processing and data persistence workflows integrated with S3 and DynamoDB. Built serverless application stacks end-to-end:\nDeployed front-end applications that call API Gateway, backed by Lambda and DynamoDB, using both manual setup and SAM. Configured API Gateway methods with CORS, implemented Cognito-based authentication, and tested flows from the front end and with tools like Postman. Secured serverless applications with custom domains, Route 53 hosted zones, ACM certificates, and CloudFront distributions over HTTPS. Implemented order processing and automation for serverless workloads:\nBuilt an order processing pipeline using SQS, SNS, DynamoDB, and multiple Lambda functions to handle checkout, management, and deletion flows. Set up CI/CD pipelines for SAM-based backends and front-end applications with CodePipeline, connecting them to Git repositories and validating successful deployments. "
},
{
	"uri": "http://localhost:1313/internship_report/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Attend AWS community event to learn about advanced cloud services and best practices. Build serverless text service infrastructure with DynamoDB table design and IAM role configuration. Implement database retrieval functions with pagination support and filter expressions. Develop in-memory caching system and API request validation logic. Integrate Amazon Bedrock Agent for AI-generated paragraph content. Configure monitoring and debugging tools for serverless applications. Develop GraphQL APIs using AWS AppSync with DynamoDB resolvers. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Attending \u0026ldquo;AWS Cloud Mastery Series #2\u0026rdquo; event 11/17/2025 11/17/2025 3 - Initialize AWS serverless infrastructure for text service: + Create DynamoDB table wordsntexts with a string partition key to store words and paragraphs + Configure IAM execution role permissions for dynamodb:Scan, dynamodb:Query, and CloudWatch logging + Set up the initial Lambda handler structure and establish boto3 database connection objects - Implement database retrieval algorithms: + Develop fetch_words_from_db using scan operations with LastEvaluatedKey for pagination + Develop fetch_paragraph_from_db applying filter expressions for content type and length attributes 11/18/2025 11/18/2025 4 - Implement caching mechanisms and data processing logic: + Design an in-memory global cache dictionary with a 300-second TTL to reduce database scan frequency + Implement get_random_words logic to retrieve data from cache or DB and perform random sampling + Implement get_paragraph logic to handle length clamping (1-3) and split content strings into word lists - Develop API request validation and response formatting: + Create a custom Decimal_encoder class to properly serialize DynamoDB numeric types for JSON output + Add input validation blocks for query parameters type and count to handle TypeErrors and ValueErrors + Standardize JSON response structures to include appropriate HTTP status codes and headers 11/19/2025 11/19/2025 5 - Integrate Amazon Bedrock Agent for generative capabilities: + Update IAM permissions to include bedrock:InvokeAgent and associate with Agent ID HUEBUXSALX + Initialize the bedrock-agent-runtime client and implement the invoke_agent call with session management + Develop logic to process streaming response payloads and decode byte chunks into a unified string - Conduct prompt engineering and model experimentation: + Design trigger prompts to ensure the Agent returns three paragraphs separated by blank lines + Experiment with different underlying models to ensure consistent formatting for the split logic + Implement parsing logic to separate the AI output 11/20/2025 11/20/2025 6 - Monitor and Debug Serverless Applications with CloudWatch and X-Ray + Analyze Lambda function logs in CloudWatch to identify and troubleshoot execution errors + Define and implement custom metrics to track application-specific performance data + Configure CloudWatch Alarms to trigger notifications based on critical metric thresholds + Enable AWS X-Ray tracing to visualize service maps and identify latency bottlenecks - Develop GraphQL APIs with AWS AppSync and DynamoDB Resolvers + Prepare the AppSync environment and configure DynamoDB as a data source + Implement resolvers to perform Write and Read operations for individual data items + Configure Update and Delete resolvers to manage data modification lifecycles + Execute Scan and Query operations to facilitate bulk data retrieval via the API + Design and implement complex object resolvers to handle nested data structures 11/21/2025 11/21/2025 CloudWatch and X-Ray Monitoring: https://000085.awsstudygroup.com/ AppSync GraphQL APIs: https://000086.awsstudygroup.com/ Week 11 Achievements: Attended the \u0026ldquo;AWS Cloud Mastery Series #2\u0026rdquo; event to learn about advanced AWS services and best practices.\nBuilt serverless infrastructure for a typing practice text service application:\nCreated DynamoDB table wordsntexts with string partition key to store approximately 64,726 words and paragraphs. Configured IAM execution role with permissions for DynamoDB Scan/Query operations and CloudWatch logging. Set up Lambda handler (128 MB memory, 15s timeout) with boto3 database connection objects. Implemented database retrieval algorithms:\nDeveloped fetch_words_from_db using DynamoDB scan operations with LastEvaluatedKey pagination. Created fetch_paragraph_from_db with filter expressions for content type and length attributes (short: 10-25 words, medium: 25-60 words, long: 60+ words). Optimized application performance and implemented request handling:\nDesigned in-memory caching system with 300-second TTL to reduce DynamoDB scan frequency for expected load of ~500 requests/hour. Implemented get_random_words and get_paragraph functions with random sampling and length clamping (1-3). Created custom Decimal_encoder class for proper JSON serialization of DynamoDB numeric types. Added input validation for query parameters type and count to handle TypeErrors and ValueErrors. Standardized JSON response structures with appropriate HTTP status codes and headers. Integrated Amazon Bedrock Agent for generative AI paragraph generation:\nUpdated IAM permissions to include bedrock:InvokeAgent and configured access to Agent ID HUEBUXSALX. Initialized bedrock-agent-runtime client with session management for invoke_agent calls. Developed streaming response processing logic to decode byte chunks into unified strings. Designed trigger prompts to generate three paragraphs separated by blank lines for future daily challenge feature. Implemented parsing logic to separate AI output. Configured monitoring and debugging capabilities for serverless applications:\nAnalyzed Lambda execution logs in CloudWatch to identify and troubleshoot errors. Defined custom CloudWatch metrics to track application-specific performance data. Configured CloudWatch Alarms with critical metric thresholds for error rates and duration monitoring. Enabled AWS X-Ray tracing to visualize service maps and identify latency bottlenecks. Developed GraphQL APIs with AWS AppSync:\nSet up AppSync environment and configured DynamoDB as a data source. Implemented resolvers for Write, Read, Update, and Delete operations. Configured Scan and Query resolvers for bulk data retrieval via the API. Designed complex object resolvers to handle nested data structures and relationships. "
},
{
	"uri": "http://localhost:1313/internship_report/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Le Tri Dung\nPhone Number: 0941423585\nEmail: tridungit2005@gmail.com\nUniversity: FPT Ho Chi Minh University\nMajor: Data Science\nStudent ID: SE196261\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 09/2025 to 02/2026\nReport Content Worklog Proposal "
},
{
	"uri": "http://localhost:1313/internship_report/4-eventparticipated/4.4-event4/",
	"title": "",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/internship_report/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/internship_report/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]