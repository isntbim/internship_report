[
{
	"uri": "http://localhost:1313/internship_report/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1: Getting acquainted with the team and mastering AWS basics\nWeek 2: Typing game project scoping, AWS microservices, and operational foundations\nWeek 3: AWS hands-on labs, UI/UX documentation, and NoSQL integration for project 1\nWeek 4: RDS, Auto Scaling, CloudWatch, Route 53, CLI, CI/CD, Docker, Serverless, Security Hub\nWeek 5: From Networking to Deployment: VPC Peering, Transit Gateway, WordPress, Lambda Cost Optimization, CI/CD, Storage Gateway, FSx \u0026amp; WAF\n"
},
{
	"uri": "http://localhost:1313/internship_report/4-eventparticipated/4.1-kick-off/",
	"title": "Kick-off AWS FCJ Workforce - FPTU OJT FALL 2025",
	"tags": [],
	"description": "",
	"content": "Post-experience Report Event Objectives Build a high-quality generation of AWS Builders for Vietnam. Equip them with practical skills in Cloud, DevOps, AI/ML, Security, and Data \u0026amp; Analytics. Connect students with the 47,000+ member AWS Study Group community and AWS partner businesses. Speakers Nguyen Gia Hung – Head of Solutions Architect, AWS Vietnam Do Huy Thang – DevOps Lead, VNG Doanh Doan Hieu Nghi – GenAI Engineer, Renova Bui Ho Linh Nhi – AI Engineer, SoftwareOne Pham Nguyen Hai Anh – Cloud Engineer, G-Asia Pacific Nguyen Dong Thanh Hiep – Principal Cloud Engineer, G-Asia Pacific Key Highlights Identifying the common pitfalls lead to failure Spending on what make you fun for the mean time while ignoring what make you better. Learning a course for it job title instead of viewing it as a competitive edges -\u0026gt; No one would see you as a valuable workforce. Learning is a life-long journey and no one can take a shortcut in the ladder of knowledge. The journey for a working opportunities No one have it easy when it come to finding a job. Its a long and challenging way involving hard working and the ability to seize opportunity. What awaits me at AWS First Cloud Journey A way to connect to others around me and find companions to accompany me for the AWS First Cloud Journey and even for life. Lots of challenges that I need to cross to be a better version of myself. Opportunities for hand-on experience to further improve my abilities. Key Takeaways Prioritize Long-Term Growth Over Short-Term Fun Success requires investing in skills that build your future value, not just spending on temporary enjoyment. Approach learning as a way to gain a genuine competitive edge, rather than just collecting a job title, to become a truly valuable professional.\nEmbrace the Challenge as a Lifelong Journey Understand that securing career opportunities is a difficult process requiring hard work and persistence. There are no shortcuts on the ladder of knowledge; view every challenge as a necessary step to becoming a better version of yourself.\nEvent Experience Attending the “Kick-off AWS First Cloud Journey” workshop was extremely valuable, giving me a solid foundation of essential concepts, the practical skills to start building, and the inspiration to continue my lifelong learning journey in the cloud. Key experiences included:\nLearning from highly skilled speakers The event provided a multi-faceted learning experience, blending high-level industry vision with practical, on-the-ground career advice. We received a strategic overview of the cloud\u0026rsquo;s future from Nguyễn Gia Hưng, Head of Solutions Architect at AWS Vietnam, and gained deep insights into the crucial role of DevOps from Đỗ Huy Thắng, DevOps Lead at VNG. This was perfectly complemented by the relatable and inspiring stories from program alumni, who shared their personal journeys from being students to becoming specialized professionals like a GenAI Engineer and a Cloud Engineer. Hearing directly about \u0026ldquo;a day in the life\u0026rdquo; and the transition from the program into a full-time tech role provided a clear and tangible picture of the path ahead.\nNetworking and discussions From the moment of check-in to the dedicated tea break, the atmosphere was buzzing with energy. There were invaluable opportunities to connect with fellow students who will be our peers and collaborators throughout this On-the-Job Training program. Beyond peer networking, the final Q\u0026amp;A session was a highlight, allowing us to directly engage with the speakers and mentors. This open forum provided a chance to ask specific questions about career paths, technical challenges, and personal development, turning the one-way flow of information into a dynamic and collaborative discussion.\nLessons learned Three core lessons stood out from the event. First, cloud computing is the foundational launchpad for modern careers, not just a single destination; it\u0026rsquo;s the gateway to specializations in AI, DevOps, Security, and Data. Second, the journey is a marathon, not a sprint. The diverse stories from the alumni emphasized that this program is a critical first step, but continuous learning and resilience are what shape a successful career. Finally, community is a powerful accelerator. The event solidified that we are now part of a larger ecosystem-the AWS Builders community-where collaboration and shared knowledge are essential for growth.\nSome event photos Overall, the event not only provided technical knowledge but also helped me reshape my thinking about the way of learning and encourage me to keep pushing harder.\n"
},
{
	"uri": "http://localhost:1313/internship_report/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "How We Built a Flywheel to Steadily Improve Security for Amazon RDS by Joshua Brindle\nThis blog details the process an AWS security team undertook to secure a new feature, PL/Rust, on Amazon Relational Database Service (Amazon RDS). The author, a principal security engineer, explains how the team moved beyond a simple implementation to build a comprehensive, self-improving security system—a \u0026ldquo;flywheel\u0026rdquo;—that combines technology, process, and testing to protect customers.\nThe Pieces of the System The project\u0026rsquo;s central component was PL/Rust, an extension that allows users to write custom PostgreSQL functions in Rust that are then compiled into highly performant native machine code. The core of this extension is a library called postgrestd, which was designed to prevent database escapes. However, at the time, the library was new and had not yet been hardened for the realities of a large-scale production environment. The primary security challenge arose from the fact that PL/Rust compiles these extensions directly on the database instance itself. This design requires a full development toolchain to be locally available, significantly increasing the potential risk. A poorly constructed extension could destabilize the database or its host instance, and attackers could use various techniques to try and bypass security controls like the write xor execute (W^X) model. This context made it clear that a series of robust mitigations were necessary to provide this functionality to customers safely.\nChallenging the Approach The AWS culture of obsessing over operational excellence—with a focus on automation, resilience, and simplicity—heavily influenced the search for a solution. The team considered SELinux (Security-Enhanced Linux), which was described as a long-debated option. SELinux is a set of kernel features that enforces mandatory access control (MAC), adding a powerful layer of protection on top of the standard authorization system. Using SELinux policies, an administrator can be extremely specific about what is allowed on a system, for instance, by preventing a process from writing to a file even if its ownership permissions would normally permit it. This level of deterministic control can greatly increase an operating system\u0026rsquo;s security. The trade-off, however, is reduced flexibility and the significant effort required to configure the access controls to meet specific security requirements. After a thorough internal debate, which involved senior leaders challenging the idea to anticipate future issues, the team agreed that for the PL/Rust use case, the benefits of SELinux outweighed the downsides. The decision was made to proceed with this approach.\nBuilding the Security Flywheel Simply implementing a tool wasn\u0026rsquo;t enough; the team built a complete, constantly improving process around it.\nEnforce and Monitor: They built the SELinux environment and created policies to lock down the system. Crucially, they configured these policies to send any denial messages to their internal telemetry systems for analysis. Respond: Working with the internal blue team, they developed specific incident response playbooks for the Amazon RDS team to investigate these denial messages. Test and Refine: The team began running quarterly \u0026ldquo;game days\u0026rdquo;. During these exercises, the red team would stage exploits against the system, and the service team would respond using their playbooks. Afterwards, all teams would analyze the response to find bottlenecks and areas for improvement. This cycle of enforcement, monitoring, response, and testing created a strong, well-oiled security machine.\nThe Flywheel in Action: A Real-World Example The effectiveness of this system was validated in a production environment. An SELinux denial message automatically generated a high-severity ticket for the service team. The system had worked as expected—it successfully blocked an unauthorized activity, acting as a proactive intrusion detection system. Even though the immediate risk was neutralized, the team\u0026rsquo;s process required them to investigate the root cause to see if the system could be improved further. The investigation eventually revealed that the activity was initiated by the research team at Varonis Threat Labs. AWS security then reached out to them to collaborate, demonstrating how a security event can lead to positive engagement with the research community. This incident provided a concrete and rewarding example of how the team\u0026rsquo;s work directly benefited customers.\nFor the security engineers involved, this was deeply validating, as it provided a concrete example of how their proactive work directly benefited customers by preventing a potential issue.\n"
},
{
	"uri": "http://localhost:1313/internship_report/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Create an SSL connection to Amazon RDS for Db2 in Java without KeyStore or Keytool by Vikram Khatri, Amine Yahsine, Ashish Saraswat, and Sumit Kumar\nThis article outlines a simplified method for establishing a secure SSL database connection in Java, specifically for Amazon Relational Database Service (Amazon RDS) for Db2. The approach allows developers to bypass the traditional complexities associated with the keytool utility and the management of Java KeyStores. The primary benefits of this technique include its simplicity, its suitability for automated environments like CI/CD pipelines, and its ability to maintain strong security through proper TLS 1.2 negotiation and server certificate validation.\nSolution Overview Instead of relying on a traditional Java TrustStore, this solution leverages a specific configuration property supported by the IBM JDBC driver. The driver can be instructed to use a PEM-formatted server certificate directly, which eliminates the need to convert the certificate or import it into a .jks file.\nThis is accomplished by setting the sslCertLocation property:\nproperties.put(\u0026#34;sslCertLocation\u0026#34;, \u0026#34;/path/to/certchain.pem\u0026#34;); To ensure the connection is encrypted and uses a secure protocol, the following JDBC driver connection properties must also be set:\nsslConnection=true sslVersion=TLSv1.2 This method is particularly well-suited for cloud environments like AWS, where Amazon RDS provides a PEM-formatted certificate bundle. The solution was tested with an IBM Db2 JDBC Driver (db2jcc4.jar v4.33.31), Java 11+, and a PEM certificate from Amazon RDS.\nPrerequisites Before implementing this solution, the following resources are assumed to be in place:\nAn Amazon RDS for Db2 server instance with SSL already enabled. A certificate chain PEM file, such as the region-specific us-east-1-bundle.pem available for download from AWS. A recent version of the IBM Data Server Driver (db2jcc4.jar version 4.33 or later). Java 8 or higher, with support for TLS 1.2. The Java Program The full source code for a Java program (Db2SSLTest.java) that connects to Amazon RDS for Db2 using this SSL method is provided below:\nimport java.sql.*; import java.util.Properties; public class Db2SSLTest { public static void main(String[] args) { if (args.length != 6) { System.out.println(\u0026#34;Usage: java Db2SSLTest \u0026#34; + \u0026#34; \u0026lt;certchain.pem\u0026gt; \u0026#34; + \u0026#34; \u0026lt;hostname\u0026gt; \u0026lt;port\u0026gt; \u0026lt;database\u0026gt; \u0026lt;userid\u0026gt; \u0026lt;password\u0026gt;\u0026#34;); System.exit(1); } Properties properties = new Properties(); String certPath = args[0]; String hostname = args[1]; String port = args[2]; String database = args[3]; String userid = args[4]; String password = args[5]; properties.put(\u0026#34;sslConnection\u0026#34;, \u0026#34;true\u0026#34;); properties.put(\u0026#34;sslVersion\u0026#34;, \u0026#34;TLSv1.2\u0026#34;); properties.put(\u0026#34;sslCertLocation\u0026#34;, certPath); properties.put(\u0026#34;user\u0026#34;, userid); properties.put(\u0026#34;password\u0026#34;, password); String url = \u0026#34;jdbc:db2://\u0026#34; + hostname + \u0026#34;:\u0026#34; + port + \u0026#34;/\u0026#34; + database; try { Class.forName(\u0026#34;com.ibm.db2.jcc.DB2Driver\u0026#34;); Connection conn = DriverManager.getConnection(url, properties); Statement stmt = conn.createStatement(); ResultSet rs = stmt.executeQuery(\u0026#34;SELECT CURRENT \u0026#34; + \u0026#34; TIMESTAMP \u0026#34; + \u0026#34; FROM SYSIBM.SYSDUMMY1\u0026#34;); if (rs.next()) { System.out.println(\u0026#34;SSL Connection successful!\u0026#34;); System.out.println(\u0026#34;Current timestamp: \u0026#34; + rs.getString(1)); } rs.close(); stmt.close(); conn.close(); } catch (Exception e) { System.err.println(\u0026#34;Error: \u0026#34; + e.getMessage()); e.printStackTrace(); } } } Compile and Run Assuming the IBM JDBC driver is located at ~/sqllib/java/db2jcc4.jar, the following shell script illustrates how to compile and run the Java program. The script also includes a function to retrieve the database password from AWS Secrets Manager or prompt the user for it manually.\n#!/usr/bin/env bash # Retrieves the master user password for a specified DB instance. # This function attempts to obtain the master user password for the provided # DB instance ID. It first checks if the password can be retrieved from the # AWS Secrets Manager. If a valid secret is not found, it prompts the user # to manually enter the password. # # Args: # DB_INSTANCE_ID (str): The database instance identifier. # # Environment Variables: # REGION: The AWS region where the DB instance is located. # # Exports: # MASTER_USER_PASSWORD: The retrieved or entered master user password. # # Returns: # int: Returns 1 if the password retrieval fails, otherwise 0. get_master_password() { DB_INSTANCE_ID=$1 SECRET_ARN=$(aws rds describe-db-instances \\ --db-instance-identifier \u0026#34;$DB_INSTANCE_ID\u0026#34; \\ --region $REGION \\ --query \u0026#34;DBInstances[0].MasterUserSecret.SecretArn\u0026#34; \\ --output text) if [[ -z \u0026#34;$SECRET_ARN\u0026#34; || \u0026#34;$SECRET_ARN\u0026#34; == \u0026#34;None\u0026#34; ]]; then read -rsp \u0026#34;Enter Master User password: \u0026#34; MASTER_USER_PASSWORD echo else SECRET_JSON=$(aws secretsmanager get-secret-value \\ --secret-id \u0026#34;$SECRET_ARN\u0026#34; \\ --query \u0026#34;SecretString\u0026#34; \\ --region $REGION \\ --output text) MASTER_USER_PASSWORD=$(jq -r \u0026#39;.password\u0026#39; \u0026lt;\u0026lt;\u0026lt; \u0026#34;$SECRET_JSON\u0026#34;) if [[ -z \u0026#34;$MASTER_USER_PASSWORD\u0026#34; ]]; then echo \u0026#34;Failed to get password from secret manager \u0026#39;$SECRET_ARN\u0026#39;. Exiting...\u0026#34; return 1 fi export MASTER_USER_PASSWORD=$MASTER_USER_PASSWORD fi } # Retrieves the master user name for a specified DB instance. # # This function queries AWS RDS to obtain the master user name for the provided # DB instance identifier. If the master user name is not found, it returns an # error message. # # Environment Variables: # DB_INSTANCE_IDENTIFIER: The database instance identifier. # REGION: The AWS region where the DB instance is located. # # Exports: # MASTER_USER_NAME: The retrieved master user name. # # Returns: # int: Returns 1 if the master user name is not found, otherwise 0. get_master_user_name() { local master_user_name=($(aws rds describe-db-instances \\ --db-instance-identifier \u0026#34;$DB_INSTANCE_IDENTIFIER\u0026#34; \\ --region $REGION \\ --query \u0026#34;DBInstances[0].MasterUsername\u0026#34; \\ --output text)) if [ \u0026#34;$master_user_name\u0026#34; = \u0026#34;None\u0026#34; ]; then echo \u0026#34;Not found\u0026#34; return 1 else export MASTER_USER_NAME=$master_user_name fi } # Retrieves the database address for a specified DB instance. # # This function queries AWS RDS to obtain the database endpoint address for the # provided DB instance identifier. If the address is not found, it returns an # error message. # # Environment Variables: # DB_INSTANCE_IDENTIFIER: The database instance identifier. # REGION: The AWS region where the DB instance is located. # # Exports: # DB_ADDRESS: The retrieved database endpoint address. # # Returns: # int: Returns 1 if the database address is not found, otherwise 0. get_db_address() { local db_address=($(aws rds describe-db-instances \\ --db-instance-identifier \u0026#34;$DB_INSTANCE_IDENTIFIER\u0026#34; \\ --region $REGION \\ --query \u0026#34;DBInstances[0].Endpoint.Address\u0026#34; \\ --output text)) if [ -z \u0026#34;$db_address\u0026#34; ]; then echo \u0026#34;Not found\u0026#34; return 1 else export DB_ADDRESS=$db_address fi } # Retrieves the SSL port number for a specified DB instance. # # This function queries AWS RDS to obtain the parameter group name associated # with the provided DB instance identifier, and then queries the parameter # group to obtain the SSL port number. If the SSL port is not found, it returns # an error message. # # Environment Variables: # DB_INSTANCE_IDENTIFIER: The database instance identifier. # REGION: The AWS region where the DB instance is located. # # Exports: # SSL_PORT: The retrieved SSL port number. # # Returns: # int: Returns 1 if the SSL port is not found, otherwise 0. get_ssl_port() { SSL_PORT=\u0026#34;\u0026#34; DB_PARAM_GROUP_NAME=$(aws rds describe-db-instances \\ --db-instance-identifier \u0026#34;$DB_INSTANCE_IDENTIFIER\u0026#34; \\ --region $REGION \\ --query \u0026#34;DBInstances[0].DBParameterGroups[0].DBParameterGroupName\u0026#34; \\ --output text) if [ \u0026#34;$DB_PARAM_GROUP_NAME\u0026#34; != \u0026#34;\u0026#34; ]; then SSL_PORT=$(aws rds describe-db-parameters \\ --db-parameter-group-name \u0026#34;$DB_PARAM_GROUP_NAME\u0026#34; \\ --region $REGION \\ --query \u0026#34;Parameters[?ParameterName==\u0026#39;ssl_svcename\u0026#39;].ParameterValue\u0026#34; \\ --output text) if [ \u0026#34;$SSL_PORT\u0026#34; = \u0026#34;None\u0026#34; ]; then SSL_PORT=\u0026#34;\u0026#34; return 1 fi fi export SSL_PORT=$SSL_PORT return 0 } # Main entry point for the script. # # This function compiles a Java program, downloads the SSL certificate, retrieves # the master user name, master password, database address, and SSL port from AWS # RDS, and then runs the Java program with the retrieved parameters. # # Exports: # None # # Returns: # int: Returns 0 if the program runs successfully, otherwise 1. main () { DB_INSTANCE_IDENTIFIER=\u0026#34;viz-demo\u0026#34; CL_PATH=.:$HOME/sqllib/java/db2jcc4.jar REGION=\u0026#34;us-east-1\u0026#34; PROG_NAME=Db2SSLTest JAVA_FILE=${PROG_NAME}.java DBNAME=\u0026#34;TEST\u0026#34; if ! command -v javac \u0026amp;\u0026gt;/dev/null; then echo \u0026#34;javac is not installed. Please install Java Development Kit (JDK) to compile Java programs.\u0026#34; exit 1 fi echo \u0026#34;Compile Java program $JAVA_FILE\u0026#34; javac -cp $CL_PATH $JAVA_FILE echo \u0026#34;Downloading SSL certificate...\u0026#34; CERTCHAIN=\u0026#34;/home/db2inst1/us-east-1-bundle.pem\u0026#34; if [ -f \u0026#34;$CERTCHAIN\u0026#34; ]; then echo \u0026#34;Certificate already exists. Skipping download.\u0026#34; else echo \u0026#34;Certificate does not exist. Downloading...\u0026#34; if ! curl -sL \u0026#34;https://truststore.pki.rds.amazonaws.com/us-east-1/$REGION-bundle.pem\u0026#34; -o $REGION-bundle.pem; then echo \u0026#34;Failed to download SSL certificate. Please check your network connection or the URL.\u0026#34; exit 1 fi fi if get_master_user_name \u0026#34;$DB_INSTANCE_IDENTIFIER\u0026#34;; then echo \u0026#34;Master user name: $MASTER_USER_NAME\u0026#34; USER=\u0026#34;$MASTER_USER_NAME\u0026#34; else echo \u0026#34;Failed to retrieve master user name. Exiting...\u0026#34; exit 1 fi if get_master_password \u0026#34;$DB_INSTANCE_IDENTIFIER\u0026#34;; then PASSWORD=$MASTER_USER_PASSWORD else echo \u0026#34;Failed to retrieve master password. Exiting...\u0026#34; exit 1 fi if get_db_address \u0026#34;$DB_INSTANCE_IDENTIFIER\u0026#34;; then echo \u0026#34;DB Address: $DB_ADDRESS\u0026#34; HOST=\u0026#34;$DB_ADDRESS\u0026#34; else echo \u0026#34;Failed to retrieve DB address. Exiting...\u0026#34; exit 1 fi if get_ssl_port \u0026#34;$DB_INSTANCE_IDENTIFIER\u0026#34;; then echo \u0026#34;SSL Port: $SSL_PORT\u0026#34; PORT=\u0026#34;$SSL_PORT\u0026#34; else echo \u0026#34;Failed to retrieve SSL port. Exiting...\u0026#34; exit 1 fi # Use -Djavax.net.debug=ssl:handshake:verbose to debug SSL issues echo \u0026#34;Running Java program...\u0026#34; java \\ -cp \u0026#34;$CL_PATH\u0026#34; $PROG_NAME $CERTCHAIN $HOST $PORT $DBNAME $USER $PASSWORD } main \u0026#34;$@\u0026#34; Considerations There is a limitation in the JDBC driver (at the time of writing) that prevents the use of a global certificate bundle (like global-bundle.pem) with the sslCertLocation property. If an application connects to a single AWS Region, it is recommended to use a region-specific certificate file (e.g., us-east-1-bundle.pem). If there is an absolute requirement to use the global bundle, developers must revert to the traditional method of using keytool to store the certificates in a keystore.\nTroubleshooting The following are solutions to common issues:\nFailing SSL connection: If the SSL connection fails after being enabled in the RDS for Db2 instance, the instance must be restarted. The SSL enablement only takes effect after a restart. Unable to locate the db2jcc4.jar file: This file is included with various IBM DB2 client packages, such as the data server client or runtime client. Unable to connect to the RDS database: If a connection fails after cataloging a database with SSL using db2cli commands, there might be an existing database connection that is not aware of the newly cataloged database. The db2 terminate command should be used to close existing connections before testing again. Conclusion This post demonstrates that it is not always necessary to use the Java KeyStore or keytool utility to enable SSL connections. With a PEM certificate and a modern JDBC driver, a secure connection can be established with minimal setup. This approach is especially valuable for developers who need to perform rapid SSL testing, for automated environments such as CI/CD and containers, and for anyone looking to simplify secure Java-to-Db2 connectivity.\n"
},
{
	"uri": "http://localhost:1313/internship_report/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Enhance the local testing experience for serverless applications with LocalStack by Patrick Galvin and Debasis Rath\nThis article announces and explains new capabilities designed to simplify the local testing experience for serverless applications. Through an integration with AWS Partner, LocalStack, the AWS Toolkit for Visual Studio Code now provides a more streamlined way for developers to build, test, and debug their serverless applications without leaving their development environment.\nChallenges with Local Serverless Development While serverless architectures are generally simple to operate and scale, the development and testing process can introduce friction that slows down the code-test-debug cycle. Developers often encounter several common roadblocks:\nSlow Iteration from Cloud-Based Validation: Previously, developers had to deploy AWS Serverless Application Model (AWS SAM) templates to the cloud just to test changes, which created significant delays in the feedback loop. Friction from Tool Context Switching: The need to constantly move between integrated development environments (IDEs), command-line interfaces (CLIs), and resource emulators like LocalStack leads to fragmented and inefficient workflows. Complex Manual Setup: Manually configuring port mapping and making code edits for local integration tests can introduce inconsistencies between the local and cloud environments. Limited Service Integration Debugging: Troubleshooting Lambda functions that interact with other AWS services, such as DynamoDB or Amazon SQS, has traditionally required complex manual configuration, extending the time needed to resolve issues. Automated Setup Process The LocalStack VSCode Extension can be installed directly from the AWS Toolkit, which provides an intelligent wizard for a streamlined setup. This wizard automatically detects if LocalStack is configured and guides the user through the process. It also handles authentication through a browser-based flow and securely stores the token. Furthermore, the wizard checks for and creates the necessary AWS CLI profiles for LocalStack, allowing developers to easily switch between their local and cloud environments.\nTesting a Serverless Application The article demonstrates these capabilities with a practical example: an event-driven order processing system that uses API Gateway, Amazon SQS, Lambda, and Amazon Simple Notification Service (Amazon SNS).\nWith the new integration, the entire workflow can be tested locally:\nDeploy Locally: The AWS SAM application is deployed to the local LocalStack environment using the LocalStack AWS profile. Debug Locally: Developers can set breakpoints in their Lambda function code directly in VS Code and use the integrated debugger to step through the execution as it interacts with other locally emulated services. Validate End-to-End: The complete workflow, from message ingestion at the API Gateway to the final notification from Amazon SNS, can be tested to confirm all service integrations work correctly before deploying to the cloud. For an in-depth technical demonstration of this LocalStack integration, refer to this youtube video.\nBest Practices for Local Testing To make the most of this new workflow, the article recommends a layered and strategic approach to testing. This involves starting with fast, isolated unit tests to validate core logic, and then progressively moving to broader integration and system-level validation.\nThe recommended testing strategy follows four main steps:\nBegin with local unit tests to focus on isolated function logic. Proceed to local integration testing using LocalStack to confirm interactions between AWS services. After local validation, test the application in the actual AWS environment to surface issues that cannot be emulated, such as IAM permission mismatches or VPC networking challenges. Finally, conduct performance and load testing in AWS to assess how the application handles real-world traffic. When to Use Local Versus Cloud Testing While local testing provides significant speed and cost advantages, it\u0026rsquo;s important to understand its limitations and when to test in the cloud. The following table lists potential use cases for each strategy.\nTesting Scenario Local Testing Cloud Testing Reason Function logic validation ✓ Fast feedback for core business logic Service integration testing ✓ Quick validation of AWS service interactions Rapid iteration during development ✓ Immediate feedback without deployment overhead Cost-sensitive development environments ✓ Minimizes cloud resource costs during development Offline development scenarios ✓ No internet connectivity required Performance and scalability testing ✓ Requires actual AWS infrastructure for accurate results IAM permission validation ✓ LocalStack doesn\u0026rsquo;t fully replicate IAM behavior VPC networking scenarios ✓ Network configurations can\u0026rsquo;t be accurately emulated Production-like load testing ✓ Real performance metrics only available in AWS Final validation before deployment ✓ Supports compatibility with actual AWS environment Conclusion The integration of LocalStack into the AWS Toolkit for VS Code significantly enhances the local development experience for serverless applications. By allowing developers to run and debug complex, multi-service applications directly in their IDE, this new capability helps reduce context switching, catch issues earlier, and lower development costs. This leads to faster test cycles and higher-quality deployments, all while keeping the developer in full control of their local environment.\n"
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Create a gateway endpoint",
	"tags": [],
	"description": "",
	"content": " Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "
},
{
	"uri": "http://localhost:1313/internship_report/4-eventparticipated/4.2-event2/",
	"title": "Data Science On AWS",
	"tags": [],
	"description": "",
	"content": "Summary Report Event Objectives Provide a comprehensive overview of building a modern Data Science system on AWS. Introduce the end-to-end Data Science pipeline, from data processing to model deployment. Offer hands-on experience with key AWS services like AWS Glue and Amazon SageMaker. Discuss practical considerations such as cost, performance, and the benefits of cloud vs. on-premise solutions. Speakers Văn Hoàng Kha – Cloud Solutions Architect, AWS Community Builder Bạch Doãn Vương – Cloud Develops Engineer, AWS Community Builder Key Highlights End-to-End Data Science Pipeline on AWS The workshop outlined the complete data science journey on the cloud, using core services:\nAmazon S3: For scalable data storage. AWS Glue: For serverless data integration, ETL (Extract, Transform, Load), and data cleaning. Amazon SageMaker: For building, training, and deploying machine learning models at scale. Practical Demonstrations Demo 1: Data Processing with AWS Glue: Showcased how to process and clean a real-world IMDb dataset, emphasizing the importance of data quality for model accuracy. Demo 2: Model Training with SageMaker: Demonstrated the process of training and deploying a Sentiment Analysis model, making the abstract concepts of ML deployment concrete. Integrating Custom Models: Showcased how to leverage frameworks like TensorFlow and PyTorch within SageMaker, using a sample project from a provided GitHub repository. Broadening AI/ML Capabilities with Managed Services An overview of AWS\u0026rsquo;s pre-built AI services that accelerate development:\nAmazon Transcribe: Speech-to-text conversion. Amazon Comprehend: Natural language processing for sentiment analysis and topic extraction. Amazon Rekognition: Image and video analysis. Amazon Personalize: Building personalized recommendation systems. Key Takeaways Data-First Mindset Business-first approach: Always start with the business context of the data, as emphasized by the need for feature engineering. Data quality is paramount: The workshop stressed that the accuracy of any ML model is directly dependent on the quality of the input data. Data as an asset: Data collection, governance, and security are the foundational pillars of a data-driven organization. Technical Architecture Modular Pipeline: The standard architecture involves a pipeline from S3 (storage) to AWS Glue (ETL) to Amazon SageMaker (ML), allowing for a clean separation of concerns. Flexibility: AWS supports both low-code solutions like SageMaker Canvas and code-intensive custom model training using frameworks like TensorFlow/PyTorch. Serverless benefits: Using services like AWS Glue removes the need for managing infrastructure, allowing teams to focus on data and models. Strategy Phased approach: Start with data collection and cleaning before moving to complex model training. Cloud vs. On-premise: The discussion highlighted that the cloud offers significant advantages in scalability, pay-for-what-you-use cost models, and access to powerful computing resources without upfront investment. ROI Measurement: Leverage cloud benefits to reduce development time and infrastructure overhead, leading to faster time-to-market for AI-powered features. Applying to Work Automate ETL: Use AWS Glue to create automated data cleaning and preparation jobs for analytics and ML. Adopt SageMaker: Pilot Amazon SageMaker for training and deploying ML models to streamline the MLOps lifecycle. Implement Sentiment Analysis: Apply the concepts from the demo to analyze customer feedback from reviews or support tickets. Explore Pre-built AI: Integrate services like Amazon Rekognition for content moderation or Amazon Transcribe for call center analytics. Consolidate Knowledge: Build a small project based on the workshop\u0026rsquo;s guidance to reinforce the concepts learned. Event Experience Attending the \u0026ldquo;Data Science on AWS\u0026rdquo; workshop provided a valuable, hands-on journey into cloud-based machine learning. Key experiences included:\nLearning from highly skilled speakers The speakers, both AWS Community Builders, shared practical insights and best practices from their real-world experience. Hands-on technical exposure The live demos of processing data with AWS Glue and training a model with SageMaker were extremely effective at translating theory into practice. Leveraging modern tools Explored the comprehensive AWS ecosystem for data science, understanding how different services fit together to form a cohesive pipeline. Learned about both fully managed AI services and the powerful customization options available within SageMaker. Lessons learned A solid data preparation strategy is non-negotiable for success in machine learning. AWS significantly lowers the barrier to entry for building and deploying sophisticated data science systems. Modern cloud platforms provide the flexibility to choose between low-code tools for speed and custom code for specific, complex requirements. Some event photos Overall, the workshop provided not only technical knowledge but also practical experience with building end-to-end data science pipelines on AWS, emphasizing the importance of data quality and the power of cloud-native ML services.\n"
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "
},
{
	"uri": "http://localhost:1313/internship_report/4-eventparticipated/4.3-event3/",
	"title": "Reinventing DevOps with AWS Generative AI",
	"tags": [],
	"description": "",
	"content": "Summary Report Event Objectives Share the current context of DevOps and the impact of Generative AI. Present real-world case studies and practical frameworks for integrating AI into DevOps. Provide a live demonstration of AWS Generative AI tools that enhance the development lifecycle. Discuss the evolving role of DevOps engineers and the skills needed for the future. Speakers Lê Thanh Đức – Cloud Delivery Manager, CMC Global Dư Quốc Thành – Technical Leader, CMC Global Văn Hoàng Kha – Cloud Engineer, AWS Community Builder Key Highlights The Evolution from DevOps to DevSecOps The modern DevOps mindset integrates security from the start, making it a shared responsibility across development, security, and operations teams. Shifting from a reactive, post-development security model to a proactive approach where security is embedded in every phase. This cultural shift is the core of DevSecOps, aiming to balance development speed with system security. Phases of a Secure DevOps Lifecycle A comprehensive, seven-phase approach to embedding security throughout the pipeline:\nPlan: Define security requirements and perform threat modeling. Code: Use static analysis (SAST) tools to detect vulnerabilities early. Build: Automate security checks, dependency scans, and configuration validation. Test: Integrate penetration testing and compliance auditing. Deploy: Scan Infrastructure as Code (IaC) to ensure secure environments. Operate: Automate patching, incident response, and remediation. Monitor: Use real-time analytics and AI-powered anomaly detection for proactive defense. Leveraging AI in the DevOps Toolchain Automation: AI automates repetitive tasks like code review, log analysis, vulnerability scanning, and filtering false positives. Enhanced Security: AI-driven tools can prioritize critical risks, suggest fixes, and detect anomalous behavior in runtime environments. Efficiency: AI assists in generating documentation, reports, and compliance policies, reducing manual workload. Tooling Examples: The session highlighted tools like SonarQube, Checkov, Prometheus, and GitHub Actions, along with AI\u0026rsquo;s role in enhancing their capabilities. AWS Tools for AI-Enhanced DevOps Amazon CodeGuru: A service demonstrated to scan code for vulnerabilities (e.g., SQL injection, secret leaks) and provide actionable recommendations for fixes. AWS Managed Control Plane (MCP) \u0026amp; Base (MCB): Tools for automating security compliance and updates for Terraform and Kubernetes (EKS) configurations. Cost Optimization: AI/ML services like AWS Cost Anomaly Detection and Compute Optimizer help predict resource needs and reduce waste. Key Takeaways Security Mindset Proactive Integration: Always start with security in mind, embedding it into the earliest stages of planning and development, not as an afterthought. Shared Responsibility: Foster a culture where developers, operations, and security teams are collectively responsible for security. Continuous Improvement: Use feedback loops from monitoring and incidents to continuously refine security processes. Technical Architecture Automated Security Pipeline: Embed automated security checks at every stage of the CI/CD pipeline, from code scanning to deployment. Observability: Implement robust monitoring, logging, and alerting systems (e.g., Prometheus, Grafana, Loki) to gain real-time insights into system health and security. Infrastructure as Code (IaC) Security: Utilize tools to scan IaC configurations to prevent misconfigurations before they reach production. AI Integration Strategy Phased Approach: Select and adopt AI tools that fit specific project needs to avoid performance overhead and unnecessary complexity. Human-in-the-Loop: View AI as a powerful assistant that enhances human capabilities, not as a replacement. Human oversight and judgment remain critical. Measure ROI: The integration of AI should be measured by its ability to increase development velocity, improve security posture, and reduce manual effort. Applying to Work Enhance CI/CD: Integrate automated static analysis (SAST) and dependency scanning tools into current pipelines. Adopt IaC Scanning: Implement tools like Checkov to validate Terraform or other IaC scripts. Pilot AWS AI Tools: Experiment with Amazon CodeGuru on a small-scale project to review code quality and security. Improve Monitoring: Leverage AI-powered anomaly detection to get proactive alerts on potential issues. Automate Documentation: Use AI to assist in generating and maintaining project documentation and reports. Event Experience Attending the \u0026ldquo;Reinventing DevOps with AWS Generative AI\u0026rdquo; session was highly valuable, offering a comprehensive overview of how AI is reshaping security and efficiency in software development. Key experiences included:\nLearning from highly skilled speakers Experts from CMC Global and AWS Vietnam shared deep insights from their extensive experience in cloud and DevOps. Through real-world case studies from clients in the Philippines and Singapore, I gained a practical understanding of implementing secure CI/CD pipelines. Hands-on technical exposure The live demonstration of Amazon CodeGuru was particularly insightful, showing how AI can concretely identify vulnerabilities and suggest code fixes in real-time. Leveraging modern tools Explored a modern DevOps toolchain, including SonarQube, Checkov, Prometheus, and GitLab CI, and understood how AI integrates with them. Learned how to use AI for infrastructure management and compliance with tools like AWS MCP and MCB. Networking and discussions The interactive Q\u0026amp;A session offered a chance to dive deeper into specific topics, such as the evolution of DevOps roles, AI\u0026rsquo;s limitations, and career advice for cloud architects. Discussions reinforced the importance of balancing AI automation with human expertise and critical thinking. Lessons learned Shifting to a DevSecOps culture is essential for building secure and reliable applications at speed. AI tools like Amazon CodeGuru can significantly boost productivity and security, but they require human oversight to verify and implement suggestions effectively. Modernization requires a clear strategy; a phased approach to adopting new tools and processes is less risky and more effective. Overall, the event provided not only technical knowledge but also reshaped my thinking about the future of DevOps, the indispensable role of integrated security, and the collaborative potential between AI and human engineers.\n"
},
{
	"uri": "http://localhost:1313/internship_report/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Learn to write worklog and working handle workshop. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 09/08/2025 09/08/2025 Policies: https://policies.fcjuni.com/ 3 - Learn about AWS and its types of services (for further project) + Compute + Storage + Networking + Database + Etc. - Learn how to write workshop via video and instructions\n09/09/2025 09/09/2025 About AWS: https://cloudjourney.awsstudygroup.com/ About workshop: https://van-hoang-kha.github.io/vi/ 4 - Apply previous day knowledge about workshop to write worklog - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Try out: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 09/10/2025 09/10/2025 My workshop git: https://github.com/isntbim/internship_report AWS Console: https://aws.amazon.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + Storage + Test launch an EC2 instance - SSH connection methods to EC2 - Learn about Elastic IP 09/11/2025 09/11/2025 EC2 console: https://ap-southeast-1.console.aws.amazon.com/ec2/ Amazon EC2 Basics: https://www.coursera.org/learn/aws-amazon-ec2-basics/ 6 - Further practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 09/12/2025 09/12/2025 EC2 console: https://ap-southeast-1.console.aws.amazon.com/ec2/ Week 1 Achievements (not finished): Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database Etc. Successfully created and configured an AWS Free Tier account.\nGetting used to workshop and writing worklogs.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region Etc. Learned basic EC2 concepts:\nInstance types: Balancing between cost and performance. AMI: Pre-configured templates for launching instances. EBS: Persistent block storage for instances. Storage: Different types of storage options for various use cases. Elastic IP: Static IP addresses for dynamic cloud computing. "
},
{
	"uri": "http://localhost:1313/internship_report/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Define and scope the first typing game project (core features, microservice boundaries, future matchmaking). Establish team foundation: shared repo, initial backlog, ER diagrams, tech stack, ownership. Standardize WPM and accuracy formulas. Prototype FastAPI service: text generation, sentence assembly, chat; validate Bedrock integration path. Set up AWS Budgets with alerting. Gain working proficiency: Lambda (function URL), VPC (subnets, gateways, peering vs transit), Flow Logs, load balancing concepts. Provision Amazon RDS; design schema and seed dataset for text retrieval. Refactor text service to DB-backed retrieval; benchmark vs prior API method. Introduce early operational practices: role assignment, benchmarking, monitoring for scalability. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Hold a team brainstorming session to define and prioritize concepts for the first project + Transfer our brainstorming notes from the project canvas into an initial task backlog on the project management board + Research and document the specific algorithms for calculating WPM and accuracy to ensure consistency +Create the shared code repository and establish the basic project structure for the chosen languages and microservices + Refine the sketched ER diagrams and begin drafting the initial database schemas + Formally assign lead responsibility for each microservice to individual team members to streamline development 09/15/2025 09/15/2025 3 - Set up AWS Budgets: + Review budget types (Cost, Usage, RI, etc.) + Define monthly cost threshold + Configure budget in the AWS console + Set up email/SNS alerts - Create a web app using AWS Lambda: + Learn Lambda and function URL fundamentals + Code a simple \u0026ldquo;Hello World\u0026rdquo; function + Configure the function and its IAM role + Enable and test the function URL endpoint - Learn and test FastAPI for microservices: + Go through the official FastAPI tutorial + Set up a local development environment + Create a proof-of-concept API + Implement and test basic endpoints using Swagger UI 09/16/2025 09/16/2025 AWS Lambda: https://ap-southeast-1.console.aws.amazon.com/lambda AWS Budgets: https://us-east-1.console.aws.amazon.com/costmanagement/ FastAPI: https://www.coursera.org/learn/packt-mastering-rest-apis-with-fastapi-1xeea/ 4 - Exploring the Fundamentals of AWS Networking and Security + Review the core concept of an Amazon VPC (Virtual Private Cloud) as an isolated section of the AWS cloud + Understand the purpose of subnets, and the distinction between public and private subnets for structuring a network + Learn the roles of an Internet Gateway for providing internet access to public subnets and a NAT Gateway for allowing private subnets to access the internet securely + Explore VPC Flow Logs as a tool for monitoring and troubleshooting network traffic within your VPC + Compare methods for connecting an on-premise data center to AWS: Site-to-Site VPN for encrypted connections over the internet and Direct Connect for a dedicated, private connection + Understand the use cases for VPC Peering (connecting two VPCs directly) versus a Transit Gateway + Grasp the overall purpose of Elastic Load Balancing (ELB) to distribute application traffic across multiple servers for high availability 09/17/2025 09/17/2025 Module 02-(01 to 03): https://www.youtube.com/watch?v=O9Ac_vGHquM https://www.youtube.com/watch?v=BPuD1l2hEQ4 https://www.youtube.com/watch?v=CXU8D3kyxIc 5 - Explore the Amazon Bedrock playground: + Review the available foundation models (e.g., Claude, Titan) + Select a model for text generation + Experiment with different prompts and parameters + Generate and analyze a sample response - Prototype the designated microservice: + Structured the service\u0026rsquo;s logic and integrated multiple APIs for generating text + Implemented the core functions for creating random sentences and managing a chat feature + Reviewed the initial build to identify limitations and outline next steps for improvement + Validate our technical approach 09/18/2025 09/18/2025 Amazon Bedrock:https://ap-southeast-1.console.aws.amazon.com/bedrock 6 - Launch and Configure an Amazon RDS Database: + Review and select a suitable database engine for the project\u0026rsquo;s needs + Configure the core instance settings, including credentials, VPC, and security group access rules + Launch the database, monitor its creation, and securely record the connection endpoint - Prototype a Database-Driven TextService: + Design a simple database schema with tables for words and sentences to store text content + Create a one-time script to populate the new RDS database with an initial dataset + Refactor the TextService to fetch data from the database instead of external APIs and benchmark the performance improvement + Run a simple benchmark to compare the response times between the old API-based method and the new database query method 09/19/2025 09/19/2025 Aurora and RDS: https://ap-southeast-1.console.aws.amazon.com/rds Week 2 Achievements: Defined first typing game scope:\nCore features Microservice boundaries Backlog seeded Ownership assigned. Standard WPM and accuracy formulas researched and documented.\nShared repository initialized with baseline multi-language structure.\nER diagrams refined and initial relational schema drafted.\nFastAPI prototype delivered (text generation, sentence assembly, chat) and verified via Swagger UI.\nAWS Budgets configured with monthly threshold and alerting.\nCore AWS foundations learned:\nLambda (function URL) VPC (subnets, IGW, NAT, Flow Logs) Peering vs transit gateway Load balancing concepts Amazon Bedrock models explored; candidate model and prompt approach validated.\nRDS instance launched, schema created, seed dataset loaded.\nTextService refactored to DB-backed retrieval with initial performance improvement benchmarked.\nEarly operational practices introduced: role ownership, benchmarking focus, scalability considerations.\n"
},
{
	"uri": "http://localhost:1313/internship_report/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Complete foundational AWS hands-on labs (Site-to-Site VPN, EC2 fundamentals). Progress through and finish all four modules of the AWS Cloud Technical Essentials course. Strengthen practical usage of AWS Console and CLI (configuration, key management, region/service discovery). Collaborate with product/design to review and document TypeRush Figma UI/UX for future implementation. Evaluate storage options and decide on a scalable NoSQL approach for TextService data. Prototype and integrate MongoDB (environment setup, seeding, service refactor, validation). Build effective communication cadence with First Cloud Journey team members. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Lab 03: AWS Site-to-Site VPN: + Create a complete Site-to-Site VPN environment, including a new VPC, an EC2 instance to act as a customer gateway, a Virtual Private Gateway, and the VPN connection itself. + Configure and test the VPN tunnel connectivity. - Lab 04: Amazon EC2 Fundamentals: + Launch and connect to both Microsoft Windows Server and Amazon Linux EC2 instances. + Deploy a sample \u0026ldquo;AWS User Management\u0026rdquo; application on both Windows and Linux environments to practice basic CRUD operations. + Explore core EC2 features like modifying instance types, managing EBS snapshots, and creating custom AMIs. 09/22/2025 09/22/2025 VPN Lab (Lab 03): https://000003.awsstudygroup.com/ EC2 Lab (Lab 04): https://000004.awsstudygroup.com/ 3 - Getting start with AWS Cloud Technical Essentials course, covering 2 modules: + Module 1: Cloud Foundations \u0026amp; IAM - Define cloud computing and its value proposition. - Differentiate between on-premises and cloud workloads. - Create an AWS account and review different methods for interacting with AWS services. - Describe the AWS Global Infrastructure, including Regions and Availability Zones. - Learn and apply best practices for AWS Identity and Access Management (IAM). + Module 2: Compute \u0026amp; Networking - Review the basic components of Amazon EC2 architecture. - Differentiate between containers and virtual machines. - Discover the features and advantages of serverless technologies. - Learn basic networking concepts and the features of Amazon Virtual Private Cloud (VPC). - Create a VPC. 09/23/2025 09/23/2025 AWS Cloud Technical Essentials: https://www.coursera.org/learn/aws-cloud-technical-essentials 4 - Collaborate and Document Figma Design for TypeRush UI/UX: + Participate in a dedicated cross-functional design review meeting to comprehensively examine the latest Figma mockups provided by the UI/UX team. + Systematically analyze each key screen (e.g., login/registration, main game interface, post-game score summary, settings menu) to grasp the visual hierarchy, component states, and intended user interactions. + Compile a list of initial technical feasibility questions and potential UI implementation considerations to facilitate further discussion and alignment between design and engineering. + Begin translating key design elements into preliminary front-end component requirements or user stories, setting the groundwork for future development sprints. - Discuss Text Service Storage with Team Lead \u0026amp; Confirm NoSQL Choice: + Engage in a focused discussion with the team leader regarding the optimal database solution for the TextService (e.g., for storing words and sentences). + Present the pros and cons of relational (SQL) vs. non-relational (NoSQL) options in the context of our data structure and anticipated access patterns. + Confirm the decision to proceed with a NoSQL database as the chosen storage solution due to its flexibility and scalability for text content. 09/24/2025 09/24/2025 5 - Integrate and Test MongoDB with TextService Prototype: + Set up MongoDB Environment: Set up a local instance using Docker. + Refactor Data Seeding Script: Modify the population script to insert the words and sentences into MongoDB collections as documents. + Rewrite Service Logic: Update the TextService data retrieval methods to query the MongoDB collections instead of the previous data source. + Verify Integration: Thoroughly test the refactored service to confirm that it can successfully connect, write to, and read from the MongoDB database. 09/25/2025 09/25/2025 6 - Completing AWS Cloud Technical Essentials course, with 2 final modules: + Module 3: Storage \u0026amp; Databases - Differentiate between file, block, and object storage models. - Explain core Amazon S3 concepts like buckets and objects, then create an S3 bucket. - Describe the function and use cases of Amazon EBS with EC2. - Explore the various database services available on AWS. - Understand the function of Amazon DynamoDB and create a DynamoDB table. + Module 4: Monitoring \u0026amp; High Availability - Define the benefits of monitoring and the role of Amazon CloudWatch. - Understand how to optimize solutions for performance and cost on AWS. - Describe the function of Elastic Load Balancing (ELB) to route and distribute traffic. - Differentiate between vertical scaling (scaling up) and horizontal scaling (scaling out). - Configure a solution for high availability. 09/26/2025 09/26/2025 AWS Cloud Technical Essentials: https://www.coursera.org/learn/aws-cloud-technical-essentials Week 3 Achievements: AWS Infrastructure Labs:\nBuilt full Site-to-Site VPN (VPC, customer gateway EC2, virtual private gateway, tunnel validation) Executed EC2 fundamentals (Windows \u0026amp; Linux instances, CRUD app deployment, snapshots, custom AMIs) Finished all four modules of AWS Cloud Technical Essentials (Foundations/IAM, Compute \u0026amp; Networking, Storage \u0026amp; Databases, Monitoring \u0026amp; High Availability).\nAWS Console \u0026amp; CLI Proficiency:\nAccount setup, credential \u0026amp; config management Region \u0026amp; service exploration Key pair handling and resource inspection workflows TypeRush UI/UX Documentation:\nAnalyzed Figma screens (login, game, score summary, settings) and navigation flows Captured component states \u0026amp; feasibility questions Drafted initial user stories / component requirements TextService Storage Architecture:\nEvaluated SQL vs NoSQL trade*offs for word/sentence storage Selected NoSQL approach for flexibility \u0026amp; scalability MongoDB Integration Prototype:\nDockerized local MongoDB environment Converted seeding script to insert documents Refactored service data layer to query MongoDB Validated read/write operations end-to-end "
},
{
	"uri": "http://localhost:1313/internship_report/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Learn to configure Amazon RDS databases with VPC setup, security groups, and backup operations. Study scalable web application deployment using Auto Scaling Groups and Application Load Balancers. Understand monitoring techniques with CloudWatch metrics, logs, alarms, and dashboards. Explore hybrid DNS solutions using Route 53 Resolver for enterprise networking. Develop AWS CLI skills for automated resource management across S3, EC2, VPC, and IAM. Practice building CI/CD pipelines with CodeCommit, CodeBuild, CodeDeploy, and CodePipeline. Implement automated backup strategies using AWS Backup with lifecycle policies. Learn containerized application deployment with Docker on AWS and container registries. Study VM migration workflows including import/export operations between environments. Build serverless applications using AWS Lambda and API Gateway with IAM configuration. Understand security monitoring using AWS Security Hub with multi-service integration. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Configure and Manage a Relational Database with Amazon RDS: + Create a VPC, security groups for EC2 and RDS, and a DB subnet group + Launch an EC2 instance and create an RDS database instance + Deploy a sample application on the EC2 instance to connect to the RDS database + Perform backup and restore operations for the RDS instance + Practice resource cleanup by deleting the created resources - Deploy and configure a scalable web application using Auto Scaling: + Set up the necessary network infrastructure, including a VPC, subnets, and security groups + Create an EC2 Launch Template to define the configuration for new instances + Configure a Target Group and an Application Load Balancer to manage and distribute incoming traffic + Create and configure an Auto Scaling Group to automatically manage the number of EC2 instances + Test various scaling solutions, including manual, scheduled, and dynamic scaling policies + Clean up all created AWS resources to avoid ongoing charges - Monitor and Analyze AWS Resources with CloudWatch: + Explore and analyze metrics from various AWS services using search and math expressions + Investigate and query log data using CloudWatch Logs Insights + Create a Metric Filter to extract data from log events for monitoring + Configure a CloudWatch Alarm to trigger notifications based on a specific metric threshold + Build a custom CloudWatch Dashboard to visualize key metrics and alarms + Perform a cleanup of all created resources, including alarms and dashboards 29/09/2025 29/09/2025 Amazon RDS: https://000005.awsstudygroup.com/ Auto Scaling Group: https://000006.awsstudygroup.com/ CloudWatch Metrics https://000008.awsstudygroup.com/ 3 - Implement a Hybrid DNS Solution with Route 53 Resolver: + Prepare the lab environment by deploying foundational infrastructure using a CloudFormation template + Deploy a Microsoft Active Directory instance to simulate an on-premises DNS server + Create a Route 53 Resolver outbound endpoint to forward DNS queries from the VPC + Configure Route 53 Resolver rules to direct DNS queries to the appropriate resolver + Set up a Route 53 Resolver inbound endpoint to allow the on-premises network to query VPC resources + Test the hybrid DNS name resolution and then clean up all deployed resources - Manage AWS Services using the Command Line Interface (CLI): + Install and configure the AWS CLI with an access key, secret key, and default region + Practice using the CLI to view and describe resources in various services like S3, SNS, and IAM + Perform Amazon S3 operations, such as creating buckets and managing objects, via the command line + Create and manage VPC components, including an Internet Gateway, using CLI commands + Launch, describe, and terminate an Amazon EC2 instance entirely from the command line + Clean up all resources created during the lab to avoid incurring costs 30/09/2025 30/09/2025 Route 53 Resolver: https://cloudjourney.awsstudygroup.com/ AWS CLI: https://000011.awsstudygroup.com/ 4 - Build a CI/CD Pipeline to Automate Application Deployment: + Create a version control repository using AWS CodeCommit to store the application\u0026rsquo;s source code + Configure a build project with AWS CodeBuild to compile, test, and package the application + Set up a deployment group and application in AWS CodeDeploy to manage the deployment process + Create a unified CI/CD pipeline with AWS CodePipeline to orchestrate the entire workflow + Test the end-to-end pipeline by pushing a code change and verifying the automated deployment + Clean up all AWS resources created during the lab to avoid unnecessary charges - Automate EC2 Instance Backups with AWS Backup: + Deploy the necessary infrastructure, including a new VPC and an EC2 instance, using a CloudFormation template + Create a backup plan in AWS Backup to define backup frequency, retention policies, and lifecycle rules + Configure notification settings to receive alerts on the status of backup jobs + Test the backup and restore process to ensure data can be successfully recovered + Clean up all resources, including the CloudFormation stack and any created backups 01/10/2025 01/10/2025 AWS IAM Identity Center: https://000012.awsstudygroup.com/ AWS Backup: https://000013.awsstudygroup.com/ 5 - Deploy a Dockerized Application on AWS: + Configure the necessary AWS infrastructure, including a VPC, security groups, and IAM roles + Launch an Amazon RDS instance to serve as the application\u0026rsquo;s database + Set up an EC2 instance and install the required dependencies for running the application + Deploy and test the application on the EC2 instance using a Docker image + Redeploy the application using Docker Compose to manage multiple containers + Push the Docker image to a container registry, such as Amazon ECR or Docker Hub + Clean up all created AWS resources to avoid incurring further charges - Migrate a Virtual Machine to and from AWS: + Export a virtual machine from an on-premises environment + Upload the exported virtual machine image to an Amazon S3 bucket + Import the virtual machine from S3 into AWS to create a new Amazon Machine Image (AMI) + Deploy a new EC2 instance from the imported AMI + Export an existing EC2 instance back to an S3 bucket + Clean up all created resources, including the S3 bucket and EC2 instance 02/10/2025 02/10/2025 Docker on AWS: https://000015.awsstudygroup.com/ VM Import/Export: https://000014.awsstudygroup.com/ 6 - Deploy a Serverless Application with Lambda and API Gateway: + Prepare and zip the Lambda deployment package, ensuring it includes the function\u0026rsquo;s source code and all required dependencies + Define and create an IAM role with the necessary execution permissions and a trust policy that allows Lambda to assume the role + Create the Lambda function in the AWS Console, specifying the runtime and uploading the zipped deployment package + Build an HTTP API endpoint using Amazon API Gateway and configure its integration to invoke the Lambda function + Deploy the API Gateway to a stage and test the end-to-end serverless application by accessing the public URL + Clean up all associated resources, including the API Gateway, the Lambda function, and the IAM role, to prevent ongoing costs - Aggregate and Prioritize Security Findings with AWS Security Hub: + Enable AWS Security Hub to begin aggregating security data from various AWS services + Review the integrated dashboard to visualize security alerts and findings + Organize and prioritize security detections from services like Amazon GuardDuty, Inspector, and Macie + Explore the summarized risks presented in interactive charts and tables 03/10/2025 03/10/2025 AWS Lambda: https://000016.awsstudygroup.com/ AWS Security Hub: https://000018.awsstudygroup.com/ Week 4 Achievements: Successfully configured and managed Amazon RDS databases:\nVPC setup with security group configuration DB subnet group creation and EC2-RDS integration Application deployment with database connectivity Backup and restore operations implementation Deployed scalable web applications using Auto Scaling infrastructure:\nNetwork infrastructure setup with VPC, subnets, and security groups EC2 Launch Template configuration for standardized instances Application Load Balancer and Target Group implementation Auto Scaling Group configuration with scaling policies Implemented comprehensive monitoring with CloudWatch services:\nMetrics exploration using search and math expressions Log data analysis with CloudWatch Logs Insights Custom alarm configuration with notification thresholds Interactive dashboard development for visualization Configured hybrid DNS solutions using Route 53 Resolver:\nCloudFormation-based infrastructure deployment Microsoft Active Directory integration for simulation Outbound and inbound endpoint configuration Bidirectional name resolution testing Developed AWS CLI proficiency for automated resource management:\nCLI installation and configuration with access credentials Multi-service resource management (S3, SNS, IAM) S3 operations including bucket and object management EC2 instance lifecycle management through CLI Built automated CI/CD pipelines using AWS developer services:\nVersion control repository setup with AWS CodeCommit Build project configuration with AWS CodeBuild Deployment automation using AWS CodeDeploy End-to-end pipeline orchestration with CodePipeline Implemented automated backup strategies with AWS Backup:\nInfrastructure deployment using CloudFormation templates Backup plan creation with lifecycle policies Notification configuration for backup job monitoring Disaster recovery testing through backup processes Deployed containerized applications using Docker on AWS:\nAWS infrastructure configuration including VPC and IAM roles RDS database integration for containerized applications Docker image deployment and testing on EC2 instances Container registry operations with Amazon ECR Completed VM migration workflows between environments:\nVirtual machine export from on-premises systems S3-based image storage and transfer operations VM import processes for AMI creation EC2 instance deployment from imported images Built serverless applications using AWS Lambda and API Gateway:\nLambda deployment package preparation and configuration IAM role creation with execution permissions Function deployment and runtime configuration API Gateway endpoint creation and Lambda integration Configured centralized security monitoring with AWS Security Hub:\nSecurity Hub enablement for multi-service data aggregation Dashboard utilization for security alert visualization Security finding organization from GuardDuty, Inspector, and Macie Risk analysis using interactive charts and tables "
},
{
	"uri": "http://localhost:1313/internship_report/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Implement advanced AWS networking with VPC Peering and Transit Gateway for multi-VPC connectivity. Deploy full-stack applications using EC2, RDS, Auto Scaling, and CloudFront integration. Create serverless cost optimization solutions with AWS Lambda for automated EC2 management. Build CI/CD pipelines using AWS Developer Tools for automated application deployment. Configure hybrid cloud storage with AWS Storage Gateway for on-premises integration. Manage enterprise file systems with Amazon FSx and secure web applications using AWS WAF. Organize AWS resources effectively using Tags and Resource Groups for governance. Develop proficiency with both AWS Management Console and CLI operations. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Set up VPC Peering between two VPCs: + Initialize CloudFormation Template to create the initial environment + Create Security Groups to control traffic to the EC2 instances + Create EC2 instances in each VPC for testing connectivity + Update Network ACLs to allow traffic between the peered VPCs + Create and accept the VPC Peering connection + Configure Route Tables to direct traffic between the peered VPCs + Enable Cross-Peer DNS for name resolution between VPCs - Implement a scalable network architecture with AWS Transit Gateway: + Generate a Key Pair for secure access to instances + Initialize the environment using a CloudFormation Template + Create a Transit Gateway to act as a central network hub + Attach VPCs to the Transit Gateway to enable communication + Configure Transit Gateway Route Tables to control traffic flow + Update VPC Route Tables to route traffic through the Transit Gateway 08/11/2025 08/11/2025 VPC Peering: https://000019.awsstudygroup.com/ AWS Transit Gateway: https://000020.awsstudygroup.com/ 3 - Deploy WordPress on AWS Cloud: + Prepare VPC and Subnets for the network infrastructure + Create Security Groups for EC2 and Database instances to control access + Launch an EC2 instance to host the WordPress application + Launch a Database instance using Amazon RDS for the WordPress database + Install and configure WordPress on the EC2 instance + Implement Auto Scaling for the WordPress instance + Perform database backup and restore operations + Set up CloudFront for the web server to improve performance and security - Optimize EC2 Costs with Lambda: + Create tags for EC2 instances to identify them for cost optimization + Create an IAM Role for the Lambda function to grant necessary permissions + Create a Lambda function to automatically stop and start EC2 instances + Test the Lambda function to ensure it is working correctly 08/12/2025 08/12/2025 WordPress on AWS: https://000021.awsstudygroup.com/ Lambda Cost Optimization: https://000022.awsstudygroup.com/ 4 - Automate Application Deployment with CI/CD Pipeline: + Prepare the necessary resources for the CI/CD pipeline + Install and configure the CodeDeploy Agent on the EC2 instances + Create a CodeCommit repository to store the application source code + Configure CodeBuild to compile and build the application + Set up CodeDeploy to automate the deployment process + Create a CodePipeline to orchestrate the entire CI/CD workflow + Troubleshoot any issues that may arise during the pipeline execution - Utilize AWS Storage Gateway for hybrid cloud storage: + Create an S3 Bucket to store data in the cloud + Set up an EC2 instance to host the Storage Gateway + Create a Storage Gateway to connect on-premises environments with AWS storage + Create File Shares on the Storage Gateway for file-based access + Mount the File Shares on an on-premises machine to access cloud storage 08/13/2025 08/13/2025 AWS CodePipeline: https://000023.awsstudygroup.com/ Storage Gateway: https://000024.awsstudygroup.com/ 5 - Manage Amazon FSx for Windows File Server: + Create the initial environment for the file server + Create both SSD and HDD Multi-AZ file systems + Create new file shares on the file systems + Test and monitor the performance of the file systems + Enable data deduplication and shadow copies for storage optimization and data protection + Manage user sessions, open files, and user storage quotas + Enable Continuous Access share for high availability + Scale throughput and storage capacity as needed + Delete the environment to clean up resources + Reference the AWS CLI for managing the file server - Implement AWS Web Application Firewall (WAF): + Create an S3 Bucket and deploy a sample web application for testing + Use AWS WAF with managed rules to protect against common threats + Create custom and advanced custom rules for specific security needs + Test new rules to ensure they are working correctly + Log requests for monitoring and analysis + Clean up all created resources to avoid ongoing charges 08/14/2025 08/15/2025 Amazon FSx: https://000025.awsstudygroup.com/ AWS WAF: https://000026.awsstudygroup.com/ 6 - Manage AWS resources using Tags and Resource Groups: + Understand and use tags on the AWS Management Console + Create an EC2 instance with tags + Add or remove tags from existing resources + Filter resources based on tags + Learn to use tags with the AWS Command Line Interface (CLI) + Add tags to an existing EC2 resource using the CLI + Add tags to a new resource during creation using the CLI + Describe tagged resources using the CLI + Create and manage a Resource Group to organize AWS resources + Create a tag-based Resource Group + View and manage resources within a Resource Group 08/15/2025 08/15/2025 AWS Tags \u0026amp; Resource Groups: https://000027.awsstudygroup.com/ Week 5 Achievements: Successfully implemented advanced AWS networking solutions:\nVPC Peering for direct inter-VPC communication Transit Gateway for centralized network hub architecture Cross-VPC DNS resolution and routing configuration Deployed and optimized full-stack cloud applications:\nWordPress application with RDS database integration Auto Scaling and CloudFront for performance and availability Serverless cost optimization using Lambda functions Built comprehensive DevOps automation workflows:\nEnd-to-end CI/CD pipelines with AWS Developer Tools Automated deployment processes with CodePipeline integration Hybrid cloud storage solutions using Storage Gateway Configured enterprise-grade file systems and security:\nAmazon FSx for Windows File Server with Multi-AZ deployment AWS WAF implementation with custom security rules Performance monitoring and storage optimization techniques Established effective AWS resource management practices:\nResource tagging strategies for cost tracking and organization Resource Groups for streamlined resource governance Proficient use of both AWS Console and CLI operations Gained hands-on experience with Infrastructure as Code using CloudFormation templates for consistent environment provisioning.\n"
},
{
	"uri": "http://localhost:1313/internship_report/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 6 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Create an S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Test the Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": "Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.3-s3-vpc/",
	"title": "Access S3 from VPC",
	"tags": [],
	"description": "",
	"content": "Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Test the Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "
},
{
	"uri": "http://localhost:1313/internship_report/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog I have translated:\nBlog 1 - How We Built a Flywheel to Steadily Improve Security for Amazon RDS This blog details the process an AWS security team undertook to secure a new feature, PL/Rust, on Amazon Relational Database Service (Amazon RDS). The author, a principal security engineer, explains how the team moved beyond a simple implementation to build a comprehensive, self-improving security system—a \u0026ldquo;flywheel\u0026rdquo;—that combines technology, process, and testing to protect customers.\nBlog 2 - Create an SSL connection to Amazon RDS for Db2 in Java without KeyStore or Keytool This blog outlines a simplified method for establishing a secure SSL database connection in Java, specifically for Amazon Relational Database Service (Amazon RDS) for Db2. The approach allows developers to bypass the traditional complexities associated with the keytool utility and the management of Java KeyStores. The primary benefits of this technique include its simplicity, its suitability for automated environments like CI/CD pipelines, and its ability to maintain strong security through proper TLS 1.2 negotiation and server certificate validation.\nBlog 3 - Enhance the local testing experience for serverless applications with LocalStack This blog announces and explains new capabilities designed to simplify the local testing experience for serverless applications. Through an integration with AWS Partner, LocalStack, the AWS Toolkit for Visual Studio Code now provides a more streamlined way for developers to build, test, and debug their serverless applications without leaving their development environment.\n"
},
{
	"uri": "http://localhost:1313/internship_report/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "During my internship, I participated in [x] events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nKick-off AWS FCJ Workforce Event Name: Kick-off AWS FCJ Workforce - FPTU OJT FALL 2025\nDate \u0026amp; Time: 08:30, September 06, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nGist: Vision talks on AWS careers (Cloud, DevOps, AI/ML, Security, Data), alumni sharing, networking, and interactive Q\u0026amp;A.\nOutcomes and value gained: Mindset reset to prioritize long-term growth; clearer view of DevOps/cloud paths; action to engage AWS Builders community and share takeaways with the team.\nData Science On AWS Event Name: Data Science On AWS\nDate \u0026amp; Time: 09:30, October 16, 2025\nLocation: FPT University HCMC, Hi-Tech Park, Thu Duc, Ho Chi Minh City\nRole: Attendee\nGist: End-to-end Data Science pipeline on AWS (S3 → Glue → SageMaker), live demos (IMDb ETL, Sentiment model), and overview of managed AI services (Transcribe, Comprehend, Rekognition, Personalize).\nOutcomes and value gained: New skills with Glue ETL and SageMaker training/deploy; stronger data-first mindset; contribution plan to automate ETL, pilot sentiment analysis, and streamline MLOps.\nReinventing DevOps with AWS Generative AI Event Name: Reinventing DevOps with AWS Generative AI\nDate \u0026amp; Time: 19:30, October 16, 2025\nLocation: Online at CMC Global on Microsoft Teams\nRole: Attendee\nGist: DevOps → DevSecOps evolution with a seven-phase secure lifecycle, AI-augmented toolchain, and live demo of Amazon CodeGuru; emphasis on IaC security and observability.\nOutcomes and value gained: New skills in SAST/dependency scans, IaC scanning (e.g., Checkov), and AI-powered anomaly detection; contribution plan to add CI/CD security gates, pilot CodeGuru, and enhance monitoring/documentation.\n"
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.4-s3-onprem/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "On-premises DNS Simulation",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"
},
{
	"uri": "http://localhost:1313/internship_report/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "
},
{
	"uri": "http://localhost:1313/internship_report/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Le Tri Dung\nPhone Number: 0941423585\nEmail: tridungit2005@gmail.com\nUniversity: FPT Ho Chi Minh University\nMajor: Data Science\nStudent ID: SE196261\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 09/2025 to 02/2026\nReport Content Worklog Proposal "
},
{
	"uri": "http://localhost:1313/internship_report/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/internship_report/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]